{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook\n",
    "\n",
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook\n",
    "\n",
    "#\n",
    "# Verify Reading Dataset via MnistDataloader class\n",
    "#\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = 'dataset/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(data_train, label_train), (data_test, label_test) = mnist_dataloader.load_data()\n",
    "\n",
    "with open(\"dataset/pickled/data_train.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_train, outfile)\n",
    "with open(\"dataset/pickled/label_train.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_train, outfile)\n",
    "with open(\"dataset/pickled/data_test.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_test, outfile)\n",
    "with open(\"dataset/pickled/label_test.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_test, outfile)\n",
    "\n",
    "first_10_data_train = []\n",
    "first_10_label_train = []\n",
    "for i in range(0, 10):\n",
    "    first_10_data_train.append(data_train[i])\n",
    "    first_10_label_train.append(label_train[i])\n",
    "\n",
    "with open(\"dataset/pickled/first_10_data_train.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(first_10_data_train, outfile)\n",
    "with open(\"dataset/pickled/first_10_label_train.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(first_10_label_train, outfile)\n",
    "\n",
    "# There are 60000 training examples\n",
    "# Try making it into 10 batches => 6000 per batch\n",
    "\n",
    "data_batch_01 = []\n",
    "data_batch_02 = []\n",
    "data_batch_03 = []\n",
    "data_batch_04 = []\n",
    "data_batch_05 = []\n",
    "data_batch_06 = []\n",
    "data_batch_07 = []\n",
    "data_batch_08 = []\n",
    "data_batch_09 = []\n",
    "data_batch_10 = []\n",
    "label_batch_01 = []\n",
    "label_batch_02 = []\n",
    "label_batch_03 = []\n",
    "label_batch_04 = []\n",
    "label_batch_05 = []\n",
    "label_batch_06 = []\n",
    "label_batch_07 = []\n",
    "label_batch_08 = []\n",
    "label_batch_09 = []\n",
    "label_batch_10 = []\n",
    "for i in range(0, 6000):\n",
    "    data_batch_01.append(data_train[i + (0 * 6000)])\n",
    "    data_batch_02.append(data_train[i + (1 * 6000)])\n",
    "    data_batch_03.append(data_train[i + (2 * 6000)])\n",
    "    data_batch_04.append(data_train[i + (3 * 6000)])\n",
    "    data_batch_05.append(data_train[i + (4 * 6000)])\n",
    "    data_batch_06.append(data_train[i + (5 * 6000)])\n",
    "    data_batch_07.append(data_train[i + (6 * 6000)])\n",
    "    data_batch_08.append(data_train[i + (7 * 6000)])\n",
    "    data_batch_09.append(data_train[i + (8 * 6000)])\n",
    "    data_batch_10.append(data_train[i + (9 * 6000)])\n",
    "    label_batch_01.append(label_train[i + (0 * 6000)])\n",
    "    label_batch_02.append(label_train[i + (1 * 6000)])\n",
    "    label_batch_03.append(label_train[i + (2 * 6000)])\n",
    "    label_batch_04.append(label_train[i + (3 * 6000)])\n",
    "    label_batch_05.append(label_train[i + (4 * 6000)])\n",
    "    label_batch_06.append(label_train[i + (5 * 6000)])\n",
    "    label_batch_07.append(label_train[i + (6 * 6000)])\n",
    "    label_batch_08.append(label_train[i + (7 * 6000)])\n",
    "    label_batch_09.append(label_train[i + (8 * 6000)])\n",
    "    label_batch_10.append(label_train[i + (9 * 6000)])\n",
    "\n",
    "with open(\"dataset/pickled/data_batch_01.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_01, outfile)\n",
    "with open(\"dataset/pickled/data_batch_02.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_02, outfile)\n",
    "with open(\"dataset/pickled/data_batch_03.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_03, outfile)\n",
    "with open(\"dataset/pickled/data_batch_04.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_04, outfile)\n",
    "with open(\"dataset/pickled/data_batch_05.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_05, outfile)\n",
    "with open(\"dataset/pickled/data_batch_06.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_06, outfile)\n",
    "with open(\"dataset/pickled/data_batch_07.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_07, outfile)\n",
    "with open(\"dataset/pickled/data_batch_08.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_08, outfile)\n",
    "with open(\"dataset/pickled/data_batch_09.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_09, outfile)\n",
    "with open(\"dataset/pickled/data_batch_10.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_10, outfile)\n",
    "\n",
    "with open(\"dataset/pickled/label_batch_01.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_01, outfile)\n",
    "with open(\"dataset/pickled/label_batch_02.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_02, outfile)\n",
    "with open(\"dataset/pickled/label_batch_03.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_03, outfile)\n",
    "with open(\"dataset/pickled/label_batch_04.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_04, outfile)\n",
    "with open(\"dataset/pickled/label_batch_05.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_05, outfile)\n",
    "with open(\"dataset/pickled/label_batch_06.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_06, outfile)\n",
    "with open(\"dataset/pickled/label_batch_07.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_07, outfile)\n",
    "with open(\"dataset/pickled/label_batch_08.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_08, outfile)\n",
    "with open(\"dataset/pickled/label_batch_09.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_09, outfile)\n",
    "with open(\"dataset/pickled/label_batch_10.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_10, outfile)\n",
    "\n",
    "data_batch_first_100 = []\n",
    "label_batch_first_100 = []\n",
    "for i in range(100):\n",
    "    data_batch_first_100.append(data_train[i])\n",
    "    label_batch_first_100.append(label_train[i])\n",
    "\n",
    "with open(\"dataset/pickled/data_batch_first_100.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(data_batch_first_100, outfile)\n",
    "with open(\"dataset/pickled/label_batch_first_100.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(label_batch_first_100, outfile)\n",
    "\n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "   images_2_show.append(first_10_data_train[i])\n",
    "   titles_2_show.append('training image [' + str(i) + '] = ' + str(first_10_label_train[i]))    \n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(data_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(label_test[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
