{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 0.1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(-1.0, 0.0) for i in range(currentLayerLen)])\n",
    "        #self.biasVector = np.zeros(currentLayerLen)\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetLayer(self):\n",
    "        self.zVector = np.zeros(self.zVector.size)\n",
    "        self.errorVector = np.zeros(self.errorVector.size)\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen) * 0.001\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    if x <= -710:\n",
    "        return 0.0\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # # FOR 4 3 3 2 test\n",
    "\n",
    "        # for idx in range(0, 4):\n",
    "        #     self.Layers[0].activationVector[idx] = dataset[idx]\n",
    "\n",
    "        #\n",
    "        # FOR MINST\n",
    "        #\n",
    "\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += (1.0 - self.Layers[self.Layers.size - 1].activationVector[i]) * (1.0 - self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "            else:\n",
    "                sum += (self.Layers[self.Layers.size - 1].activationVector[i]) * (self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "        return sum / (self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        # The desired value for target is 1\n",
    "        CGradient = np.zeros(self.Layers[self.Layers.size - 1].size)\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            CGradient[i] = self.Layers[self.Layers.size - 1].activationVector[i]\n",
    "            if i == target:\n",
    "                CGradient[i] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def resetNetwork(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetLayer()\n",
    "\n",
    "    def resetAdjs(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        self.resetAdjs()\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nRESET NETWORK\\n\\n\")\n",
    "            self.resetNetwork()\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nSET START LAYER ACTIVATIONS\\n\\n\")\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL FORWARD PROPAGATION\\n\\n\")\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL BACKWARD PROPAGATION\\n\\n\")\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nADJUST BASED ON GRADIENT DESCENT FOR CURRENT EXAMPLE\\n\\n\")\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "        #     self.cout()\n",
    "\n",
    "        # print(\"\\n\\nADJUST WITH ADJUST VARIABLES\\n\\n\")\n",
    "        self.adjustWithAdjustVariables()\n",
    "        # self.cout()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[self.Layers.size - 1].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def checkRandomExamples(self, data, labels):\n",
    "        numberOfData = len(data)\n",
    "\n",
    "        for num in range (0, 20):\n",
    "            randIdx = random.randint(0, numberOfData)\n",
    "            \n",
    "            self.setStartLayerActivations(data[randIdx])\n",
    "            self.fullForwardPropagation(labels[randIdx])\n",
    "\n",
    "            print(\"Label for the data is: \" + str(labels[randIdx]))\n",
    "            self.coutLastLayer()\n",
    "            print(\"The cost is: \" + str(self.cost(labels[randIdx])))\n",
    "\n",
    "    def coutLastLayer(self):\n",
    "        print(\"The last layer activations are: \")\n",
    "        print(self.Layers[self.Layers.size - 1].activationVector)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on the batch\n",
      "The last layer activations are: \n",
      "[0.30591497 0.3955885  0.38828481 0.30468098 0.479464   0.36209484\n",
      " 0.33898756 0.45473099 0.4841126  0.32541455]\n",
      "The last layer activations are: \n",
      "[0.04309991 0.02308545 0.02429242 0.04345276 0.01359778 0.02922144\n",
      " 0.03438707 0.01566102 0.01326649 0.98869299]\n",
      "The last layer activations are: \n",
      "[0.03987736 0.02209266 0.02321956 0.04017063 0.01307063 0.02776047\n",
      " 0.03239646 0.01505299 0.01275166 0.98919741]\n",
      "The last layer activations are: \n",
      "[0.03721422 0.02121376 0.02227098 0.0374638  0.0126048  0.02648034\n",
      " 0.03068056 0.01451467 0.01229687 0.989639  ]\n",
      "The last layer activations are: \n",
      "[0.03496302 0.02042434 0.02142021 0.03517935 0.01218595 0.02534271\n",
      " 0.02917829 0.01403007 0.01188805 0.9900329 ]\n",
      "The last layer activations are: \n",
      "[0.03302573 0.0197073  0.02064866 0.03321603 0.01180435 0.02432034\n",
      " 0.02784653 0.01358828 0.01151566 0.9903893 ]\n",
      "The last layer activations are: \n",
      "[0.03133429 0.0190502  0.01994275 0.03150374 0.01145312 0.02339316\n",
      " 0.02665373 0.01318153 0.01117292 0.99071538]\n",
      "The last layer activations are: \n",
      "[0.02983978 0.0184437  0.01929227 0.0299922  0.0111272  0.02254599\n",
      " 0.02557618 0.0128041  0.0108549  0.99101636]\n",
      "The last layer activations are: \n",
      "[0.02850601 0.01788058 0.01868928 0.0286443  0.01082279 0.021767\n",
      " 0.02459563 0.01245165 0.01055787 0.99129614]\n",
      "The last layer activations are: \n",
      "[0.02730556 0.01735518 0.01812756 0.02743196 0.01053696 0.02104686\n",
      " 0.02369778 0.01212085 0.01027898 0.9915577 ]\n",
      "The last layer activations are: \n",
      "[0.02621721 0.01686293 0.01760209 0.02633349 0.01026742 0.02037805\n",
      " 0.02287122 0.01180905 0.01001596 0.99180339]\n",
      "The last layer activations are: \n",
      "[0.02522428 0.01640012 0.01710875 0.02533186 0.01001233 0.01975443\n",
      " 0.02210671 0.01151414 0.00976704 0.99203505]\n",
      "The last layer activations are: \n",
      "[0.02431343 0.01596367 0.01664417 0.02441345 0.0097702  0.01917092\n",
      " 0.02139667 0.01123437 0.00953074 0.9922542 ]\n",
      "The last layer activations are: \n",
      "[0.02347383 0.01555101 0.01620549 0.02356725 0.00953978 0.01862328\n",
      " 0.02073484 0.01096832 0.00930587 0.99246206]\n",
      "The last layer activations are: \n",
      "[0.02269663 0.01515996 0.01579031 0.02278422 0.00932007 0.01810791\n",
      " 0.02011597 0.01071479 0.00909142 0.99265965]\n",
      "The last layer activations are: \n",
      "[0.02197447 0.01478865 0.01539657 0.0220569  0.00911019 0.01762176\n",
      " 0.01953559 0.01047276 0.00888656 0.99284784]\n",
      "The last layer activations are: \n",
      "[0.0213012  0.01443549 0.01502249 0.02137903 0.0089094  0.0171622\n",
      " 0.01898993 0.01024136 0.00869056 0.99302737]\n",
      "The last layer activations are: \n",
      "[0.02067161 0.01409906 0.01466651 0.02074532 0.00871705 0.01672693\n",
      " 0.01847573 0.01001983 0.00850279 0.99319886]\n",
      "The last layer activations are: \n",
      "[0.02008128 0.01377813 0.01432729 0.02015127 0.00853259 0.01631398\n",
      " 0.01799017 0.00980752 0.00832271 0.99336288]\n",
      "The last layer activations are: \n",
      "[0.0195264  0.01347162 0.01400361 0.01959302 0.00835553 0.01592158\n",
      " 0.0175308  0.00960383 0.00814984 0.99351991]\n",
      "The last layer activations are: \n",
      "[0.01900369 0.01317855 0.01369441 0.01906725 0.00818542 0.01554819\n",
      " 0.01709544 0.00940824 0.00798375 0.99367037]\n",
      "The last layer activations are: \n",
      "[0.01851027 0.01289806 0.01339874 0.01857104 0.00802188 0.01519243\n",
      " 0.01668221 0.00922029 0.00782406 0.99381466]\n",
      "The last layer activations are: \n",
      "[0.01804365 0.01262936 0.01311572 0.01810187 0.00786453 0.01485307\n",
      " 0.01628941 0.00903956 0.00767042 0.99395313]\n",
      "The last layer activations are: \n",
      "[0.01760162 0.01237174 0.01284457 0.01765749 0.00771306 0.01452899\n",
      " 0.01591554 0.00886566 0.00752251 0.99408609]\n",
      "The last layer activations are: \n",
      "[0.01718223 0.01212455 0.01258459 0.01723593 0.00756718 0.0142192\n",
      " 0.01555925 0.00869822 0.00738004 0.99421384]\n",
      "The last layer activations are: \n",
      "[0.01678374 0.01188719 0.01233513 0.01683544 0.00742659 0.01392278\n",
      " 0.01521933 0.00853694 0.00724275 0.99433664]\n",
      "The last layer activations are: \n",
      "[0.01640459 0.01165913 0.01209558 0.01645445 0.00729106 0.01363892\n",
      " 0.01489467 0.0083815  0.00711039 0.99445475]\n",
      "The last layer activations are: \n",
      "[0.0160434  0.01143986 0.01186541 0.01609154 0.00716033 0.01336684\n",
      " 0.01458429 0.00823163 0.00698271 0.9945684 ]\n",
      "The last layer activations are: \n",
      "[0.01569891 0.01122892 0.0116441  0.01574545 0.00703419 0.01310587\n",
      " 0.01428728 0.00808706 0.00685952 0.99467779]\n",
      "The last layer activations are: \n",
      "[0.01536999 0.01102586 0.01143118 0.01541503 0.00691244 0.01285535\n",
      " 0.0140028  0.00794754 0.0067406  0.99478314]\n",
      "The last layer activations are: \n",
      "[0.01505561 0.01083029 0.01122622 0.01509925 0.00679486 0.01261471\n",
      " 0.01373012 0.00781285 0.00662577 0.99488464]\n",
      "The last layer activations are: \n",
      "[0.01475482 0.01064184 0.01102881 0.01479716 0.00668128 0.01238339\n",
      " 0.01346851 0.00768277 0.00651484 0.99498246]\n",
      "The last layer activations are: \n",
      "[0.01446678 0.01046015 0.01083856 0.01450789 0.00657153 0.0121609\n",
      " 0.01321736 0.00755709 0.00640764 0.99507677]\n",
      "The last layer activations are: \n",
      "[0.01419071 0.01028489 0.01065514 0.01423066 0.00646543 0.01194676\n",
      " 0.01297605 0.00743561 0.00630401 0.99516773]\n",
      "The last layer activations are: \n",
      "[0.01392587 0.01011576 0.0104782  0.01396474 0.00636284 0.01174053\n",
      " 0.01274406 0.00731817 0.00620381 0.99525549]\n",
      "The last layer activations are: \n",
      "[0.01367162 0.00995246 0.01030742 0.01370947 0.00626359 0.01154182\n",
      " 0.01252086 0.00720457 0.00610687 0.9953402 ]\n",
      "The last layer activations are: \n",
      "[0.01342735 0.00979473 0.01014253 0.01346422 0.00616756 0.01135024\n",
      " 0.01230598 0.00709466 0.00601307 0.99542199]\n",
      "The last layer activations are: \n",
      "[0.01319248 0.0096423  0.00998323 0.01322844 0.0060746  0.01116543\n",
      " 0.012099   0.00698828 0.00592227 0.99550099]\n",
      "The last layer activations are: \n",
      "[0.0129665  0.00949494 0.00982928 0.01300159 0.00598458 0.01098706\n",
      " 0.01189949 0.00688528 0.00583436 0.99557732]\n",
      "The last layer activations are: \n",
      "[0.01274893 0.0093524  0.00968043 0.01278319 0.00589739 0.01081482\n",
      " 0.01170709 0.00678552 0.0057492  0.9956511 ]\n",
      "The last layer activations are: \n",
      "[0.01253932 0.00921449 0.00953644 0.01257279 0.00581291 0.01064841\n",
      " 0.01152142 0.00668887 0.00566669 0.99572244]\n",
      "The last layer activations are: \n",
      "[0.01233724 0.00908099 0.0093971  0.01236997 0.00573103 0.01048757\n",
      " 0.01134216 0.00659519 0.00558671 0.99579144]\n",
      "The last layer activations are: \n",
      "[0.01214232 0.00895172 0.0092622  0.01217433 0.00565164 0.01033203\n",
      " 0.011169   0.00650436 0.00550918 0.9958582 ]\n",
      "The last layer activations are: \n",
      "[0.01195418 0.00882647 0.00913155 0.01198552 0.00557465 0.01018154\n",
      " 0.01100164 0.00641628 0.00543398 0.99592282]\n",
      "The last layer activations are: \n",
      "[0.01177248 0.0087051  0.00900496 0.01180318 0.00549995 0.01003588\n",
      " 0.01083981 0.00633082 0.00536102 0.99598538]\n",
      "The last layer activations are: \n",
      "[0.01159692 0.00858742 0.00888226 0.01162699 0.00542745 0.00989484\n",
      " 0.01068324 0.00624788 0.00529022 0.99604598]\n",
      "The last layer activations are: \n",
      "[0.01142719 0.00847329 0.00876328 0.01145667 0.00535707 0.00975819\n",
      " 0.0105317  0.00616737 0.00522149 0.99610469]\n",
      "The last layer activations are: \n",
      "[0.01126301 0.00836256 0.00864787 0.01129193 0.00528873 0.00962576\n",
      " 0.01038496 0.00608918 0.00515474 0.9961616 ]\n",
      "The last layer activations are: \n",
      "[0.01110413 0.00825508 0.00853587 0.0111325  0.00522234 0.00949736\n",
      " 0.01024279 0.00601323 0.00508991 0.99621677]\n",
      "The last layer activations are: \n",
      "[0.01095029 0.00815072 0.00842715 0.01097814 0.00515783 0.00937282\n",
      " 0.010105   0.00593942 0.00502691 0.99627028]\n",
      "The last layer activations are: \n",
      "[0.01080126 0.00804937 0.00832157 0.01082861 0.00509513 0.00925196\n",
      " 0.0099714  0.00586768 0.00496568 0.9963222 ]\n",
      "The last layer activations are: \n",
      "[0.01065683 0.00795088 0.00821901 0.01068371 0.00503416 0.00913465\n",
      " 0.0098418  0.00579793 0.00490614 0.99637258]\n",
      "The last layer activations are: \n",
      "[0.0105168  0.00785516 0.00811934 0.01054321 0.00497487 0.00902072\n",
      " 0.00971603 0.00573008 0.00484824 0.99642149]\n",
      "The last layer activations are: \n",
      "[0.01038096 0.00776209 0.00802244 0.01040693 0.00491718 0.00891005\n",
      " 0.00959393 0.00566407 0.00479191 0.99646899]\n",
      "The last layer activations are: \n",
      "[0.01024914 0.00767157 0.00792822 0.01027468 0.00486105 0.00880249\n",
      " 0.00947534 0.00559983 0.00473709 0.99651513]\n",
      "The last layer activations are: \n",
      "[0.01012116 0.0075835  0.00783656 0.01014629 0.0048064  0.00869793\n",
      " 0.00936012 0.00553729 0.00468374 0.99655997]\n",
      "The last layer activations are: \n",
      "[0.00999687 0.00749779 0.00774736 0.0100216  0.00475319 0.00859624\n",
      " 0.00924814 0.0054764  0.00463178 0.99660355]\n",
      "The last layer activations are: \n",
      "[0.0098761  0.00741434 0.00766054 0.00990045 0.00470136 0.00849731\n",
      " 0.00913925 0.00541708 0.00458118 0.99664593]\n",
      "The last layer activations are: \n",
      "[0.00975872 0.00733307 0.00757599 0.00978269 0.00465087 0.00840104\n",
      " 0.00903334 0.00535928 0.00453187 0.99668715]\n",
      "The last layer activations are: \n",
      "[0.00964457 0.00725391 0.00749364 0.00966819 0.00460166 0.00830731\n",
      " 0.0089303  0.00530294 0.00448383 0.99672725]\n",
      "The last layer activations are: \n",
      "[0.00953355 0.00717676 0.00741341 0.00955682 0.00455369 0.00821604\n",
      " 0.00883    0.00524803 0.00443699 0.99676628]\n",
      "The last layer activations are: \n",
      "[0.0094255  0.00710157 0.00733521 0.00944844 0.00450691 0.00812713\n",
      " 0.00873234 0.00519447 0.00439132 0.99680428]\n",
      "The last layer activations are: \n",
      "[0.00932034 0.00702825 0.00725897 0.00934295 0.00446129 0.00804049\n",
      " 0.00863722 0.00514223 0.00434678 0.99684128]\n",
      "The last layer activations are: \n",
      "[0.00921793 0.00695674 0.00718463 0.00924023 0.00441678 0.00795604\n",
      " 0.00854454 0.00509126 0.00430333 0.99687732]\n",
      "The last layer activations are: \n",
      "[0.00911817 0.00688698 0.0071121  0.00914017 0.00437334 0.0078737\n",
      " 0.00845422 0.00504152 0.00426092 0.99691243]\n",
      "The last layer activations are: \n",
      "[0.00902096 0.0068189  0.00704134 0.00904267 0.00433094 0.00779339\n",
      " 0.00836616 0.00499296 0.00421953 0.99694666]\n",
      "The last layer activations are: \n",
      "[0.00892622 0.00675245 0.00697227 0.00894763 0.00428955 0.00771504\n",
      " 0.00828028 0.00494554 0.00417912 0.99698002]\n",
      "The last layer activations are: \n",
      "[0.00883383 0.00668756 0.00690484 0.00885497 0.00424912 0.00763857\n",
      " 0.00819651 0.00489923 0.00413965 0.99701255]\n",
      "The last layer activations are: \n",
      "[0.00874373 0.0066242  0.00683898 0.0087646  0.00420962 0.00756393\n",
      " 0.00811476 0.00485399 0.0041011  0.99704429]\n",
      "The last layer activations are: \n",
      "[0.00865581 0.00656229 0.00677466 0.00867642 0.00417103 0.00749105\n",
      " 0.00803497 0.00480977 0.00406343 0.99707525]\n",
      "The last layer activations are: \n",
      "[0.00857002 0.0065018  0.00671181 0.00859037 0.00413332 0.00741986\n",
      " 0.00795706 0.00476656 0.00402662 0.99710547]\n",
      "The last layer activations are: \n",
      "[0.00848626 0.00644267 0.00665039 0.00850637 0.00409644 0.00735032\n",
      " 0.00788097 0.0047243  0.00399063 0.99713497]\n",
      "The last layer activations are: \n",
      "[0.00840448 0.00638487 0.00659034 0.00842434 0.00406039 0.00728236\n",
      " 0.00780665 0.00468299 0.00395544 0.99716377]\n",
      "The last layer activations are: \n",
      "[0.00832459 0.00632835 0.00653163 0.00834422 0.00402513 0.00721593\n",
      " 0.00773402 0.00464257 0.00392102 0.9971919 ]\n",
      "The last layer activations are: \n",
      "[0.00824654 0.00627306 0.0064742  0.00826594 0.00399064 0.00715098\n",
      " 0.00766303 0.00460303 0.00388736 0.99721938]\n",
      "The last layer activations are: \n",
      "[0.00817026 0.00621897 0.00641803 0.00818944 0.00395689 0.00708746\n",
      " 0.00759363 0.00456434 0.00385442 0.99724623]\n",
      "The last layer activations are: \n",
      "[0.00809569 0.00616603 0.00636306 0.00811466 0.00392385 0.00702532\n",
      " 0.00752575 0.00452647 0.00382218 0.99727247]\n",
      "The last layer activations are: \n",
      "[0.00802277 0.00611422 0.00630926 0.00804153 0.00389152 0.00696453\n",
      " 0.00745937 0.0044894  0.00379062 0.99729813]\n",
      "The last layer activations are: \n",
      "[0.00795146 0.0060635  0.00625659 0.00797001 0.00385986 0.00690503\n",
      " 0.00739441 0.00445309 0.00375972 0.99732321]\n",
      "The last layer activations are: \n",
      "[0.00788169 0.00601383 0.00620502 0.00790005 0.00382885 0.00684679\n",
      " 0.00733085 0.00441753 0.00372946 0.99734775]\n",
      "The last layer activations are: \n",
      "[0.00781342 0.00596518 0.00615451 0.00783158 0.00379848 0.00678977\n",
      " 0.00726863 0.0043827  0.00369982 0.99737175]\n",
      "The last layer activations are: \n",
      "[0.0077466  0.00591752 0.00610503 0.00776457 0.00376872 0.00673392\n",
      " 0.00720771 0.00434857 0.00367079 0.99739524]\n",
      "The last layer activations are: \n",
      "[0.00768119 0.00587082 0.00605656 0.00769897 0.00373956 0.00667922\n",
      " 0.00714805 0.00431512 0.00364233 0.99741823]\n",
      "The last layer activations are: \n",
      "[0.00761713 0.00582505 0.00600905 0.00763473 0.00371098 0.00662562\n",
      " 0.00708962 0.00428233 0.00361444 0.99744073]\n",
      "The last layer activations are: \n",
      "[0.00755439 0.00578018 0.00596248 0.00757181 0.00368297 0.0065731\n",
      " 0.00703237 0.00425018 0.00358711 0.99746276]\n",
      "The last layer activations are: \n",
      "[0.00749292 0.00573619 0.00591682 0.00751018 0.00365549 0.00652162\n",
      " 0.00697626 0.00421866 0.0035603  0.99748433]\n",
      "The last layer activations are: \n",
      "[0.00743269 0.00569305 0.00587205 0.00744978 0.00362855 0.00647115\n",
      " 0.00692128 0.00418774 0.00353401 0.99750547]\n",
      "The last layer activations are: \n",
      "[0.00737367 0.00565074 0.00582814 0.00739059 0.00360213 0.00642166\n",
      " 0.00686737 0.00415741 0.00350823 0.99752617]\n",
      "The last layer activations are: \n",
      "[0.0073158  0.00560923 0.00578507 0.00733256 0.0035762  0.00637313\n",
      " 0.00681451 0.00412765 0.00348293 0.99754646]\n",
      "The last layer activations are: \n",
      "[0.00725906 0.0055685  0.0057428  0.00727566 0.00355076 0.00632552\n",
      " 0.00676267 0.00409845 0.00345811 0.99756635]\n",
      "The last layer activations are: \n",
      "[0.00720342 0.00552852 0.00570133 0.00721987 0.00352579 0.0062788\n",
      " 0.00671182 0.00406978 0.00343375 0.99758584]\n",
      "The last layer activations are: \n",
      "[0.00714885 0.00548929 0.00566062 0.00716514 0.00350128 0.00623296\n",
      " 0.00666193 0.00404164 0.00340984 0.99760495]\n",
      "The last layer activations are: \n",
      "[0.0070953  0.00545077 0.00562065 0.00711145 0.00347722 0.00618797\n",
      " 0.00661297 0.00401402 0.00338637 0.9976237 ]\n",
      "The last layer activations are: \n",
      "[0.00704276 0.00541294 0.00558141 0.00705876 0.0034536  0.0061438\n",
      " 0.00656492 0.00398688 0.00336332 0.99764208]\n",
      "The last layer activations are: \n",
      "[0.0069912  0.00537579 0.00554288 0.00700706 0.00343039 0.00610044\n",
      " 0.00651774 0.00396023 0.00334068 0.99766011]\n",
      "The last layer activations are: \n",
      "[0.00694058 0.00533931 0.00550503 0.0069563  0.0034076  0.00605785\n",
      " 0.00647143 0.00393406 0.00331845 0.99767781]\n",
      "The last layer activations are: \n",
      "[0.00689088 0.00530346 0.00546785 0.00690647 0.00338521 0.00601603\n",
      " 0.00642595 0.00390834 0.0032966  0.99769517]\n",
      "The last layer activations are: \n",
      "[0.00684208 0.00526824 0.00543131 0.00685753 0.00336321 0.00597494\n",
      " 0.00638128 0.00388306 0.00327514 0.99771222]\n",
      "The last layer activations are: \n",
      "[0.00679415 0.00523362 0.00539541 0.00680947 0.00334159 0.00593457\n",
      " 0.0063374  0.00385822 0.00325405 0.99772895]\n",
      "The last layer activations are: \n",
      "[0.00674707 0.0051996  0.00536012 0.00676226 0.00332034 0.00589489\n",
      " 0.00629428 0.0038338  0.00323332 0.99774538]\n",
      "Average cost is:  0.17816845744273083\n",
      "Percentage of correct is:  0.1009\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.00840634 0.00671173 0.00689643 0.00842131 0.00447417 0.00749944\n",
      " 0.00793551 0.00509832 0.00436724 0.99675875]\n",
      "The cost is: 0.19801536406802725\n",
      "Label for the data is: 6\n",
      "The last layer activations are: \n",
      "[0.00795641 0.00629882 0.00647721 0.00797148 0.00415546 0.00706262\n",
      " 0.00748949 0.00475039 0.0040538  0.99703718]\n",
      "The cost is: 0.19794748870269113\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[0.00813224 0.00645976 0.00664065 0.00814728 0.00427929 0.00723307\n",
      " 0.00766364 0.00488571 0.00417556 0.99692953]\n",
      "The cost is: 0.19813379678591087\n",
      "Label for the data is: 9\n",
      "The last layer activations are: \n",
      "[0.00739122 0.00578525 0.00595533 0.00740634 0.00376388 0.00651711\n",
      " 0.00693105 0.00432129 0.00366896 0.99737298]\n",
      "The cost is: 3.2213402495968946e-05\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.00777102 0.00612971 0.00630543 0.00778611 0.00402592 0.00688328\n",
      " 0.00730608 0.00460864 0.00392646 0.99714906]\n",
      "The cost is: 0.19820473539051217\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "[0.00859808 0.00688874 0.00707605 0.008613   0.00461182 0.00768624\n",
      " 0.00812596 0.00524824 0.00450265 0.99663715]\n",
      "The cost is: 0.19845018190979952\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.00761107 0.00598432 0.00615769 0.00762618 0.00391501 0.00672887\n",
      " 0.00714802 0.00448712 0.00381746 0.99724423]\n",
      "The cost is: 0.19795799180041537\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[0.00857818 0.00687034 0.00705738 0.0085931  0.00459748 0.00766684\n",
      " 0.00810618 0.00523264 0.00448855 0.99664986]\n",
      "The cost is: 0.19800078355151793\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.00860541 0.00689552 0.00708292 0.00862032 0.0046171  0.00769339\n",
      " 0.00813324 0.00525399 0.00450785 0.99663247]\n",
      "The cost is: 0.19795510995788101\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.00873951 0.00701971 0.0072089  0.00875438 0.00471403 0.00782428\n",
      " 0.00826658 0.00535945 0.00460323 0.99654635]\n",
      "The cost is: 0.19779120240008843\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[0.00802772 0.00636402 0.00654343 0.00804278 0.00420557 0.0071317\n",
      " 0.00756009 0.00480517 0.00410307 0.9969937 ]\n",
      "The cost is: 0.19861683902498473\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[0.00775066 0.00611118 0.00628661 0.00776576 0.00401176 0.00686361\n",
      " 0.00728595 0.00459314 0.00391254 0.99716124]\n",
      "The cost is: 0.19854942033724826\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[0.00896752 0.00723153 0.00742371 0.00898229 0.00488001 0.00804723\n",
      " 0.00849353 0.0055398  0.0047666  0.99639799]\n",
      "The cost is: 0.19753565395941447\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.00734576 0.0057442  0.00591359 0.00736088 0.00373282 0.00647339\n",
      " 0.00688623 0.00428717 0.00363845 0.9973993 ]\n",
      "The cost is: 0.19803946001334818\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.00859043 0.00688167 0.00706886 0.00860534 0.0046063  0.00767878\n",
      " 0.00811835 0.00524224 0.00449723 0.99664204]\n",
      "The cost is: 0.19765236426932536\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[0.00809234 0.00642319 0.00660352 0.00810738 0.00425111 0.00719436\n",
      " 0.0076241  0.00485492 0.00414784 0.99695409]\n",
      "The cost is: 0.19781175986470548\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[0.00802534 0.00636184 0.00654122 0.0080404  0.00420389 0.0071294\n",
      " 0.00755773 0.00480333 0.00410142 0.99699516]\n",
      "The cost is: 0.1978326505745783\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.00808852 0.00641969 0.00659997 0.00810357 0.00424842 0.00719066\n",
      " 0.00762032 0.00485198 0.0041452  0.99695644]\n",
      "The cost is: 0.19780994040872388\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.00874763 0.00702723 0.00721653 0.00876249 0.00471991 0.0078322\n",
      " 0.00827465 0.00536584 0.00460902 0.99654111]\n",
      "The cost is: 0.19778866768590123\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[0.00802302 0.00635972 0.00653907 0.00803808 0.00420226 0.00712715\n",
      " 0.00755543 0.00480155 0.00409981 0.99699658]\n",
      "The cost is: 0.19847766574990527\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.92156863 0.54901961\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.91764706 0.99215686 0.18039216 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.91764706 0.99215686 0.71764706 0.04705882 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.91764706 0.99215686\n",
      " 0.99607843 0.56078431 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.91764706 0.99215686 0.99607843 0.97254902\n",
      " 0.38431373 0.         0.         0.         0.         0.\n",
      " 0.         0.14901961 0.76862745 0.41176471 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.92156863 0.84705882 0.4745098  0.96078431 0.99607843 0.43529412\n",
      " 0.05490196 0.         0.01960784 0.23137255 0.43921569 0.90588235\n",
      " 1.         0.91764706 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.91764706 0.66666667\n",
      " 0.         0.3372549  0.96862745 0.99215686 0.8745098  0.84313725\n",
      " 0.85098039 0.99215686 0.99215686 0.99215686 0.99607843 0.7372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.91764706 0.49411765 0.         0.\n",
      " 0.32941176 0.85098039 0.99215686 0.99607843 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.83529412 0.16470588 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07058824 0.01960784 0.         0.         0.         0.10196078\n",
      " 0.45882353 0.83921569 0.96862745 0.99215686 0.99215686 0.92941176\n",
      " 0.20784314 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09019608\n",
      " 0.89803922 0.99215686 0.99215686 0.4627451  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.85098039 0.99607843 0.99607843\n",
      " 0.60392157 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01568627\n",
      " 0.57647059 0.99607843 0.99215686 0.67058824 0.09803922 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09803922 0.70588235 0.99215686 0.99607843\n",
      " 0.84705882 0.21176471 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.30980392 0.99215686 0.99215686 0.81176471 0.09803922 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03137255 0.71764706 0.99215686\n",
      " 0.99215686 0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05490196 0.72941176 0.99607843 0.99607843 0.67058824 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.58431373 0.99215686\n",
      " 0.99215686 0.85882353 0.03529412 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666667 0.99607843 0.99215686 0.99215686 0.45490196\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.38431373\n",
      " 0.99607843 0.98039216 0.45490196 0.00784314 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.09019608 0.99607843 0.4745098\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Biases: \n",
      "[-0.14501745 -0.49605669 -0.7010877  -0.76688441 -0.22828718 -0.45277064\n",
      " -0.2575183  -0.89055968 -0.86279791 -0.5639509  -0.52342817 -0.94341261\n",
      " -0.83009278 -0.8294234  -0.31465661 -0.74065755 -0.30382678 -0.58173855\n",
      " -0.78745665 -0.7554908  -0.29144808 -0.85955643 -0.52823557 -0.99873135\n",
      " -0.87018509 -0.8816601  -0.39279707 -0.85240592 -0.23811431 -0.04556015\n",
      " -0.12432405 -0.34666159 -0.97083196 -0.38267635 -0.73017117 -0.58899649\n",
      " -0.31428032 -0.17950438 -0.81615451 -0.44149703 -0.12499354 -0.13754667\n",
      " -0.29877503 -0.25294929 -0.64320876 -0.5578437  -0.79299343 -0.87155898\n",
      " -0.96178702 -0.144025   -0.61568211 -0.99472476 -0.974458   -0.06785313\n",
      " -0.11049973 -0.79372413 -0.0776981  -0.56644433 -0.23043546 -0.45682554\n",
      " -0.58796802 -0.74956296 -0.76499241 -0.23524013 -0.48530191 -0.0964659\n",
      " -0.07403419 -0.93281715 -0.32099372 -0.64031664 -0.75920165 -0.7646207\n",
      " -0.51927602 -0.78210456 -0.5567771  -0.34641237 -0.56117105 -0.19619118\n",
      " -0.46514097 -0.86135373 -0.292223   -0.21033566 -0.83116534 -0.74747494\n",
      " -0.43894513 -0.24729408 -0.49938761 -0.43303418 -0.8793584  -0.30059352\n",
      " -0.93769114 -0.18138469 -0.53699048 -0.33478795 -0.21468858 -0.73847919\n",
      " -0.29798482 -0.18665136 -0.35208709 -0.55778738 -0.63231908 -0.91232377\n",
      " -0.17591598 -0.76136263 -0.67559361 -0.22140905 -0.59696414 -0.26850061\n",
      " -0.7409379  -0.32533554 -0.37631298 -0.94615564 -0.02411111 -0.45642991\n",
      " -0.29754445 -0.57079024 -0.41303303 -0.63650486 -0.75901833 -0.41217342\n",
      " -0.1190833  -0.5357968  -0.36309634 -0.93187561 -0.11210826 -0.74644529\n",
      " -0.43811098 -0.86142879 -0.22979561 -0.53260071 -0.151866   -0.50176175\n",
      " -0.32645523 -0.52238909 -0.36852252 -0.63946462 -0.78299475 -0.58744876\n",
      " -0.60141161 -0.17828778 -0.5400268  -0.29429717 -0.31559961 -0.05704138\n",
      " -0.64519445 -0.23683229 -0.59853223 -0.61511991 -0.53706071 -0.53978824\n",
      " -0.82824315 -0.84611665 -0.12654207 -0.30395659 -0.9451432  -0.81160686\n",
      " -0.13514782 -0.46094483 -0.56401321 -0.80512763 -0.1331682  -0.55370647\n",
      " -0.32079628 -0.54757702 -0.35291277 -0.20276391 -0.40809964 -0.02761212\n",
      " -0.85446799 -0.87977514 -0.72812074 -0.85119881 -0.67681979 -0.01693272\n",
      " -0.95853359 -0.49959567 -0.21487411 -0.6480024  -0.290316   -0.04688983\n",
      " -0.6364439  -0.92454072 -0.50552393 -0.94805699 -0.74616889 -0.82532191\n",
      " -0.00718559 -0.80774653 -0.96965615 -0.83259494 -0.65228125 -0.93290944\n",
      " -0.25310177 -0.887387   -0.89025519 -0.30043816 -0.34272141 -0.97067632\n",
      " -0.24627607 -0.27863547 -0.63463098 -0.07664744 -0.34423985 -0.59064103\n",
      " -0.23244556 -0.58399737 -0.86792475 -0.37068534 -0.03247749 -0.14342575\n",
      " -0.09548651 -0.60554019 -0.71332266 -0.54513191 -0.96195378 -0.53934773\n",
      " -0.67905096 -0.60399701 -0.93020666 -0.65890013 -0.30841762 -0.49913553\n",
      " -0.41473797 -0.81233573 -0.11573144 -0.59563897 -0.05238762 -0.49561861\n",
      " -0.80912202 -0.49196521 -0.15102257 -0.98943368 -0.81690585 -0.27188518\n",
      " -0.58079043 -0.72755478 -0.82265627 -0.3880382  -0.40564615 -0.06486315\n",
      " -0.03472714 -0.46517032 -0.11202033 -0.00992055 -0.72551689 -0.64471155\n",
      " -0.67522005 -0.01805196 -0.72134583 -0.90141387 -0.18829618 -0.95772146\n",
      " -0.76312802 -0.07932145 -0.5416311  -0.01169008 -0.90641213 -0.5657474\n",
      " -0.86567514 -0.75417932 -0.43792591 -0.36723472 -0.37389602 -0.93241702\n",
      " -0.76077232 -0.09768203 -0.98835981 -0.68719748 -0.27088577 -0.02730872\n",
      " -0.03811542 -0.63545324 -0.0925598  -0.08588027 -0.96653816 -0.58721514\n",
      " -0.76497008 -0.31213316 -0.1398811  -0.36393274 -0.7553501  -0.87559165\n",
      " -0.48365913 -0.19829945 -0.50524452 -0.97327152 -0.79965157 -0.1714926\n",
      " -0.87952743 -0.86574731 -0.96115636 -0.81749842 -0.78230524 -0.94397811\n",
      " -0.58377208 -0.24903669 -0.15076446 -0.2937726  -0.71722068 -0.73170172\n",
      " -0.15397353 -0.13962627 -0.74148952 -0.44491379 -0.37058585 -0.10970383\n",
      " -0.26067097 -0.63461296 -0.58345691 -0.20931203 -0.20505708 -0.36005394\n",
      " -0.49695372 -0.72576166 -0.82182901 -0.53894383 -0.68709175 -0.91437074\n",
      " -0.16496489 -0.23312833 -0.7257581  -0.89797515 -0.61841582 -0.63854604\n",
      " -0.25441537 -0.48803122 -0.70776786 -0.7984402  -0.98575971 -0.43558759\n",
      " -0.20007391 -0.63685408 -0.09739002 -0.27037961 -0.89967032 -0.31949252\n",
      " -0.33013404 -0.76405622 -0.81503118 -0.9764732  -0.93591456 -0.1288329\n",
      " -0.70178344 -0.80096401 -0.21210852 -0.88358562 -0.98857169 -0.46722224\n",
      " -0.30934926 -0.14126122 -0.52270479 -0.89226561 -0.26891419 -0.21566013\n",
      " -0.5721276  -0.33385171 -0.866944   -0.35550455 -0.79918335 -0.52191976\n",
      " -0.72650216 -0.00154164 -0.80241641 -0.92276175 -0.37042834 -0.76870754\n",
      " -0.46084709 -0.13447798 -0.84164501 -0.97779696 -0.68760834 -0.81130642\n",
      " -0.00289267 -0.67228284 -0.94303384 -0.21678933 -0.29496817 -0.12891789\n",
      " -0.49056015 -0.10389346 -0.05066505 -0.22465264 -0.32370583 -0.43877767\n",
      " -0.83532979 -0.2466801  -0.13041941 -0.8942843  -0.50908047 -0.71802059\n",
      " -0.86931165 -0.48993626 -0.31135918 -0.54530623 -0.06412262 -0.69294098\n",
      " -0.30026109 -0.1493914  -0.88931287 -0.16034183 -0.39637692 -0.62604129\n",
      " -0.80619713 -0.78167561 -0.79909095 -0.49732586 -0.09992025 -0.36221599\n",
      " -0.26602241 -0.03756386 -0.63990564 -0.38740769 -0.74036726 -0.69615361\n",
      " -0.10796437 -0.61609553 -0.54425876 -0.96425223 -0.60093045 -0.20705741\n",
      " -0.75090865 -0.03956463 -0.03577349 -0.60726923 -0.2362061  -0.02572906\n",
      " -0.78771914 -0.97118604 -0.86129964 -0.33050195 -0.57879474 -0.42585523\n",
      " -0.76650974 -0.81301547 -0.55348177 -0.20588124 -0.66657121 -0.23224828\n",
      " -0.39563583 -0.2755035  -0.14883932 -0.7713568  -0.67558203 -0.63936451\n",
      " -0.23161252 -0.97857066 -0.36516835 -0.79476555 -0.20713109 -0.65103535\n",
      " -0.75977756 -0.91783248 -0.56761283 -0.63171596 -0.14436399 -0.74702252\n",
      " -0.7514971  -0.76225208 -0.03810249 -0.51401868 -0.34217552 -0.54186156\n",
      " -0.37612528 -0.73885429 -0.48731573 -0.07395216 -0.19694585 -0.51203616\n",
      " -0.95052535 -0.06670487 -0.10328012 -0.74918202 -0.11602623 -0.05919389\n",
      " -0.39947124 -0.84444872 -0.38312672 -0.85480464 -0.90891495 -0.13349741\n",
      " -0.95785075 -0.01896533 -0.08420816 -0.89950506 -0.21991574 -0.32270618\n",
      " -0.17588896 -0.53374592 -0.85253528 -0.78607526 -0.64278768 -0.01536805\n",
      " -0.32775048 -0.65875534 -0.51064974 -0.14649279 -0.6955443  -0.38425805\n",
      " -0.3119267  -0.7238312  -0.5845847  -0.71095755 -0.6434763  -0.65689995\n",
      " -0.56938173 -0.20483291 -0.46813595 -0.97009346 -0.79380931 -0.26539265\n",
      " -0.45076497 -0.20251996 -0.37259231 -0.19570935 -0.18553323 -0.0132344\n",
      " -0.40200332 -0.16562295 -0.91624867 -0.16449982 -0.46107847 -0.27115731\n",
      " -0.87648488 -0.70733309 -0.52756363 -0.66600505 -0.43753819 -0.49449455\n",
      " -0.90111195 -0.20049022 -0.94989572 -0.0876266  -0.86840575 -0.84317416\n",
      " -0.36489389 -0.35482445 -0.9744596  -0.63104314 -0.2180562  -0.3084969\n",
      " -0.11436107 -0.15252367 -0.27465136 -0.52408936 -0.96044228 -0.51796946\n",
      " -0.17431065 -0.96784954 -0.84610533 -0.31360673 -0.06743136 -0.79339094\n",
      " -0.8421865  -0.72353805 -0.05464245 -0.0515403  -0.50708298 -0.19260377\n",
      " -0.33461071 -0.87521963 -0.33207873 -0.38287741 -0.02814706 -0.25412418\n",
      " -0.43340611 -0.07851214 -0.05871473 -0.80569416 -0.60429426 -0.91845609\n",
      " -0.19457752 -0.56404077 -0.39801039 -0.59440259 -0.2572263  -0.14609533\n",
      " -0.73215958 -0.42050913 -0.95325421 -0.77946105 -0.45473311 -0.86837444\n",
      " -0.57725255 -0.5993182  -0.70604281 -0.38654203 -0.15330982 -0.59158161\n",
      " -0.71534375 -0.06198126 -0.36623971 -0.93450676 -0.47347788 -0.57245908\n",
      " -0.2094923  -0.49021131 -0.12025073 -0.31664048 -0.77248638 -0.52040918\n",
      " -0.96917314 -0.08765581 -0.13450938 -0.22679293 -0.77129002 -0.97933742\n",
      " -0.9796801  -0.68016723 -0.1102709  -0.64402692 -0.78846411 -0.94717894\n",
      " -0.31791811 -0.64676327 -0.94894831 -0.33127888 -0.58134032 -0.1774956\n",
      " -0.28825797 -0.30740693 -0.81694444 -0.51444395 -0.5058112  -0.15546681\n",
      " -0.98002238 -0.37521399 -0.16346191 -0.07077166 -0.37087367 -0.51534652\n",
      " -0.61500053 -0.68841948 -0.61787695 -0.80443231 -0.7922705  -0.3228623\n",
      " -0.88614752 -0.79480573 -0.32227159 -0.29770038 -0.91740443 -0.11891705\n",
      " -0.51604292 -0.10131466 -0.83662239 -0.4240688  -0.98370485 -0.82774279\n",
      " -0.45142149 -0.80882594 -0.82461185 -0.13448155 -0.31600416 -0.2381604\n",
      " -0.88995636 -0.36947982 -0.24865243 -0.31610336 -0.67669761 -0.28258185\n",
      " -0.96202852 -0.96810092 -0.25339458 -0.92805305 -0.59347925 -0.46895135\n",
      " -0.57712829 -0.22717263 -0.34716444 -0.70003066 -0.7920346  -0.84494185\n",
      " -0.91425406 -0.94515868 -0.32768058 -0.43905985 -0.86877902 -0.85226098\n",
      " -0.11428429 -0.66136451 -0.11712513 -0.66179976 -0.89270051 -0.69365309\n",
      " -0.8715718  -0.84992129 -0.63167911 -0.55194782 -0.9574197  -0.08335346\n",
      " -0.48198879 -0.5536614  -0.98775457 -0.08539948 -0.94204546 -0.95168793\n",
      " -0.73037817 -0.46612319 -0.86166518 -0.4534784  -0.92992349 -0.90057204\n",
      " -0.33615728 -0.08305305 -0.83206053 -0.89164019 -0.25443168 -0.94871417\n",
      " -0.23341204 -0.08063802 -0.1909288  -0.00667479 -0.16687155 -0.88317056\n",
      " -0.59737388 -0.8388203  -0.61093893 -0.15398968 -0.33127559 -0.63085282\n",
      " -0.08912162 -0.22095022 -0.11399835 -0.75834525 -0.29605141 -0.20426079\n",
      " -0.57195004 -0.57389208 -0.15867368 -0.34515294 -0.52597776 -0.25739456\n",
      " -0.63416128 -0.4352111  -0.41730901 -0.7888028  -0.14724482 -0.80281794\n",
      " -0.84414721 -0.04984484 -0.23943119 -0.383201   -0.54230644 -0.02816464\n",
      " -0.75357648 -0.07868162 -0.66060193 -0.29073017 -0.85983355 -0.5202521\n",
      " -0.19098647 -0.70274664 -0.28479579 -0.33412675 -0.04164547 -0.45584405\n",
      " -0.17474345 -0.39399371 -0.87246768 -0.42851813 -0.75005974 -0.56906473\n",
      " -0.57722859 -0.49238482 -0.54643973 -0.13353961 -0.4180878  -0.90600685\n",
      " -0.18757849 -0.2819325  -0.67773599 -0.2319067  -0.59293716 -0.8275016\n",
      " -0.12165446 -0.66741383 -0.96426876 -0.79326803 -0.08530027 -0.08349289\n",
      " -0.35727065 -0.48116614 -0.10224523 -0.30633511]\n",
      "Z Vector: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Error: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "[[0.00050562 0.00098493 0.00049865 ... 0.0002519  0.00040149 0.00026626]\n",
      " [0.00074803 0.00065912 0.00061857 ... 0.00014891 0.00055887 0.00013192]\n",
      " [0.00080969 0.00094223 0.00081188 ... 0.0005326  0.00082298 0.0004711 ]\n",
      " ...\n",
      " [0.00076762 0.00068887 0.00078579 ... 0.00052005 0.00056432 0.00093561]\n",
      " [0.00074445 0.00016981 0.00036764 ... 0.00071608 0.0006672  0.00024794]\n",
      " [0.00060755 0.00046796 0.00049331 ... 0.00049514 0.00031633 0.00076268]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.47610589 0.43620887 0.52917595 0.57624136 0.48506189 0.54195002\n",
      " 0.38818918 0.61824245 0.45534349 0.54389445 0.63449886 0.43134926\n",
      " 0.55037304 0.4968489  0.38139929 0.56154978]\n",
      "Biases: \n",
      "[-0.65222995 -0.76254041 -0.48943213 -0.3254445  -0.61981152 -0.44317058\n",
      " -0.90682128 -0.17972971 -0.71450105 -0.43810933 -0.11261221 -0.78647314\n",
      " -0.41598723 -0.58572148 -0.93559248 -0.37556282]\n",
      "Z Vector: \n",
      "[-0.0956493  -0.25656265  0.11683651  0.30736257 -0.05977023  0.16819546\n",
      " -0.4549303   0.48209498 -0.17910328  0.17603097  0.5515661  -0.27634832\n",
      "  0.20217803 -0.01260456 -0.48361317  0.24745414]\n",
      "Error: \n",
      "[-4.48240294e-06 -4.31530826e-06 -4.56445389e-06 -4.50841814e-06\n",
      " -4.50351458e-06 -4.55349917e-06 -3.94546025e-06 -4.38374156e-06\n",
      " -4.41520409e-06 -4.55406228e-06 -4.30990857e-06 -4.28810443e-06\n",
      " -4.55411241e-06 -4.52506371e-06 -3.92288520e-06 -4.53446145e-06]\n",
      "\n",
      "[[0.04537146 0.04122915 0.05001677 0.05506134 0.04616725 0.0514962\n",
      "  0.03698744 0.05871672 0.04377016 0.05202089 0.06056614 0.04109077\n",
      "  0.05248551 0.04728506 0.03669916 0.05355499]\n",
      " [0.04511359 0.04120856 0.05003906 0.05408353 0.04523016 0.05070655\n",
      "  0.03629863 0.05844561 0.04317558 0.05093791 0.05982396 0.04105015\n",
      "  0.05148002 0.04644147 0.03654792 0.05227526]\n",
      " [0.02941576 0.0277237  0.03286295 0.0361034  0.03019058 0.03423035\n",
      "  0.02429868 0.03899727 0.02878196 0.03367262 0.03955794 0.02709313\n",
      "  0.03434279 0.03081912 0.02410686 0.03516633]\n",
      " [0.04819013 0.04381208 0.05373246 0.05859338 0.04876924 0.05452396\n",
      "  0.03969445 0.06281908 0.04596193 0.05436768 0.06427161 0.04370167\n",
      "  0.05538422 0.05018406 0.03924763 0.05647931]\n",
      " [0.05658714 0.05212586 0.06347368 0.06887755 0.05735758 0.06438914\n",
      "  0.04696357 0.07441629 0.05475605 0.06443165 0.07697322 0.05170581\n",
      "  0.06537679 0.05934921 0.0460161  0.06747474]\n",
      " [0.0531548  0.04900018 0.0590975  0.06467685 0.05412587 0.06078279\n",
      "  0.04348026 0.07008225 0.05111485 0.06083443 0.0726159  0.04877508\n",
      "  0.0614891  0.05539777 0.04385693 0.06324657]\n",
      " [0.0510945  0.04790057 0.05784947 0.06286376 0.05242854 0.05916643\n",
      "  0.04189054 0.06758613 0.04913101 0.05938035 0.06923406 0.04733098\n",
      "  0.06004951 0.05339882 0.04149234 0.06114121]\n",
      " [0.03845534 0.03542182 0.0425749  0.04671844 0.03946405 0.04344528\n",
      "  0.03170911 0.05047506 0.0373314  0.04445895 0.05202425 0.03522167\n",
      "  0.04402984 0.03981025 0.03077206 0.04539921]\n",
      " [0.05038317 0.04674153 0.05578653 0.06080007 0.0513185  0.05777389\n",
      "  0.04098142 0.06625004 0.04856643 0.05785921 0.06792687 0.04549108\n",
      "  0.05865169 0.05262836 0.04095485 0.0593603 ]\n",
      " [0.0545091  0.04925801 0.06033059 0.06573912 0.05457618 0.06180017\n",
      "  0.04480941 0.07070552 0.05176826 0.06181541 0.07247269 0.04920204\n",
      "  0.06250561 0.05660962 0.04431921 0.06382126]\n",
      " [0.05169993 0.04746226 0.05752409 0.06292946 0.05316337 0.05934318\n",
      "  0.04204627 0.06743814 0.0492173  0.05917831 0.06990727 0.04694826\n",
      "  0.05947213 0.0536751  0.04247701 0.06132558]\n",
      " [0.03273699 0.02984803 0.0363891  0.03934474 0.03332369 0.03726888\n",
      "  0.02681423 0.04246654 0.03149418 0.03668369 0.04341335 0.02975881\n",
      "  0.03764262 0.03389444 0.02615219 0.03852443]\n",
      " [0.04833403 0.04458867 0.05430845 0.05844842 0.04926365 0.05562769\n",
      "  0.03927992 0.0638048  0.04643385 0.05509063 0.06573538 0.0437714\n",
      "  0.05618246 0.05010205 0.03977044 0.05764878]\n",
      " [0.05856592 0.05442771 0.06548316 0.07151214 0.0603348  0.06699328\n",
      "  0.04806006 0.07699646 0.05633122 0.06711276 0.08005581 0.0535494\n",
      "  0.06799443 0.06099918 0.0477194  0.06966015]\n",
      " [0.05596813 0.05180371 0.06206082 0.06780488 0.0564133  0.06354688\n",
      "  0.04626024 0.07303395 0.05384887 0.06413286 0.07531359 0.05083319\n",
      "  0.0648102  0.05811474 0.04542037 0.06569935]\n",
      " [0.03387479 0.03105436 0.03729233 0.04081852 0.0340902  0.03765408\n",
      "  0.02750665 0.04332907 0.03201277 0.03848366 0.04478192 0.03073429\n",
      "  0.03901951 0.03526587 0.0274772  0.03963871]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.49316638 0.48732203 0.34552005 0.52191175 0.6206715  0.58088094\n",
      " 0.55923646 0.4287437  0.54776609 0.587717   0.56231315 0.37094855\n",
      " 0.52657509 0.65181498 0.60907992 0.3823807 ]\n",
      "Biases: \n",
      "[-0.42690917 -0.44519778 -0.90118273 -0.33623177 -0.00954408 -0.1453208\n",
      " -0.21828984 -0.62701676 -0.25404306 -0.12345403 -0.20667245 -0.81555034\n",
      " -0.32227096  0.10661538 -0.05049959 -0.77570851]\n",
      "Z Vector: \n",
      "[-0.02733618 -0.05072275 -0.63879026  0.08770318  0.49239938  0.32639079\n",
      "  0.23806386 -0.28697865  0.19164882  0.35453553  0.25055521 -0.52814966\n",
      "  0.10640064  0.62702677  0.44344634 -0.47945553]\n",
      "Error: \n",
      "[-2.48865443e-05 -2.46409873e-05 -1.70529346e-05 -2.59919415e-05\n",
      " -2.83268197e-05 -2.76636688e-05 -2.71389739e-05 -2.18699935e-05\n",
      " -2.68260095e-05 -2.78042573e-05 -2.72249678e-05 -1.86133966e-05\n",
      " -2.61493707e-05 -2.85727434e-05 -2.81710594e-05 -1.92975279e-05]\n",
      "\n",
      "[[-0.34496461 -0.34151519 -0.25468929 -0.36438217 -0.43434251 -0.40498485\n",
      "  -0.38953968 -0.30512831 -0.38120437 -0.40952495 -0.39137116 -0.27025642\n",
      "  -0.36757098 -0.45994949 -0.42534479 -0.27743543]\n",
      " [-0.39845442 -0.39359953 -0.29466386 -0.42011552 -0.49997345 -0.46603773\n",
      "  -0.44820353 -0.35227782 -0.43961711 -0.47129231 -0.45120592 -0.3127887\n",
      "  -0.42327164 -0.52932598 -0.48975735 -0.32019381]\n",
      " [-0.39356089 -0.38921142 -0.29128541 -0.41458182 -0.49465363 -0.45986699\n",
      "  -0.44311097 -0.34804838 -0.43426105 -0.46536017 -0.44537431 -0.30848124\n",
      "  -0.41835104 -0.52267962 -0.4841258  -0.31634828]\n",
      " [-0.34512376 -0.3409762  -0.25368786 -0.36323349 -0.43335048 -0.40396257\n",
      "  -0.38899689 -0.30425571 -0.38138479 -0.40905774 -0.39082276 -0.26917\n",
      "  -0.3661485  -0.45858992 -0.42467915 -0.27620732]\n",
      " [-0.46261982 -0.45777642 -0.34304306 -0.48715331 -0.58026262 -0.54067617\n",
      "  -0.51975248 -0.40988035 -0.51046507 -0.54763792 -0.52385465 -0.363498\n",
      "  -0.49140257 -0.61472887 -0.56877058 -0.3726647 ]\n",
      " [-0.37666894 -0.37280498 -0.27836472 -0.39721711 -0.47336562 -0.44064602\n",
      "  -0.42376029 -0.3332229  -0.41621278 -0.44622796 -0.42668641 -0.294735\n",
      "  -0.40047029 -0.50070855 -0.46328438 -0.30242746]\n",
      " [-0.36292934 -0.35898136 -0.2679264  -0.38271556 -0.45638724 -0.42510371\n",
      "  -0.40953199 -0.32150416 -0.40061484 -0.43079643 -0.41154042 -0.28456205\n",
      "  -0.38597107 -0.48271999 -0.44719013 -0.29199567]\n",
      " [-0.44247348 -0.43784079 -0.32863067 -0.4660701  -0.55614524 -0.51799994\n",
      "  -0.49808088 -0.39202664 -0.48791963 -0.5243973  -0.5010689  -0.34790096\n",
      "  -0.46975655 -0.58795833 -0.54451095 -0.35713825]\n",
      " [-0.46554427 -0.46144813 -0.34608613 -0.49114055 -0.58517363 -0.54527156\n",
      "  -0.52454321 -0.41326307 -0.51387645 -0.55173048 -0.527208   -0.36616443\n",
      "  -0.49474436 -0.61932211 -0.57293846 -0.37607402]\n",
      " [ 0.56173889  0.55595422  0.41760539  0.59130555  0.70464488  0.6561634\n",
      "   0.63147013  0.49835298  0.6190747   0.66421134  0.63507799  0.44208703\n",
      "   0.59634333  0.74584185  0.6900806   0.45336347]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[0.00802302 0.00635972 0.00653907 0.00803808 0.00420226 0.00712715\n",
      " 0.00755543 0.00480155 0.00409981 0.99699658]\n",
      "Biases: \n",
      "[-1.72082746 -1.48309538 -1.49838065 -1.72535134 -1.32403254 -1.56157225\n",
      " -1.62307598 -1.36783102 -1.31686091  0.7743994 ]\n",
      "Z Vector: \n",
      "[-4.81738511 -5.05139087 -5.02340026 -4.81549468 -5.46792138 -4.93669107\n",
      " -4.87790404 -5.33400286 -5.49270521  5.80499465]\n",
      "Error: \n",
      "[ 4.52157472e-05  2.68952660e-05  2.85769217e-05  4.54189132e-05\n",
      "  1.09880203e-05  3.45449241e-05  3.93685975e-05  1.46416738e-05\n",
      "  1.04205556e-05 -5.07184513e-06]\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "# net = Network(4, 3, 3, 2)\n",
    "net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = True\n",
    "\n",
    "if training:\n",
    "    # #\n",
    "    # # 4 3 3 2 Test run\n",
    "    # #\n",
    "\n",
    "    # dummyData = [[0.125, 0.25, 0.675, 0.885]]\n",
    "    # dummyLabel = [1]\n",
    "\n",
    "    # for i in range(100):\n",
    "    #     net.trainBatch(dummyData, dummyLabel, 5)\n",
    "    #     net.coutLastLayer()\n",
    "    \n",
    "    # net.test(dummyData, dummyLabel)\n",
    "\n",
    "    # #\n",
    "    # # Training on the actual data\n",
    "    # #\n",
    "\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_01 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_05 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_05 = pickle.load(infile)\n",
    "\n",
    "    # with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_first_100 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    print(\"Testing on the batch\")\n",
    "    for i in range(100):\n",
    "        # Number in one batch is 6000 so a learning rate of 6000 will result in an npm of 1\n",
    "        net.trainBatch(data_batch_01, label_batch_01, 60000)\n",
    "        net.coutLastLayer()\n",
    "        # net.cout()\n",
    "        \n",
    "        # net.test(data_batch_01, label_batch_01)\n",
    "        #net.test(data_test, label_test)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "    # #\n",
    "    # #\n",
    "    # ### CHECKING\n",
    "    # #\n",
    "    # #\n",
    "\n",
    "    # \n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    net.test(data_test, label_test)\n",
    "    net.checkRandomExamples(data_batch_01, label_batch_01)\n",
    "\n",
    "    net.cout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
