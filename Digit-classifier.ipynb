{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 0.1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(-1.0, 0.0) for i in range(currentLayerLen)])\n",
    "        #self.biasVector = np.zeros(currentLayerLen)\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetLayer(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "        self.zVector = np.zeros(self.zVector.size)\n",
    "        self.errorVector = np.zeros(self.errorVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen) * 0.001\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    if x <= -710:\n",
    "        return 0.0\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # # FOR 4 3 3 2 test\n",
    "\n",
    "        # for idx in range(0, 4):\n",
    "        #     self.Layers[0].activationVector[idx] = dataset[idx]\n",
    "\n",
    "        #\n",
    "        # FOR MINST\n",
    "        #\n",
    "\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += (1.0 - self.Layers[self.Layers.size - 1].activationVector[i]) * (1.0 - self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "            else:\n",
    "                sum += (self.Layers[self.Layers.size - 1].activationVector[i]) * (self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "        return sum / (self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        # The desired value for target is 1\n",
    "        CGradient = np.zeros(self.Layers[self.Layers.size - 1].size)\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            CGradient[i] = self.Layers[self.Layers.size - 1].activationVector[i]\n",
    "            if i == target:\n",
    "                CGradient[i] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def resetNetwork(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetLayer()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nRESET NETWORK\\n\\n\")\n",
    "            self.resetNetwork()\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nSET START LAYER ACTIVATIONS\\n\\n\")\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL FORWARD PROPAGATION\\n\\n\")\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL BACKWARD PROPAGATION\\n\\n\")\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nADJUST BASED ON GRADIENT DESCENT FOR CURRENT EXAMPLE\\n\\n\")\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "        #     self.cout()\n",
    "\n",
    "        # print(\"\\n\\nADJUST WITH ADJUST VARIABLES\\n\\n\")\n",
    "        self.adjustWithAdjustVariables()\n",
    "        # self.cout()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[self.Layers.size - 1].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def checkRandomExamples(self, data, labels):\n",
    "        numberOfData = len(data)\n",
    "\n",
    "        for num in range (0, 20):\n",
    "            randIdx = random.randint(0, numberOfData)\n",
    "            \n",
    "            self.setStartLayerActivations(data[randIdx])\n",
    "            self.fullForwardPropagation(labels[randIdx])\n",
    "\n",
    "            print(\"Label for the data is: \" + str(labels[randIdx]))\n",
    "            self.coutLastLayer()\n",
    "            print(\"The cost is: \" + str(self.cost(labels[randIdx])))\n",
    "\n",
    "    def coutLastLayer(self):\n",
    "        print(\"The last layer activations are: \")\n",
    "        print(self.Layers[self.Layers.size - 1].activationVector)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on the batch\n",
      "The last layer activations are: \n",
      "[0.30591497 0.3955885  0.38828481 0.30468098 0.479464   0.36209484\n",
      " 0.33898756 0.45473099 0.4841126  0.32541455]\n",
      "The last layer activations are: \n",
      "[0.04309991 0.02308545 0.02429242 0.04345276 0.01359778 0.02922144\n",
      " 0.03438707 0.01566102 0.01326649 0.98869299]\n",
      "The last layer activations are: \n",
      "[0.03987736 0.02209266 0.02321956 0.04017063 0.01307063 0.02776047\n",
      " 0.03239646 0.01505299 0.01275166 0.98919741]\n",
      "The last layer activations are: \n",
      "[0.03721422 0.02121376 0.02227098 0.0374638  0.0126048  0.02648034\n",
      " 0.03068056 0.01451467 0.01229687 0.989639  ]\n",
      "The last layer activations are: \n",
      "[0.03496302 0.02042434 0.02142021 0.03517935 0.01218595 0.02534271\n",
      " 0.02917829 0.01403007 0.01188805 0.9900329 ]\n",
      "The last layer activations are: \n",
      "[0.03302573 0.0197073  0.02064866 0.03321603 0.01180435 0.02432034\n",
      " 0.02784653 0.01358828 0.01151566 0.9903893 ]\n",
      "The last layer activations are: \n",
      "[0.03133429 0.0190502  0.01994275 0.03150374 0.01145312 0.02339316\n",
      " 0.02665373 0.01318153 0.01117292 0.99071538]\n",
      "The last layer activations are: \n",
      "[0.02983978 0.0184437  0.01929227 0.0299922  0.0111272  0.02254599\n",
      " 0.02557618 0.0128041  0.0108549  0.99101636]\n",
      "The last layer activations are: \n",
      "[0.02850601 0.01788058 0.01868928 0.0286443  0.01082279 0.021767\n",
      " 0.02459563 0.01245165 0.01055787 0.99129614]\n",
      "The last layer activations are: \n",
      "[0.02730556 0.01735518 0.01812756 0.02743196 0.01053696 0.02104686\n",
      " 0.02369778 0.01212085 0.01027898 0.9915577 ]\n",
      "The last layer activations are: \n",
      "[0.02621721 0.01686293 0.01760209 0.02633349 0.01026742 0.02037805\n",
      " 0.02287122 0.01180905 0.01001596 0.99180339]\n",
      "The last layer activations are: \n",
      "[0.02522428 0.01640012 0.01710875 0.02533186 0.01001233 0.01975443\n",
      " 0.02210671 0.01151414 0.00976704 0.99203505]\n",
      "The last layer activations are: \n",
      "[0.02431343 0.01596367 0.01664417 0.02441345 0.0097702  0.01917092\n",
      " 0.02139667 0.01123437 0.00953074 0.9922542 ]\n",
      "The last layer activations are: \n",
      "[0.02347383 0.01555101 0.01620549 0.02356725 0.00953978 0.01862328\n",
      " 0.02073484 0.01096832 0.00930587 0.99246206]\n",
      "The last layer activations are: \n",
      "[0.02269663 0.01515996 0.01579031 0.02278422 0.00932007 0.01810791\n",
      " 0.02011597 0.01071479 0.00909142 0.99265965]\n",
      "The last layer activations are: \n",
      "[0.02197447 0.01478865 0.01539657 0.0220569  0.00911019 0.01762176\n",
      " 0.01953559 0.01047276 0.00888656 0.99284784]\n",
      "The last layer activations are: \n",
      "[0.0213012  0.01443549 0.01502249 0.02137903 0.0089094  0.0171622\n",
      " 0.01898993 0.01024136 0.00869056 0.99302737]\n",
      "The last layer activations are: \n",
      "[0.02067161 0.01409906 0.01466651 0.02074532 0.00871705 0.01672693\n",
      " 0.01847573 0.01001983 0.00850279 0.99319886]\n",
      "The last layer activations are: \n",
      "[0.02008128 0.01377813 0.01432729 0.02015127 0.00853259 0.01631398\n",
      " 0.01799017 0.00980752 0.00832271 0.99336288]\n",
      "The last layer activations are: \n",
      "[0.0195264  0.01347162 0.01400361 0.01959302 0.00835553 0.01592158\n",
      " 0.0175308  0.00960383 0.00814984 0.99351991]\n",
      "The last layer activations are: \n",
      "[0.01900369 0.01317855 0.01369441 0.01906725 0.00818542 0.01554819\n",
      " 0.01709544 0.00940824 0.00798375 0.99367037]\n",
      "The last layer activations are: \n",
      "[0.01851027 0.01289806 0.01339874 0.01857104 0.00802188 0.01519243\n",
      " 0.01668221 0.00922029 0.00782406 0.99381466]\n",
      "The last layer activations are: \n",
      "[0.01804365 0.01262936 0.01311572 0.01810187 0.00786453 0.01485307\n",
      " 0.01628941 0.00903956 0.00767042 0.99395313]\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "# net = Network(4, 3, 3, 2)\n",
    "net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = True\n",
    "\n",
    "if training:\n",
    "    # #\n",
    "    # # 4 3 3 2 Test run\n",
    "    # #\n",
    "\n",
    "    # dummyData = [[0.125, 0.25, 0.675, 0.885]]\n",
    "    # dummyLabel = [1]\n",
    "\n",
    "    # for i in range(100):\n",
    "    #     net.trainBatch(dummyData, dummyLabel, 5)\n",
    "    #     net.coutLastLayer()\n",
    "    \n",
    "    # net.test(dummyData, dummyLabel)\n",
    "\n",
    "    # #\n",
    "    # # Training on the actual data\n",
    "    # #\n",
    "\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_01 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_05 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_05 = pickle.load(infile)\n",
    "\n",
    "    # with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_first_100 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    print(\"Testing on the batch\")\n",
    "    for i in range(100):\n",
    "        # Number in one batch is 6000 so a learning rate of 6000 will result in an npm of 1\n",
    "        net.trainBatch(data_batch_01, label_batch_01, 60000)\n",
    "        net.coutLastLayer()\n",
    "        # net.cout()\n",
    "        \n",
    "        # net.test(data_batch_01, label_batch_01)\n",
    "        #net.test(data_test, label_test)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "    # #\n",
    "    # #\n",
    "    # ### CHECKING\n",
    "    # #\n",
    "    # #\n",
    "\n",
    "    # \n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    net.test(data_test, label_test)\n",
    "    net.checkRandomExamples(data_batch_01, label_batch_01)\n",
    "\n",
    "    net.cout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
