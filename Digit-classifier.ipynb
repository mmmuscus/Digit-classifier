{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 0.1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(-1.0, 0.0) for i in range(currentLayerLen)])\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetLayer(self):\n",
    "        self.zVector = np.zeros(self.zVector.size)\n",
    "        self.errorVector = np.zeros(self.errorVector.size)\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen) * 0.001\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x <= -700:\n",
    "        x = -700\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    if x <= -350:\n",
    "        x = -350\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    # The class is currently set up to only support exactly 4 deep neural networks\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        # Apply sigmoid in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        # Dataset is in 2D array but the first layer expects a vector\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += (1.0 - self.Layers[self.Layers.size - 1].activationVector[i]) * (1.0 - self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "            else:\n",
    "                sum += (self.Layers[self.Layers.size - 1].activationVector[i]) * (self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "        return sum / (self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        # Iterate error back from last layer\n",
    "        for idx in range(self.Layers.size - 2, 0, -1):\n",
    "            self.calculateErrorFromNextLayerError(idx)\n",
    "    \n",
    "    # The cost function is hard coded, and it is quadratic\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        # Creating the CGradient vector based on hard coded cost function\n",
    "        CGradient = np.zeros(self.Layers[self.Layers.size - 1].size)\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            CGradient[i] = self.Layers[self.Layers.size - 1].activationVector[i]\n",
    "            # The desired value for target is 1\n",
    "            if i == target:\n",
    "                CGradient[i] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            # We will subtract the adjBiasVector from the corresponding bias vector in the adjustWithAdjustVariables function\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "            \n",
    "            # We will subtract the adjMatrix from the corresponding weight matrix in the adjustWithAdjustVariables function\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch between data and labels\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberInBatch = len(data)\n",
    "        self.resetAdjs()\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            self.resetNetwork()\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "\n",
    "        self.adjustWithAdjustVariables()\n",
    "\n",
    "    # Reset everything in network EXCEPT for adj biases and matrices\n",
    "    def resetNetwork(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetLayer()\n",
    "\n",
    "    # Reset the adj biases and matrices\n",
    "    def resetAdjs(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "    \n",
    "    # Adjust biases and matrices with the calculated values\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[self.Layers.size - 1].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 0\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    # Test network, find average cost and percentage of correct predictions\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch between data and label\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    # Get random examples and print expected value and the activation of the last layer\n",
    "    def checkRandomExamples(self, data, labels):\n",
    "        numberOfData = len(data)\n",
    "\n",
    "        for i in range (20):\n",
    "            randIdx = random.randint(0, numberOfData)\n",
    "            \n",
    "            self.setStartLayerActivations(data[randIdx])\n",
    "            self.fullForwardPropagation(labels[randIdx])\n",
    "\n",
    "            print(\"Label for the data is: \" + str(labels[randIdx]))\n",
    "            self.coutLastLayer()\n",
    "            print(\"The cost is: \" + str(self.cost(labels[randIdx])))\n",
    "\n",
    "    # Prints last layer activations with its labels\n",
    "    def coutLastLayer(self):\n",
    "        print(\"The last layer activations are: \")\n",
    "        for idx in range(self.Layers[self.Layers.size - 1].activationVector.size):\n",
    "            print(idx, \":\", self.Layers[self.Layers.size - 1].activationVector[idx])\n",
    "            \n",
    "    # Print activations of each layer\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    # Print activations, biases and matrices\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    # Print all values in network\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cost is:  0.012255575622968024\n",
      "Percentage of correct is:  0.9279\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "0 :  1.3881602052622304e-07\n",
      "1 :  0.0001782682501283659\n",
      "2 :  1.2192423108938535e-05\n",
      "3 :  0.0003585852282618038\n",
      "4 :  1.59537602093823e-06\n",
      "5 :  0.0015865740599436168\n",
      "6 :  4.317048237297486e-10\n",
      "7 :  1.1266382298286316e-07\n",
      "8 :  0.9903980654757699\n",
      "9 :  0.010965430752803129\n",
      "The cost is: 2.1511554961710864e-05\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "0 :  2.7187972168345945e-12\n",
      "1 :  0.9999999843258238\n",
      "2 :  8.034761065176947e-12\n",
      "3 :  2.4294368951400196e-10\n",
      "4 :  1.0340316624006663e-08\n",
      "5 :  1.9081072144300928e-13\n",
      "6 :  5.007160336903986e-11\n",
      "7 :  1.8672417182564337e-08\n",
      "8 :  2.887304807219824e-06\n",
      "9 :  8.450393258698449e-10\n",
      "The cost is: 8.337231086597177e-13\n",
      "Label for the data is: 9\n",
      "The last layer activations are: \n",
      "0 :  5.768991041779938e-07\n",
      "1 :  0.0005216845676581154\n",
      "2 :  0.00039616233640026553\n",
      "3 :  8.43769415220257e-06\n",
      "4 :  0.01392137773800689\n",
      "5 :  0.014609276113992216\n",
      "6 :  1.54482301058884e-11\n",
      "7 :  0.01202833995370969\n",
      "8 :  2.3142281081565083e-05\n",
      "9 :  0.9797157961406845\n",
      "The cost is: 9.637953014250002e-05\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "0 :  4.0248232655108155e-07\n",
      "1 :  0.0008784611816173341\n",
      "2 :  0.00024282904620089466\n",
      "3 :  1.5942463212974497e-10\n",
      "4 :  0.9981335964030829\n",
      "5 :  6.323399958455695e-06\n",
      "6 :  0.003158706533678907\n",
      "7 :  7.584641861579262e-06\n",
      "8 :  6.420750361916535e-06\n",
      "9 :  0.00011737037727733417\n",
      "The cost is: 1.4305464051447107e-06\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "0 :  0.0027353865833455782\n",
      "1 :  0.00043009672370235237\n",
      "2 :  0.019478621870634885\n",
      "3 :  0.9968175277131514\n",
      "4 :  3.4866455057883564e-08\n",
      "5 :  0.0003639578956731951\n",
      "6 :  3.5574575719523395e-07\n",
      "7 :  0.0025297822023470124\n",
      "8 :  0.010668128902151046\n",
      "9 :  1.095714594408595e-05\n",
      "The cost is: 5.175535205886849e-05\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "0 :  9.336072387574566e-08\n",
      "1 :  0.0007939690723007129\n",
      "2 :  0.00011042259924528091\n",
      "3 :  7.52143906932547e-11\n",
      "4 :  0.9992678106594377\n",
      "5 :  5.9575622380733564e-05\n",
      "6 :  0.0016152337346898094\n",
      "7 :  4.2759252983068754e-07\n",
      "8 :  9.375271384036494e-06\n",
      "9 :  0.0010372331669412894\n",
      "The cost is: 4.86715127095741e-07\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "0 :  0.0026715933971075377\n",
      "1 :  0.00018931768462910145\n",
      "2 :  0.09409428696313517\n",
      "3 :  0.004911046388963066\n",
      "4 :  5.473792000132606e-06\n",
      "5 :  0.0014577100469766223\n",
      "6 :  6.024488792967122e-08\n",
      "7 :  0.6375983516466313\n",
      "8 :  0.00019370287762141204\n",
      "9 :  0.00010584546558004584\n",
      "The cost is: 0.014022215487005376\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "0 :  1.6287317082693108e-05\n",
      "1 :  0.00015129720815582276\n",
      "2 :  0.9928055494072223\n",
      "3 :  0.00021360039808992492\n",
      "4 :  1.6141122632885092e-06\n",
      "5 :  0.0033709566926054587\n",
      "6 :  0.0036952942144109135\n",
      "7 :  0.0001810237117159491\n",
      "8 :  0.0009526738823003571\n",
      "9 :  1.8323976051864377e-07\n",
      "The cost is: 7.778780868751295e-06\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "0 :  3.923159102309786e-05\n",
      "1 :  0.001575052381158559\n",
      "2 :  0.008660529292524888\n",
      "3 :  0.0015129253634385289\n",
      "4 :  0.0011044949051078202\n",
      "5 :  1.4922346748608134e-06\n",
      "6 :  0.8685818294530061\n",
      "7 :  5.8968895289081446e-08\n",
      "8 :  0.0026884363853350384\n",
      "9 :  1.033732399008031e-09\n",
      "The cost is: 0.17523136282870422\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "0 :  2.1487132957269757e-08\n",
      "1 :  0.00024313830924129317\n",
      "2 :  4.172649030941915e-06\n",
      "3 :  0.0009586148441849565\n",
      "4 :  1.5435976684999315e-06\n",
      "5 :  0.00013553752816172163\n",
      "6 :  2.155058866077123e-10\n",
      "7 :  3.11763813256368e-08\n",
      "8 :  0.997259040478934\n",
      "9 :  0.01903043107594183\n",
      "The cost is: 3.70666614905875e-05\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "0 :  1.2952607764999543e-06\n",
      "1 :  0.0006874885052539905\n",
      "2 :  8.151752940407829e-09\n",
      "3 :  4.88226024261031e-10\n",
      "4 :  0.9866817762191548\n",
      "5 :  5.810480285489603e-06\n",
      "6 :  7.525239155535606e-08\n",
      "7 :  4.693704639935217e-06\n",
      "8 :  0.009593548397795245\n",
      "9 :  0.01191341986326724\n",
      "The cost is: 4.1181352629683263e-05\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "0 :  2.90060139643009e-06\n",
      "1 :  0.02565222153709722\n",
      "2 :  1.625228139168013e-07\n",
      "3 :  1.1467084919078948e-05\n",
      "4 :  2.1914733906508174e-05\n",
      "5 :  4.275100360306456e-09\n",
      "6 :  0.00028139906581577145\n",
      "7 :  2.579072169992242e-08\n",
      "8 :  0.9546088412253319\n",
      "9 :  8.395853011211189e-10\n",
      "The cost is: 0.00027184735703198755\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "0 :  1.29721664993958e-06\n",
      "1 :  0.0007053257715984701\n",
      "2 :  4.5491258089258924e-05\n",
      "3 :  0.049467741399640726\n",
      "4 :  3.4494379217028015e-08\n",
      "5 :  1.2775218436661919e-08\n",
      "6 :  2.4105329508962366e-09\n",
      "7 :  0.0002482806486095033\n",
      "8 :  0.9910677933048103\n",
      "9 :  7.580670657127637e-06\n",
      "The cost is: 0.0002527403011957135\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "0 :  0.00022373911501958357\n",
      "1 :  0.0008836208372375472\n",
      "2 :  4.148708114694727e-09\n",
      "3 :  1.6385609441361032e-07\n",
      "4 :  0.00047720377542324687\n",
      "5 :  0.00024896989567711677\n",
      "6 :  4.936726875759219e-06\n",
      "7 :  9.305996716777815e-09\n",
      "8 :  0.9857841957443313\n",
      "9 :  2.575576235824867e-07\n",
      "The cost is: 2.0320966952787116e-05\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "0 :  0.0001664028352092441\n",
      "1 :  0.0003647743743794133\n",
      "2 :  0.0037122899969633267\n",
      "3 :  0.007155193818279114\n",
      "4 :  2.318290621684862e-06\n",
      "5 :  0.9905118607328535\n",
      "6 :  7.462398145476162e-08\n",
      "7 :  1.952244877004375e-06\n",
      "8 :  0.0028624989062668673\n",
      "9 :  0.004633219381478872\n",
      "The cost is: 1.8482406361582184e-05\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "0 :  0.9670203713749826\n",
      "1 :  0.00023091550714735205\n",
      "2 :  2.4196884658285883e-05\n",
      "3 :  0.004094236419875178\n",
      "4 :  0.00027291568659684637\n",
      "5 :  0.013543307298575866\n",
      "6 :  0.022681393676488745\n",
      "7 :  0.00014556507147785194\n",
      "8 :  0.0049597568336225825\n",
      "9 :  9.32787023191063e-10\n",
      "The cost is: 0.0001827034235268778\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "0 :  0.0003366369326046118\n",
      "1 :  0.00038824049641500636\n",
      "2 :  0.012128692713429154\n",
      "3 :  0.00040415426816930436\n",
      "4 :  0.00024600667204629416\n",
      "5 :  2.5315164849081618e-05\n",
      "6 :  4.761017154747008e-10\n",
      "7 :  0.9943541102910046\n",
      "8 :  1.2699604605290358e-05\n",
      "9 :  0.010397092441819395\n",
      "The cost is: 2.8756950598681684e-05\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "0 :  0.988647838967056\n",
      "1 :  0.0001814529441390025\n",
      "2 :  1.3286810748321747e-05\n",
      "3 :  0.008131109749657816\n",
      "4 :  0.00021790123999971918\n",
      "5 :  0.022394784887177034\n",
      "6 :  0.00969969004291309\n",
      "7 :  0.00030569209259668687\n",
      "8 :  0.006435068867289777\n",
      "9 :  1.258551273775731e-09\n",
      "The cost is: 8.32181024593303e-05\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "0 :  1.1022843666394383e-13\n",
      "1 :  0.999999986287847\n",
      "2 :  6.652590027196878e-11\n",
      "3 :  7.908104892294108e-10\n",
      "4 :  9.215795244863624e-10\n",
      "5 :  2.4261240759849548e-14\n",
      "6 :  6.862517404559269e-13\n",
      "7 :  2.0549973361399633e-07\n",
      "8 :  3.480524161229884e-06\n",
      "9 :  1.5247332874147955e-08\n",
      "The cost is: 1.2156700560835453e-12\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "0 :  5.573180529796083e-05\n",
      "1 :  0.000690586109082186\n",
      "2 :  0.005985384868068334\n",
      "3 :  0.006241056745695965\n",
      "4 :  0.0002136135789049217\n",
      "5 :  1.8333371934398892e-06\n",
      "6 :  3.3359890322495074e-10\n",
      "7 :  0.9782165262887574\n",
      "8 :  1.9525953797006785e-05\n",
      "9 :  0.021436845052229505\n",
      "The cost is: 0.00010093597046368596\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load network\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "# Load test data and labels\n",
    "with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "    data_test = pickle.load(infile)\n",
    "with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "    label_test = pickle.load(infile)\n",
    "\n",
    "training = False\n",
    "testing = True\n",
    "\n",
    "if training:\n",
    "    # Loading training batches\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "        data_batch_02 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "        label_batch_02 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "        data_batch_03 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "        label_batch_03 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "        data_batch_04 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "        label_batch_04 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "        data_batch_05 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "        label_batch_05 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_06 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_06 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "        data_batch_07 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "        label_batch_07 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "        data_batch_08 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "        label_batch_08 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "        data_batch_09 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "        label_batch_09 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "        data_batch_10 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "        label_batch_10 = pickle.load(infile)\n",
    "    \n",
    "    # Testing before training\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    # Training\n",
    "    for i in range(200):\n",
    "        # Number in one batch is 6000 so a learning rate of 6000 will result in an npm of 1\n",
    "        net.trainBatch(data_batch_01, label_batch_01, 60)\n",
    "        net.trainBatch(data_batch_02, label_batch_02, 60)\n",
    "        net.trainBatch(data_batch_03, label_batch_03, 60)\n",
    "        net.trainBatch(data_batch_04, label_batch_04, 60)\n",
    "        net.trainBatch(data_batch_05, label_batch_05, 60)\n",
    "        net.trainBatch(data_batch_06, label_batch_06, 60)\n",
    "        net.trainBatch(data_batch_07, label_batch_07, 60)\n",
    "        net.trainBatch(data_batch_08, label_batch_08, 60)\n",
    "        net.trainBatch(data_batch_09, label_batch_09, 60)\n",
    "        net.trainBatch(data_batch_10, label_batch_10, 60)\n",
    "\n",
    "    # Testing after training\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    # Write trained network into its file\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "if testing:\n",
    "    # Test network\n",
    "    net.test(data_test, label_test)\n",
    "    # See random examples of label\n",
    "    net.checkRandomExamples(data_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
