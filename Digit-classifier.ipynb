{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook\n",
    "\n",
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook\n",
    "\n",
    "# #\n",
    "# # Verify Reading Dataset via MnistDataloader class\n",
    "# #\n",
    "# %matplotlib inline\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pickle\n",
    "\n",
    "# #\n",
    "# # Set file paths based on added MNIST Datasets\n",
    "# #\n",
    "# input_path = 'dataset/'\n",
    "# training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "# training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "# test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "# test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# #\n",
    "# # Helper function to show a list of images with their relating titles\n",
    "# #\n",
    "# def show_images(images, title_texts):\n",
    "#     cols = 5\n",
    "#     rows = int(len(images)/cols) + 1\n",
    "#     plt.figure(figsize=(30,20))\n",
    "#     index = 1    \n",
    "#     for x in zip(images, title_texts):        \n",
    "#         image = x[0]        \n",
    "#         title_text = x[1]\n",
    "#         plt.subplot(rows, cols, index)        \n",
    "#         plt.imshow(image, cmap=plt.cm.gray)\n",
    "#         if (title_text != ''):\n",
    "#             plt.title(title_text, fontsize = 15);        \n",
    "#         index += 1\n",
    "\n",
    "# #\n",
    "# # Load MINST dataset\n",
    "# #\n",
    "# mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "# (data_train, label_train), (data_test, label_test) = mnist_dataloader.load_data()\n",
    "\n",
    "# with open(\"dataset/pickled/data_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_train, outfile)\n",
    "# with open(\"dataset/pickled/label_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_train, outfile)\n",
    "# with open(\"dataset/pickled/data_test.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_test, outfile)\n",
    "# with open(\"dataset/pickled/label_test.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_test, outfile)\n",
    "\n",
    "# first_10_data_train = []\n",
    "# first_10_label_train = []\n",
    "# for i in range(0, 10):\n",
    "#     first_10_data_train.append(data_train[i])\n",
    "#     first_10_label_train.append(label_train[i])\n",
    "\n",
    "# with open(\"dataset/pickled/first_10_data_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(first_10_data_train, outfile)\n",
    "# with open(\"dataset/pickled/first_10_label_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(first_10_label_train, outfile)\n",
    "\n",
    "# #\n",
    "# # Show some random training and test images \n",
    "# #\n",
    "# images_2_show = []\n",
    "# titles_2_show = []\n",
    "# for i in range(0, 10):\n",
    "#    images_2_show.append(first_10_data_train[i])\n",
    "#    titles_2_show.append('training image [' + str(i) + '] = ' + str(first_10_label_train[i]))    \n",
    "\n",
    "# #for i in range(0, 5):\n",
    "# #   r = random.randint(1, 10000)\n",
    "# #   images_2_show.append(data_test[r])        \n",
    "# #   titles_2_show.append('test image [' + str(r) + '] = ' + str(label_test[r]))    \n",
    "\n",
    "# show_images(images_2_show, titles_2_show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, numberOfIncoming):\n",
    "        self.activation = random.uniform(0, 1)\n",
    "        self.bias = random.uniform(0, 1)\n",
    "        self.incomingWeights = np.array([random.uniform(0, 1) for i in range(numberOfIncoming)])\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activation: \", self.activation)\n",
    "        print(\"Bias: \", self.bias)\n",
    "        print(\"Incoming Weights: \", self.incomingWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        #self.neurons = np.array([Neuron(prevLayerLen) for i in range(currentLayerLen)])\n",
    "        self.activations = np.array([random.uniform(0, 1) for i in range(currentLayerLen)])\n",
    "        self.biases = np.array([random.uniform(0, 1) for i in range(currentLayerLen)])\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activations)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biases)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activations)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen)\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        prevLayer = self.Layers[layerIdx - 1]\n",
    "        currLayer = self.Layers[layerIdx]\n",
    "\n",
    "        # weight matrix is Matrices[layerIdx - 1]\n",
    "        weightMatrix = self.Matrices[layerIdx - 1].matrix\n",
    "\n",
    "        # input activation is the activation of the previous layer\n",
    "        prevActivations = prevLayer.activations\n",
    "        \n",
    "        # output biases is the bias of the current layer\n",
    "        currBias = currLayer.biases\n",
    "\n",
    "        currLayer.zVector = np.dot(weightMatrix, prevActivations)\n",
    "        currLayer.zVector += currBias\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        currLayer = self.Layers[layerIdx]\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(currLayer.size):\n",
    "            currLayer.activations[i] = sigmoid(currLayer.zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        startLayer = self.Layers[0]\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != startLayer.size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(startLayer.size))\n",
    "\n",
    "        print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                startLayer.activations[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        print(\"Target is: \" + str(target))\n",
    "        print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        endLayer = self.Layers[self.Layers.size - 1]\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(endLayer.size):\n",
    "            if (i == target):\n",
    "                sum += pow(endLayer.activations[i] - 1.0, 2)\n",
    "            else:\n",
    "                sum += pow(endLayer.activations[i], 2)\n",
    "        return sum / (2 * endLayer.size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        endLayer = self.Layers[self.Layers.size - 1]\n",
    "\n",
    "        CGradient = endLayer.activations\n",
    "        CGradient[target] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            endLayer.zVector[i] = sigmoidDeriv(endLayer.zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        # TODO: Maybe we can calculate sigmoid' to a temporary vector\n",
    "        endLayer.errorVector = np.multiply(CGradient, endLayer.zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        currLayer = self.Layers[layerIdx]\n",
    "        nextLayer = self.Layers[layerIdx + 1]\n",
    "\n",
    "        # We need the transposition of the weightmatrix\n",
    "        weightMatrix = self.Matrices[layerIdx].matrix.transpose()\n",
    "\n",
    "        weightTimesError = np.dot(weightMatrix, nextLayer.errorVector)\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            currLayer.zVector[i] = sigmoidDeriv(currLayer.zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # TODO: Maybe we can calculate sigmoid' to a temporary vector\n",
    "        currLayer.errorVector = np.multiply(weightTimesError, currLayer.zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        print(\"5) Gradient Descent\")\n",
    "\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        layerIdx = 3\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].neurons[i].bias -= npm * self.Layers[layerIdx].errorVector[i]\n",
    "        layerIdx = 2\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].neurons[i].bias -= npm * self.Layers[layerIdx].errorVector[i]\n",
    "        layerIdx = 1\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].neurons[i].bias -= npm * self.Layers[layerIdx].errorVector[i]\n",
    "\n",
    "        # Adjust weights\n",
    "        layerIdx = 3\n",
    "        for nIdx in range(self.Layers[layerIdx].size):\n",
    "            for wIdx in range(self.Layers[layerIdx].neurons[nIdx].incomingWeights.size):\n",
    "                self.Layers[layerIdx].neurons[nIdx].incomingWeights[wIdx] -= npm * self.Layers[layerIdx].errorVector[i] * self.Layers[layerIdx - 1].neurons[wIdx].activation\n",
    "        layerIdx = 2\n",
    "        for nIdx in range(self.Layers[layerIdx].size):\n",
    "            for wIdx in range(self.Layers[layerIdx].neurons[nIdx].incomingWeights.size):\n",
    "                self.Layers[layerIdx].neurons[nIdx].incomingWeights[wIdx] -= npm * self.Layers[layerIdx].errorVector[i] * self.Layers[layerIdx - 1].neurons[wIdx].activation\n",
    "        layerIdx = 1\n",
    "        for nIdx in range(self.Layers[layerIdx].size):\n",
    "            for wIdx in range(self.Layers[layerIdx].neurons[nIdx].incomingWeights.size):\n",
    "                self.Layers[layerIdx].neurons[nIdx].incomingWeights[wIdx] -= npm * self.Layers[layerIdx].errorVector[i] * self.Layers[layerIdx - 1].neurons[wIdx].activation\n",
    "        \n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: \n",
      "[0.63812718 0.37727305 0.30067157 0.47212682]\n",
      "Biases: \n",
      "[0.61285521 0.33100534 0.64583    0.45346297]\n",
      "\n",
      "[[0.53868438 0.3076507  0.75736853 0.69632286]\n",
      " [0.68977026 0.71179891 0.22317052 0.28986183]\n",
      " [0.02492046 0.60822664 0.65642905 0.65217316]]\n",
      "\n",
      "Activations: \n",
      "[0.0480375  0.07219713 0.9777653 ]\n",
      "Biases: \n",
      "[0.91568093 0.10384257 0.30362663]\n",
      "\n",
      "[[0.55312102 0.54126127 0.17085934]\n",
      " [0.20869274 0.33995442 0.87352611]\n",
      " [0.03462209 0.09109957 0.07698062]]\n",
      "\n",
      "Activations: \n",
      "[0.29231249 0.03376021 0.75274579]\n",
      "Biases: \n",
      "[0.31443663 0.11240763 0.55300343]\n",
      "\n",
      "[[0.56844171 0.92113935 0.72169758]\n",
      " [0.24560663 0.27240178 0.4380934 ]]\n",
      "\n",
      "Activations: \n",
      "[0.49997205 0.70861789]\n",
      "Biases: \n",
      "[0.96829497 0.20958697]\n",
      "2) Feedforward: Compute all activations for all layers\n",
      "Target is: 1\n",
      "Cost is: 0.23755050849900627\n",
      "Activations: \n",
      "[0.63812718 0.37727305 0.30067157 0.47212682]\n",
      "Biases: \n",
      "[0.61285521 0.33100534 0.64583    0.45346297]\n",
      "\n",
      "[[0.53868438 0.3076507  0.75736853 0.69632286]\n",
      " [0.68977026 0.71179891 0.22317052 0.28986183]\n",
      " [0.02492046 0.60822664 0.65642905 0.65217316]]\n",
      "\n",
      "Activations: \n",
      "[0.87346734 0.73429005 0.74159489]\n",
      "Biases: \n",
      "[0.91568093 0.10384257 0.30362663]\n",
      "\n",
      "[[0.55312102 0.54126127 0.17085934]\n",
      " [0.20869274 0.33995442 0.87352611]\n",
      " [0.03462209 0.09109957 0.07698062]]\n",
      "\n",
      "Activations: \n",
      "[0.78946789 0.76712029 0.66978806]\n",
      "Biases: \n",
      "[0.31443663 0.11240763 0.55300343]\n",
      "\n",
      "[[0.56844171 0.92113935 0.72169758]\n",
      " [0.24560663 0.27240178 0.4380934 ]]\n",
      "\n",
      "Activations: \n",
      "[0.93131524 0.71215638]\n",
      "Biases: \n",
      "[0.96829497 0.20958697]\n",
      "3) Output Error in last layer\n",
      "4) Backpropagate error: calculate error for all layers\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'weightMatrix' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m net\u001b[38;5;241m.\u001b[39mfullForwardPropagation(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m net\u001b[38;5;241m.\u001b[39mcoutBase()\n\u001b[1;32m---> 18\u001b[0m net\u001b[38;5;241m.\u001b[39mfullBackwardPropagation(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m net\u001b[38;5;241m.\u001b[39mcout()\n",
      "Cell \u001b[1;32mIn[84], line 90\u001b[0m, in \u001b[0;36mNetwork.fullBackwardPropagation\u001b[1;34m(self, target)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculateErrorInLastLayerForTarget(target)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4) Backpropagate error: calculate error for all layers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculateErrorFromNextLayerError(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculateErrorFromNextLayerError(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[84], line 114\u001b[0m, in \u001b[0;36mNetwork.calculateErrorFromNextLayerError\u001b[1;34m(self, layerIdx)\u001b[0m\n\u001b[0;32m    111\u001b[0m nextLayer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayers[layerIdx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We need the transposition of the weightmatrix\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m weightMatrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMatrices[layerIdx]\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m    116\u001b[0m weightTimesError \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(weightMatrix, nextLayer\u001b[38;5;241m.\u001b[39merrorVector)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Apply sigmoid' to currLayer.zVector in place\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'weightMatrix' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "#net = Network(784, 16, 16, 10)\n",
    "net = Network(4, 3, 3, 2)\n",
    "\n",
    "#with open(\"dataset/pickled/first_10_data_train.pickle\", \"rb\") as infile:\n",
    "#    data_batch = pickle.load(infile)\n",
    "#with open(\"dataset/pickled/first_10_label_train.pickle\", \"rb\") as infile:\n",
    "#    label_batch = pickle.load(infile)\n",
    "\n",
    "#with open(\"network.pickle\", \"rb\") as infile:\n",
    "#   net = pickle.load(infile)\n",
    "\n",
    "net.coutBase()\n",
    "net.fullForwardPropagation(1)\n",
    "net.coutBase()\n",
    "net.fullBackwardPropagation(1)\n",
    "net.cout()\n",
    "# net.adjustBasedOnGradientDescentForCurrentExample(1, 1)\n",
    "# net.coutBase()\n",
    "\n",
    "#with open(\"network.pickle\", \"wb\") as outfile:\n",
    "#    pickle.dump(net, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
