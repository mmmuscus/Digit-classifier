{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(0, 1) for i in range(currentLayerLen)])\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen)\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if (abs(x) > 500):\n",
    "        return 0\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    if (abs(x) > 500):\n",
    "        return 1\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # FOR test\n",
    "\n",
    "        for idx in range(0, 4):\n",
    "            self.Layers[0].activationVector[idx] = dataset[idx]\n",
    "\n",
    "        # #\n",
    "        # # FOR MINST\n",
    "        # #\n",
    "\n",
    "        # # self.Layers[0] is start Layer\n",
    "\n",
    "        # if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "        #     print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "        #     print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "        #     print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        # #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        # layerIdx = 0\n",
    "        # for row in range(0, len(dataset)):\n",
    "        #     for col in range(0, len(dataset[0])):\n",
    "        #         self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "        #         layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += pow(self.Layers[self.Layers.size - 1].activationVector[i] - 1.0, 2)\n",
    "            else:\n",
    "                sum += pow(self.Layers[self.Layers.size - 1].activationVector[i], 2)\n",
    "        return sum / (2 * self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "        self.cout(\"\\n\\n ----- \\n\\n\")\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.cout(\"\\n\\n ----- \\n\\n\")\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "        self.cout(\"\\n\\n ----- \\n\\n\")\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        CGradient = self.Layers[self.Layers.size - 1].activationVector\n",
    "        # The desired value for target is 1\n",
    "        # This subtracts from the activations[target] as well !\n",
    "        # The change of the activations shouldnt matter\n",
    "        # It is not used later in the algo for the training example\n",
    "        CGradient[target] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def clearAdjustVariables(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        self.clearAdjustVariables()\n",
    "\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            print(\"\\n\\n------------------ Set start layer activations ------------------\\n\\n\")\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.cout()\n",
    "            print(\"\\n\\n------------------ Full forward propagation ------------------\\n\\n\")\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            self.cout()\n",
    "            print(\"\\n\\n------------------ Full backward propagation ------------------\\n\\n\")\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            print(\"\\n\\n------------------ Adjust based on gradient descent for current example ------------------\\n\\n\")\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "            self.cout()\n",
    "\n",
    "        self.adjustWithAdjustVariables()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[3].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        # Test for percentage of correct classifications\n",
    "        # numberOfTest = len(data)\n",
    "        # correct = 0\n",
    "\n",
    "        # for idx in range(numberOfTest):\n",
    "        #     self.setStartLayerActivations(data[idx])\n",
    "        #     self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "        #     correctIdx = self.findPrediction()\n",
    "\n",
    "        #     if correctIdx == labels[idx]:\n",
    "        #         correct += 1\n",
    "\n",
    "        # return correct/numberOfTest\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------ Set start layer activations ------------------\n",
      "\n",
      "\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.125 0.25  0.675 0.885]\n",
      "Biases: \n",
      "[0.81017369 0.79850249 0.21358366 0.75343774]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.2186861  0.05628616 0.50158869 0.07855625]\n",
      " [0.24375452 0.97271557 0.1624343  0.15810173]\n",
      " [0.58186105 0.73510785 0.2521241  0.28141627]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.79907755 0.93657437 0.30411115]\n",
      "Biases: \n",
      "[0.65877022 0.31752805 0.54370358]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.3343155  0.09066698 0.39867784]\n",
      " [0.47740735 0.72693575 0.00087981]\n",
      " [0.32778366 0.23349489 0.81914665]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.02441467 0.57682694 0.42887925]\n",
      "Biases: \n",
      "[0.70919512 0.55997546 0.34556808]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.53792913 0.26647645 0.82172938]\n",
      " [0.04437043 0.1676009  0.18768672]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[0.39815912 0.25884777]\n",
      "Biases: \n",
      "[0.46764252 0.39521433]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "------------------ Full forward propagation ------------------\n",
      "\n",
      "\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.125 0.25  0.675 0.885]\n",
      "Biases: \n",
      "[0.81017369 0.79850249 0.21358366 0.75343774]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.2186861  0.05628616 0.50158869 0.07855625]\n",
      " [0.24375452 0.97271557 0.1624343  0.15810173]\n",
      " [0.58186105 0.73510785 0.2521241  0.28141627]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.75180685 0.69862093 0.7719668 ]\n",
      "Biases: \n",
      "[0.65877022 0.31752805 0.54370358]\n",
      "Z Vector: \n",
      "[1.10827217 0.84073945 1.21945034]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.3343155  0.09066698 0.39867784]\n",
      " [0.47740735 0.72693575 0.00087981]\n",
      " [0.32778366 0.23349489 0.81914665]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.79111239 0.80649983 0.80018908]\n",
      "Biases: \n",
      "[0.70919512 0.55997546 0.34556808]\n",
      "Z Vector: \n",
      "[1.33164371 1.4274253  1.38747652]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.53792913 0.26647645 0.82172938]\n",
      " [0.04437043 0.1676009  0.18768672]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[0.85391675 0.67165309]\n",
      "Biases: \n",
      "[0.46764252 0.39521433]\n",
      "Z Vector: \n",
      "[1.76565701 0.71567129]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "------------------ Full backward propagation ------------------\n",
      "\n",
      "\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.125 0.25  0.675 0.885]\n",
      "Biases: \n",
      "[0.81017369 0.79850249 0.21358366 0.75343774]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.2186861  0.05628616 0.50158869 0.07855625]\n",
      " [0.24375452 0.97271557 0.1624343  0.15810173]\n",
      " [0.58186105 0.73510785 0.2521241  0.28141627]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.75180685 0.69862093 0.7719668 ]\n",
      "Biases: \n",
      "[0.65877022 0.31752805 0.54370358]\n",
      "Z Vector: \n",
      "[1.10827217 0.84073945 1.21945034]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.3343155  0.09066698 0.39867784]\n",
      " [0.47740735 0.72693575 0.00087981]\n",
      " [0.32778366 0.23349489 0.81914665]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.79111239 0.80649983 0.80018908]\n",
      "Biases: \n",
      "[0.70919512 0.55997546 0.34556808]\n",
      "Z Vector: \n",
      "[1.33164371 1.4274253  1.38747652]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.53792913 0.26647645 0.82172938]\n",
      " [0.04437043 0.1676009  0.18768672]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[-0.14608325  0.67165309]\n",
      "Biases: \n",
      "[0.46764252 0.39521433]\n",
      "Z Vector: \n",
      "[0.12474294 0.22053522]\n",
      "Error: \n",
      "[-0.01822285  0.14812316]\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.125 0.25  0.675 0.885]\n",
      "Biases: \n",
      "[0.81017369 0.79850249 0.21358366 0.75343774]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.2186861  0.05628616 0.50158869 0.07855625]\n",
      " [0.24375452 0.97271557 0.1624343  0.15810173]\n",
      " [0.58186105 0.73510785 0.2521241  0.28141627]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.75180685 0.69862093 0.7719668 ]\n",
      "Biases: \n",
      "[0.65877022 0.31752805 0.54370358]\n",
      "Z Vector: \n",
      "[1.10827217 0.84073945 1.21945034]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.3343155  0.09066698 0.39867784]\n",
      " [0.47740735 0.72693575 0.00087981]\n",
      " [0.32778366 0.23349489 0.81914665]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.79111239 0.80649983 0.80018908]\n",
      "Biases: \n",
      "[0.70919512 0.55997546 0.34556808]\n",
      "Z Vector: \n",
      "[0.16525357 0.15605785 0.15988652]\n",
      "Error: \n",
      "[-0.00053382  0.00311641  0.00205078]\n",
      "\n",
      "[[0.53792913 0.26647645 0.82172938]\n",
      " [0.04437043 0.1676009  0.18768672]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[-0.14608325  0.67165309]\n",
      "Biases: \n",
      "[0.46764252 0.39521433]\n",
      "Z Vector: \n",
      "[0.12474294 0.22053522]\n",
      "Error: \n",
      "[-0.01822285  0.14812316]\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.125 0.25  0.675 0.885]\n",
      "Biases: \n",
      "[0.81017369 0.79850249 0.21358366 0.75343774]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.2186861  0.05628616 0.50158869 0.07855625]\n",
      " [0.24375452 0.97271557 0.1624343  0.15810173]\n",
      " [0.58186105 0.73510785 0.2521241  0.28141627]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.75180685 0.69862093 0.7719668 ]\n",
      "Biases: \n",
      "[0.65877022 0.31752805 0.54370358]\n",
      "Z Vector: \n",
      "[0.18659331 0.21054973 0.17603406]\n",
      "Error: \n",
      "[0.00036974 0.00056762 0.00025874]\n",
      "\n",
      "[[0.3343155  0.09066698 0.39867784]\n",
      " [0.47740735 0.72693575 0.00087981]\n",
      " [0.32778366 0.23349489 0.81914665]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.79111239 0.80649983 0.80018908]\n",
      "Biases: \n",
      "[0.70919512 0.55997546 0.34556808]\n",
      "Z Vector: \n",
      "[0.16525357 0.15605785 0.15988652]\n",
      "Error: \n",
      "[-0.00053382  0.00311641  0.00205078]\n",
      "\n",
      "[[0.53792913 0.26647645 0.82172938]\n",
      " [0.04437043 0.1676009  0.18768672]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[-0.14608325  0.67165309]\n",
      "Biases: \n",
      "[0.46764252 0.39521433]\n",
      "Z Vector: \n",
      "[0.12474294 0.22053522]\n",
      "Error: \n",
      "[-0.01822285  0.14812316]\n",
      "\n",
      "\n",
      "------------------ Adjust based on gradient descent for current example ------------------\n",
      "\n",
      "\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.125 0.25  0.675 0.885]\n",
      "Biases: \n",
      "[0.81017369 0.79850249 0.21358366 0.75343774]\n",
      "Z Vector: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Error: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "[[0.2186861  0.05628616 0.50158869 0.07855625]\n",
      " [0.24375452 0.97271557 0.1624343  0.15810173]\n",
      " [0.58186105 0.73510785 0.2521241  0.28141627]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.75180685 0.69862093 0.7719668 ]\n",
      "Biases: \n",
      "[0.65877022 0.31752805 0.54370358]\n",
      "Z Vector: \n",
      "[0.18659331 0.21054973 0.17603406]\n",
      "Error: \n",
      "[0.00036974 0.00056762 0.00025874]\n",
      "\n",
      "[[0.3343155  0.09066698 0.39867784]\n",
      " [0.47740735 0.72693575 0.00087981]\n",
      " [0.32778366 0.23349489 0.81914665]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.79111239 0.80649983 0.80018908]\n",
      "Biases: \n",
      "[0.70919512 0.55997546 0.34556808]\n",
      "Z Vector: \n",
      "[0.16525357 0.15605785 0.15988652]\n",
      "Error: \n",
      "[-0.00053382  0.00311641  0.00205078]\n",
      "\n",
      "[[0.53792913 0.26647645 0.82172938]\n",
      " [0.04437043 0.1676009  0.18768672]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[-0.14608325  0.67165309]\n",
      "Biases: \n",
      "[0.46764252 0.39521433]\n",
      "Z Vector: \n",
      "[0.12474294 0.22053522]\n",
      "Error: \n",
      "[-0.01822285  0.14812316]\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "# with open(\"network.pickle\", \"rb\") as infile:\n",
    "#     net = pickle.load(infile)\n",
    "\n",
    "net = Network(4, 3, 3, 2)\n",
    "#net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = False\n",
    "\n",
    "if training:\n",
    "    #\n",
    "    # 4 3 3 2 Test run\n",
    "    #\n",
    "\n",
    "    dummyData = [[0.125, 0.25, 0.675, 0.885]]\n",
    "    dummyLabel = [0]\n",
    "\n",
    "    net.trainBatch(dummyData, dummyLabel, 100)\n",
    "\n",
    "    # #\n",
    "    # # Training on the actual data\n",
    "    # #\n",
    "\n",
    "    # with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_01 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_01 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "    # #     data_batch_02 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "    # #     label_batch_02 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "    # #     data_batch_03 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "    # #     label_batch_03 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "    # #     data_batch_04 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "    # #     label_batch_04 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "    # #     data_batch_05 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "    # #     label_batch_05 = pickle.load(infile)\n",
    "\n",
    "    # # with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    # #     data_batch_first_100 = pickle.load(infile)\n",
    "    # # with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    # #     label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    # with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "    #     data_test = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "    #     label_test = pickle.load(infile)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    # print(\"Testing on the batch\")\n",
    "    # for i in range(1000):\n",
    "    #     net.trainBatch(data_batch_01, label_batch_01, 1000)\n",
    "    #     net.test(data_batch_01, label_batch_01)\n",
    "    #     #net.test(data_test, label_test)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    # with open(\"network.pickle\", \"wb\") as outfile:\n",
    "    #     pickle.dump(net, outfile)\n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    #print(\"Percentage of correct is: \" + str(net.test(data_test, label_test)))\n",
    "    net.test(data_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
