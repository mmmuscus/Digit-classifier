{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, numberOfOutgoing):\n",
    "        self.activation = random.uniform(0, 1)\n",
    "        self.bias = random.uniform(0, 1)\n",
    "        self.outgoingWeights = np.array([random.uniform(0, 1) for i in range(numberOfOutgoing)])\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activation: \", self.activation)\n",
    "        print(\"Bias: \", self.bias)\n",
    "        print(\"Outgoing Weights: \", self.outgoingWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        self.startLayer = np.array([Neuron(first) for i in range(start)])\n",
    "        self.firstLayer = np.array([Neuron(second) for i in range(first)])\n",
    "        self.secondLayer = np.array([Neuron(end) for i in range(second)])\n",
    "        self.endLayer = np.array([Neuron(0) for i in range(end)])\n",
    "\n",
    "    def forwardPropagationStep(self, inLayer, outLayer):\n",
    "        # weight matrix\n",
    "        weightMatrix = np.array([inLayer[0].outgoingWeights])\n",
    "        for i in range(1, outLayer.size):\n",
    "            weightMatrix = np.r_[weightMatrix, [inLayer[i].outgoingWeights]]\n",
    "\n",
    "        # input activation\n",
    "        inActivation = np.array([neur.activation for neur in inLayer])\n",
    "        \n",
    "        # output biases\n",
    "        outBias = np.array([neur.bias for neur in outLayer])\n",
    "\n",
    "        outputMat = np.dot(weightMatrix, inActivation)\n",
    "        outputMat = outputMat + outBias\n",
    "\n",
    "        for i in range(outLayer.size):\n",
    "            outLayer[i].activation = sigmoid(outputMat[i])\n",
    "\n",
    "    def fullForwardPropagation(self):\n",
    "        self.forwardPropagationStep(self.startLayer, self.firstLayer)\n",
    "        self.forwardPropagationStep(self.firstLayer, self.secondLayer)\n",
    "        self.forwardPropagationStep(self.secondLayer, self.endLayer)\n",
    "        print(self.cost(1))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        sum = 0\n",
    "        for i in range(self.endLayer.size):\n",
    "            if (i == target):\n",
    "                sum += pow(self.endLayer[i].activation - 1.0, 2)\n",
    "            else:\n",
    "                sum += pow(self.endLayer[i].activation, 2)\n",
    "        return sum\n",
    "\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Start Layer\")\n",
    "        for neur in self.startLayer:\n",
    "            neur.cout()\n",
    "        print(\"First Layer\")\n",
    "        for neur in self.firstLayer:\n",
    "            neur.cout()\n",
    "        print(\"Second Layer\")\n",
    "        for neur in self.secondLayer:\n",
    "            neur.cout()\n",
    "        print(\"End Layer\")\n",
    "        for neur in self.endLayer:\n",
    "            neur.cout()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Layer\n",
      "Activation:  0.4266208392251526\n",
      "Bias:  0.4117882876714718\n",
      "Outgoing Weights:  [0.67842548]\n",
      "First Layer\n",
      "Activation:  0.5794827167910693\n",
      "Bias:  0.031219811037589196\n",
      "Outgoing Weights:  [0.42468731]\n",
      "Second Layer\n",
      "Activation:  0.7582191049763222\n",
      "Bias:  0.8968415220315691\n",
      "Outgoing Weights:  [0.89160617]\n",
      "End Layer\n",
      "Activation:  0.818340939370869\n",
      "Bias:  0.8291145762049613\n",
      "Outgoing Weights:  []\n",
      "0.6696818930503964\n",
      "\n",
      "Start Layer\n",
      "Activation:  0.4266208392251526\n",
      "Bias:  0.4117882876714718\n",
      "Outgoing Weights:  [0.67842548]\n",
      "First Layer\n",
      "Activation:  0.5794827167910693\n",
      "Bias:  0.031219811037589196\n",
      "Outgoing Weights:  [0.42468731]\n",
      "Second Layer\n",
      "Activation:  0.7582191049763222\n",
      "Bias:  0.8968415220315691\n",
      "Outgoing Weights:  [0.89160617]\n",
      "End Layer\n",
      "Activation:  0.818340939370869\n",
      "Bias:  0.8291145762049613\n",
      "Outgoing Weights:  []\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "net.cout()\n",
    "\n",
    "net.fullForwardPropagation()\n",
    "\n",
    "print()\n",
    "net.cout()\n",
    "\n",
    "with open(\"network.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(net, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
