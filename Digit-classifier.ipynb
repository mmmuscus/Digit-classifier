{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 0.1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(-1.0, 0.0) for i in range(currentLayerLen)])\n",
    "        #self.biasVector = np.zeros(currentLayerLen)\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetLayer(self):\n",
    "        self.zVector = np.zeros(self.zVector.size)\n",
    "        self.errorVector = np.zeros(self.errorVector.size)\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen) * 0.001\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # print(\"Sigmoid called with x = \", str(x))\n",
    "    if x <= -700:\n",
    "        x = -700\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    # print(\"Sigmoid deriv called with x = \", str(x))\n",
    "    if x <= -350:\n",
    "        x = -350\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # # FOR 4 3 3 2 test\n",
    "\n",
    "        # for idx in range(0, 4):\n",
    "        #     self.Layers[0].activationVector[idx] = dataset[idx]\n",
    "\n",
    "        #\n",
    "        # FOR MINST\n",
    "        #\n",
    "\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += (1.0 - self.Layers[self.Layers.size - 1].activationVector[i]) * (1.0 - self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "            else:\n",
    "                sum += (self.Layers[self.Layers.size - 1].activationVector[i]) * (self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "        return sum / (self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        # The desired value for target is 1\n",
    "        CGradient = np.zeros(self.Layers[self.Layers.size - 1].size)\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            CGradient[i] = self.Layers[self.Layers.size - 1].activationVector[i]\n",
    "            if i == target:\n",
    "                CGradient[i] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def resetNetwork(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetLayer()\n",
    "\n",
    "    def resetAdjs(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        self.resetAdjs()\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nRESET NETWORK\\n\\n\")\n",
    "            self.resetNetwork()\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nSET START LAYER ACTIVATIONS\\n\\n\")\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL FORWARD PROPAGATION\\n\\n\")\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL BACKWARD PROPAGATION\\n\\n\")\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nADJUST BASED ON GRADIENT DESCENT FOR CURRENT EXAMPLE\\n\\n\")\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "        #     self.cout()\n",
    "\n",
    "        # print(\"\\n\\nADJUST WITH ADJUST VARIABLES\\n\\n\")\n",
    "        self.adjustWithAdjustVariables()\n",
    "        # self.cout()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[self.Layers.size - 1].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def checkRandomExamples(self, data, labels):\n",
    "        numberOfData = len(data)\n",
    "\n",
    "        for num in range (0, 20):\n",
    "            randIdx = random.randint(0, numberOfData)\n",
    "            \n",
    "            self.setStartLayerActivations(data[randIdx])\n",
    "            self.fullForwardPropagation(labels[randIdx])\n",
    "\n",
    "            print(\"Label for the data is: \" + str(labels[randIdx]))\n",
    "            self.coutLastLayer()\n",
    "            print(\"The cost is: \" + str(self.cost(labels[randIdx])))\n",
    "\n",
    "    def coutLastLayer(self):\n",
    "        print(\"The last layer activations are: \")\n",
    "        print(self.Layers[self.Layers.size - 1].activationVector)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on batch data:\n",
      "Average cost is:  0.07142301226381016\n",
      "Percentage of correct is:  0.32766666666666666\n",
      "Testing on test data:\n",
      "Average cost is:  0.07183512180118339\n",
      "Percentage of correct is:  0.3188\n",
      "Testing on the batch\n",
      "The last layer activations are: \n",
      "[0.19416429 0.00414918 0.02346422 0.02500601 0.2391434  0.07113481\n",
      " 0.02732293 0.2515182  0.03015781 0.23116524]\n",
      "The last layer activations are: \n",
      "[0.19945317 0.00406828 0.02437782 0.02659821 0.24622768 0.07168623\n",
      " 0.02959998 0.25955672 0.02999619 0.23781682]\n",
      "The last layer activations are: \n",
      "[0.20016668 0.00413383 0.02303147 0.0247692  0.25502561 0.07140869\n",
      " 0.0272225  0.27116372 0.02947796 0.24685106]\n",
      "The last layer activations are: \n",
      "[0.19967547 0.00408369 0.02400189 0.02612543 0.26216198 0.07165689\n",
      " 0.02897149 0.27943362 0.0295891  0.25341241]\n",
      "The last layer activations are: \n",
      "[0.19561501 0.00409571 0.02252027 0.02433354 0.28165449 0.0707464\n",
      " 0.02661974 0.30471191 0.02880868 0.27231088]\n",
      "The last layer activations are: \n",
      "[0.18984208 0.00418876 0.02603779 0.02825303 0.25383048 0.07192835\n",
      " 0.03107899 0.26785504 0.03124696 0.24556124]\n",
      "The last layer activations are: \n",
      "[0.18388914 0.00404161 0.01997208 0.02165707 0.31617003 0.0683681\n",
      " 0.02342641 0.3516321  0.02690254 0.3046638 ]\n",
      "The last layer activations are: \n",
      "[0.17657094 0.00419678 0.02660897 0.02813632 0.18751246 0.06638191\n",
      " 0.0309105  0.18044405 0.03072361 0.18408237]\n",
      "The last layer activations are: \n",
      "[0.18259024 0.00406036 0.01740363 0.01963077 0.25084502 0.0684318\n",
      " 0.02141513 0.26340914 0.0250266  0.24396994]\n",
      "The last layer activations are: \n",
      "[0.18749765 0.00392301 0.02304136 0.0256636  0.2273194  0.06832065\n",
      " 0.02839153 0.23554856 0.02843568 0.21803132]\n",
      "The last layer activations are: \n",
      "[0.19489524 0.00405875 0.0216065  0.02305807 0.25442534 0.07005144\n",
      " 0.0252407  0.27162913 0.02722152 0.24461679]\n",
      "The last layer activations are: \n",
      "[0.19999081 0.00387117 0.02359794 0.02576522 0.26222374 0.07181386\n",
      " 0.02879543 0.27805533 0.02785356 0.25123038]\n",
      "The last layer activations are: \n",
      "[0.19354803 0.00397558 0.02047817 0.02214091 0.29092624 0.0696219\n",
      " 0.02385501 0.3168972  0.02670254 0.28036514]\n",
      "The last layer activations are: \n",
      "[0.18806305 0.00399559 0.02869684 0.03094749 0.23210762 0.07575373\n",
      " 0.03474162 0.2379801  0.03105744 0.22217991]\n",
      "The last layer activations are: \n",
      "[0.18157154 0.00391729 0.01661097 0.01804868 0.32303576 0.06528984\n",
      " 0.01878401 0.36375258 0.02426443 0.31107171]\n",
      "The last layer activations are: \n",
      "[0.1678739  0.00441708 0.02123277 0.0230383  0.17557202 0.06146595\n",
      " 0.02352095 0.16472245 0.03016503 0.17319368]\n",
      "The last layer activations are: \n",
      "[0.21264235 0.00340801 0.02299551 0.02406127 0.25132149 0.07443243\n",
      " 0.03015572 0.24470829 0.01550056 0.23145633]\n",
      "The last layer activations are: \n",
      "[0.18220276 0.00352951 0.02039218 0.02299988 0.24109015 0.06168864\n",
      " 0.02072606 0.25021888 0.01912596 0.23123864]\n",
      "The last layer activations are: \n",
      "[0.19229782 0.00340804 0.02182748 0.02347294 0.25068563 0.06408273\n",
      " 0.02608183 0.26379184 0.02402289 0.24031375]\n",
      "The last layer activations are: \n",
      "[0.19399398 0.00354104 0.02002015 0.02194775 0.26261849 0.06484903\n",
      " 0.02347065 0.28087562 0.02737407 0.25355073]\n",
      "The last layer activations are: \n",
      "[0.19647768 0.00340934 0.02187917 0.02360775 0.27591167 0.06688667\n",
      " 0.02651613 0.29610977 0.026687   0.2650933 ]\n",
      "The last layer activations are: \n",
      "[0.19055681 0.00346961 0.01966132 0.02125452 0.30388089 0.06548942\n",
      " 0.02293856 0.33179825 0.02562646 0.29296921]\n",
      "The last layer activations are: \n",
      "[0.17847113 0.00358487 0.03371955 0.03530169 0.19662176 0.07740073\n",
      " 0.04100817 0.19436604 0.03370827 0.18676911]\n",
      "The last layer activations are: \n",
      "[0.17967222 0.00345018 0.01447794 0.0158792  0.32491789 0.06088046\n",
      " 0.01589767 0.36888907 0.02155662 0.31483976]\n",
      "The last layer activations are: \n",
      "[0.18915975 0.00307252 0.01886276 0.0205015  0.20291611 0.06236637\n",
      " 0.02065055 0.18512938 0.02449851 0.1874199 ]\n",
      "The last layer activations are: \n",
      "[0.21828556 0.00283356 0.02248559 0.02283076 0.25657094 0.06861267\n",
      " 0.02854547 0.24978479 0.01453871 0.22990662]\n",
      "The last layer activations are: \n",
      "[0.18136621 0.00290422 0.0177186  0.02063952 0.25037878 0.05920134\n",
      " 0.01837421 0.26014338 0.01912134 0.23745632]\n",
      "The last layer activations are: \n",
      "[0.19532669 0.0027621  0.02103435 0.02227197 0.2593625  0.06178532\n",
      " 0.02479293 0.27332203 0.02616522 0.24558028]\n",
      "The last layer activations are: \n",
      "[0.20627194 0.00275647 0.02000953 0.02034625 0.28173103 0.06472354\n",
      " 0.02567018 0.30068227 0.0140293  0.2644732 ]\n",
      "The last layer activations are: \n",
      "[0.19130961 0.00280982 0.0201165  0.0220326  0.26980712 0.06177959\n",
      " 0.02273751 0.2878282  0.02172842 0.25951143]\n",
      "The last layer activations are: \n",
      "[0.20438286 0.00244939 0.02085159 0.02003355 0.33349775 0.06541599\n",
      " 0.02769973 0.36116603 0.01221711 0.30793211]\n",
      "The last layer activations are: \n",
      "[0.16621997 0.00283028 0.0220994  0.02559453 0.20042335 0.05901827\n",
      " 0.02123336 0.19971626 0.01771405 0.20014684]\n",
      "The last layer activations are: \n",
      "[0.1750252  0.00245729 0.01812601 0.01786366 0.29716506 0.05902697\n",
      " 0.02288669 0.32257009 0.02354659 0.28989665]\n",
      "The last layer activations are: \n",
      "[0.19527098 0.00251876 0.02276901 0.02232889 0.2541506  0.06103304\n",
      " 0.02975737 0.25729607 0.01046148 0.23331647]\n",
      "The last layer activations are: \n",
      "[0.17719209 0.00256317 0.01361357 0.01745695 0.29064436 0.05816035\n",
      " 0.0120767  0.31148695 0.01319519 0.27569889]\n",
      "The last layer activations are: \n",
      "[0.17749425 0.00247778 0.02125999 0.0253782  0.22719943 0.06224117\n",
      " 0.01892153 0.23186432 0.02420832 0.2153501 ]\n",
      "The last layer activations are: \n",
      "[0.18937189 0.00235927 0.01765849 0.01548406 0.33087874 0.06060024\n",
      " 0.0220063  0.36072314 0.00883281 0.30580138]\n",
      "The last layer activations are: \n",
      "[0.15942309 0.00263397 0.02177528 0.02229593 0.20136405 0.05665553\n",
      " 0.0246343  0.19947758 0.01097178 0.2007371 ]\n",
      "The last layer activations are: \n",
      "[0.16450052 0.0024409  0.01527806 0.01759839 0.28149624 0.05737962\n",
      " 0.01887411 0.30128067 0.01556713 0.2770736 ]\n",
      "The last layer activations are: \n",
      "[0.18145339 0.00243905 0.02014971 0.02159242 0.25090683 0.05800492\n",
      " 0.02612382 0.25828088 0.01901299 0.23300815]\n",
      "The last layer activations are: \n",
      "[0.19453032 0.00230446 0.01725732 0.01719717 0.30364466 0.06106317\n",
      " 0.01980978 0.32546969 0.01191715 0.27813783]\n",
      "The last layer activations are: \n",
      "[0.1304262  0.00369236 0.0174222  0.02082213 0.19564866 0.04947102\n",
      " 0.01920147 0.21775016 0.03368239 0.2229689 ]\n",
      "The last layer activations are: \n",
      "[0.20477129 0.00201228 0.01846443 0.01696112 0.34025094 0.06025392\n",
      " 0.02479538 0.37197584 0.00345278 0.30920851]\n",
      "The last layer activations are: \n",
      "[0.13597379 0.00229899 0.02072114 0.02252877 0.16681466 0.05314453\n",
      " 0.01884706 0.16128644 0.00359698 0.17331126]\n",
      "The last layer activations are: \n",
      "[0.14402951 0.00302131 0.01256322 0.01489393 0.25031025 0.05361423\n",
      " 0.01714799 0.2659929  0.00362571 0.25618653]\n",
      "The last layer activations are: \n",
      "[0.15498322 0.00658559 0.01613191 0.01904202 0.23591173 0.05258959\n",
      " 0.0204989  0.24976627 0.00371441 0.23091303]\n",
      "The last layer activations are: \n",
      "[0.17350904 0.00496805 0.01718949 0.01988772 0.27957225 0.05811717\n",
      " 0.01945866 0.30326503 0.00380788 0.27034745]\n",
      "The last layer activations are: \n",
      "[0.16724939 0.00570749 0.01554565 0.01722875 0.28759864 0.05669378\n",
      " 0.01759418 0.3115556  0.00392992 0.28028515]\n",
      "The last layer activations are: \n",
      "[0.16522108 0.00535175 0.01604527 0.01861311 0.31870848 0.05672473\n",
      " 0.01858096 0.34664767 0.0040364  0.3095802 ]\n",
      "The last layer activations are: \n",
      "[0.14907195 0.00630928 0.02793376 0.03081959 0.17531702 0.06809908\n",
      " 0.03131397 0.17050252 0.0045825  0.16897222]\n",
      "The last layer activations are: \n",
      "[0.15480974 0.00538699 0.01196084 0.0138705  0.332043   0.05168782\n",
      " 0.01328378 0.37774389 0.00430581 0.32724104]\n",
      "The last layer activations are: \n",
      "[0.14868568 0.00581554 0.02046361 0.02302232 0.16621951 0.05738412\n",
      " 0.02336566 0.15187274 0.00469731 0.15926087]\n",
      "The last layer activations are: \n",
      "[0.15093439 0.00533526 0.01288956 0.01488085 0.2762927  0.05306483\n",
      " 0.01427293 0.28865487 0.00462346 0.26744796]\n",
      "The last layer activations are: \n",
      "[0.15551262 0.00532314 0.01636533 0.01895604 0.25486356 0.0542512\n",
      " 0.01898638 0.26729802 0.00482159 0.2456436 ]\n",
      "The last layer activations are: \n",
      "[0.1547451  0.00532008 0.01432322 0.01616729 0.30357646 0.05487541\n",
      " 0.01574584 0.32824998 0.00502468 0.29333185]\n",
      "The last layer activations are: \n",
      "[0.15673006 0.00540776 0.01784569 0.02041191 0.25198818 0.0562693\n",
      " 0.02062518 0.2607793  0.00536657 0.24353682]\n",
      "The last layer activations are: \n",
      "[0.151384   0.00531149 0.01205565 0.01371053 0.31756587 0.05236164\n",
      " 0.01281449 0.34651673 0.00554041 0.30887415]\n",
      "The last layer activations are: \n",
      "[0.15371181 0.00533233 0.0164132  0.01873292 0.2354853  0.05406308\n",
      " 0.01843303 0.23848423 0.0059276  0.2270813 ]\n",
      "The last layer activations are: \n",
      "[0.15244801 0.00524768 0.01326437 0.01524451 0.30237487 0.05349037\n",
      " 0.01495224 0.32447715 0.00622003 0.29220498]\n",
      "The last layer activations are: \n",
      "[0.15406701 0.00528091 0.01679467 0.01930313 0.25123205 0.0543488\n",
      " 0.0196124  0.25956342 0.00672414 0.24210868]\n",
      "The last layer activations are: \n",
      "[0.15308151 0.00525342 0.01276241 0.01451379 0.30899907 0.05333828\n",
      " 0.01348728 0.33363701 0.00719289 0.2985799 ]\n",
      "The last layer activations are: \n",
      "[0.15674768 0.00524168 0.01666651 0.01901022 0.24510895 0.05422334\n",
      " 0.01890362 0.25056477 0.00792368 0.23568632]\n",
      "The last layer activations are: \n",
      "[0.15721652 0.00522485 0.01285492 0.01477119 0.30478733 0.05394894\n",
      " 0.01428111 0.32740677 0.00868859 0.2936427 ]\n",
      "The last layer activations are: \n",
      "[0.16142221 0.00520457 0.01672037 0.01917412 0.25021567 0.05526423\n",
      " 0.01965219 0.25660959 0.00993288 0.23964958]\n",
      "The last layer activations are: \n",
      "[0.1596415  0.00527442 0.01256431 0.01432906 0.30736947 0.05347642\n",
      " 0.01328757 0.33122572 0.0113526  0.29618956]\n",
      "The last layer activations are: \n",
      "[0.16788268 0.00513391 0.01664155 0.01894675 0.25029446 0.05594087\n",
      " 0.01912591 0.2543553  0.01369718 0.23761317]\n",
      "The last layer activations are: \n",
      "[0.16464031 0.00530128 0.01251453 0.0143685  0.30422668 0.05379438\n",
      " 0.01392476 0.32667613 0.01679973 0.2922504 ]\n",
      "The last layer activations are: \n",
      "[0.1780139  0.00497621 0.01696746 0.01933522 0.25868023 0.05876233\n",
      " 0.02055454 0.26182609 0.02139223 0.24202448]\n",
      "The last layer activations are: \n",
      "[0.15894496 0.00570214 0.0115312  0.01321103 0.30038521 0.04948476\n",
      " 0.01154946 0.32714007 0.02389908 0.29356535]\n",
      "The last layer activations are: \n",
      "[0.19930401 0.00437888 0.01602128 0.0180246  0.28151224 0.05886364\n",
      " 0.01795863 0.27571465 0.0198582  0.24895269]\n",
      "The last layer activations are: \n",
      "[0.14471491 0.00572946 0.01173667 0.01387263 0.2743057  0.04606089\n",
      " 0.01297264 0.29900241 0.02658417 0.27216088]\n",
      "The last layer activations are: \n",
      "[0.20453708 0.00390953 0.01544588 0.01744302 0.35196695 0.06134626\n",
      " 0.0202414  0.36123373 0.01439376 0.30958396]\n",
      "The last layer activations are: \n",
      "[0.14212136 0.00407626 0.01163028 0.01368025 0.3303577  0.04007325\n",
      " 0.00897811 0.35291247 0.01799482 0.31877607]\n",
      "The last layer activations are: \n",
      "[0.14469556 0.0042053  0.02257125 0.02579308 0.17724305 0.05410238\n",
      " 0.01840088 0.16975712 0.03191041 0.16877495]\n",
      "The last layer activations are: \n",
      "[0.15027481 0.00400446 0.00990033 0.01099468 0.31688396 0.04493951\n",
      " 0.01376832 0.3507678  0.02057267 0.30994618]\n",
      "The last layer activations are: \n",
      "[0.15529873 0.00384315 0.01547189 0.01721934 0.23530121 0.05250721\n",
      " 0.02156613 0.23510728 0.0262691  0.22115871]\n",
      "The last layer activations are: \n",
      "[0.16673494 0.00333196 0.0116173  0.01303115 0.33720327 0.04640868\n",
      " 0.01055454 0.36337976 0.00913139 0.3178064 ]\n",
      "The last layer activations are: \n",
      "[0.14620751 0.00321555 0.01427803 0.01619063 0.30873556 0.04824674\n",
      " 0.01484112 0.32393355 0.01423514 0.29256316]\n",
      "The last layer activations are: \n",
      "[0.15107174 0.00319105 0.01141788 0.01240695 0.34207626 0.04474737\n",
      " 0.01449888 0.36385417 0.01580753 0.32276291]\n",
      "The last layer activations are: \n",
      "[0.15811239 0.00277401 0.01667214 0.01801387 0.29726888 0.05315971\n",
      " 0.02203587 0.29998128 0.01249097 0.26490185]\n",
      "The last layer activations are: \n",
      "[0.06787581 0.00667988 0.00823754 0.01119293 0.2153126  0.02622821\n",
      " 0.00471556 0.27928676 0.04513087 0.30024725]\n",
      "The last layer activations are: \n",
      "[0.20162478 0.00222244 0.01309541 0.01508643 0.40295073 0.04781765\n",
      " 0.01083681 0.40187258 0.00329358 0.3174987 ]\n",
      "The last layer activations are: \n",
      "[0.06522633 0.00273331 0.01142335 0.01343514 0.32139719 0.03855582\n",
      " 0.01302067 0.34225857 0.00336287 0.31338455]\n",
      "The last layer activations are: \n",
      "[0.07533349 0.00259219 0.01241188 0.01465756 0.33254823 0.04120213\n",
      " 0.01353301 0.35193803 0.00344849 0.31969852]\n",
      "The last layer activations are: \n",
      "[0.08805056 0.00265468 0.01320209 0.01531292 0.30516906 0.04221789\n",
      " 0.01431233 0.32413868 0.00356322 0.29493585]\n",
      "The last layer activations are: \n",
      "[0.09955093 0.00268753 0.0107962  0.01259949 0.33440223 0.03932617\n",
      " 0.01156626 0.35868081 0.00365288 0.32331379]\n",
      "The last layer activations are: \n",
      "[0.10459843 0.00277156 0.04042139 0.04492835 0.09031069 0.0689822\n",
      " 0.04473263 0.07597493 0.00466082 0.08466266]\n",
      "The last layer activations are: \n",
      "[0.11510377 0.00277015 0.00840465 0.00975737 0.32784917 0.03676589\n",
      " 0.00848879 0.37586255 0.003887   0.32614985]\n",
      "The last layer activations are: \n",
      "[0.11567126 0.00276577 0.01350787 0.01563636 0.20202814 0.04349982\n",
      " 0.01370099 0.19187702 0.00421002 0.19161304]\n",
      "The last layer activations are: \n",
      "[0.12117707 0.00279135 0.01043389 0.01222807 0.30316792 0.03801704\n",
      " 0.0117266  0.32333543 0.00417972 0.30232254]\n",
      "The last layer activations are: \n",
      "[0.11934242 0.00279691 0.01216775 0.01423689 0.3199241  0.04001647\n",
      " 0.01361863 0.33822403 0.00436035 0.30677792]\n",
      "The last layer activations are: \n",
      "[0.11467669 0.00282903 0.0121148  0.0139088  0.32402562 0.0403278\n",
      " 0.01301131 0.34289294 0.00456824 0.31076709]\n",
      "The last layer activations are: \n",
      "[0.11275245 0.00284016 0.01266706 0.01460885 0.31924086 0.0408417\n",
      " 0.01404524 0.3390297  0.00481568 0.30682448]\n",
      "The last layer activations are: \n",
      "[0.11677393 0.00286043 0.01115697 0.01287645 0.33036601 0.03976497\n",
      " 0.01190859 0.35012725 0.00511838 0.31728875]\n",
      "The last layer activations are: \n",
      "[0.11177983 0.00287665 0.01399442 0.01594343 0.2793     0.04147074\n",
      " 0.01578023 0.294037   0.00557234 0.26954706]\n",
      "The last layer activations are: \n",
      "[0.1196919  0.00290447 0.00808941 0.00961735 0.32890333 0.03749696\n",
      " 0.00797939 0.35844292 0.00601142 0.32226801]\n",
      "The last layer activations are: \n",
      "[0.12016544 0.00288051 0.01010165 0.01205096 0.33045691 0.03914281\n",
      " 0.01054367 0.34666908 0.00677135 0.31300674]\n",
      "The last layer activations are: \n",
      "[0.11162086 0.00290651 0.01115457 0.01293424 0.3339371  0.03868954\n",
      " 0.01255445 0.3511533  0.00799178 0.31711137]\n",
      "The last layer activations are: \n",
      "[0.1066302  0.00290805 0.01138755 0.01304687 0.33550316 0.0390069\n",
      " 0.01269694 0.3521748  0.01047784 0.31768031]\n",
      "The last layer activations are: \n",
      "[0.10012001 0.00297126 0.01228507 0.01394257 0.32891242 0.03944449\n",
      " 0.01402286 0.34666223 0.01715645 0.31242093]\n",
      "The last layer activations are: \n",
      "[0.11602847 0.00260628 0.01158659 0.01280429 0.35633053 0.03994825\n",
      " 0.013536   0.36714807 0.01094504 0.32050091]\n",
      "The last layer activations are: \n",
      "[0.07641281 0.00330477 0.01254012 0.01478531 0.25910605 0.03486948\n",
      " 0.01274081 0.29065517 0.02145644 0.28099039]\n",
      "The last layer activations are: \n",
      "[0.13259391 0.00230843 0.00884903 0.00966194 0.37200741 0.03854277\n",
      " 0.01062085 0.38867601 0.00646484 0.32778707]\n",
      "The last layer activations are: \n",
      "[0.09784926 0.00227765 0.01037809 0.0118702  0.33047672 0.0368086\n",
      " 0.0120105  0.34534267 0.00832708 0.31359984]\n",
      "The last layer activations are: \n",
      "[0.08940604 0.0027839  0.01029555 0.0123859  0.30666164 0.03337443\n",
      " 0.01040701 0.33529344 0.0155255  0.3163543 ]\n",
      "The last layer activations are: \n",
      "[0.12660103 0.00228857 0.01120498 0.01260576 0.37263671 0.03834524\n",
      " 0.01397442 0.37851601 0.00951518 0.31874938]\n",
      "The last layer activations are: \n",
      "[0.01262249 0.00939481 0.01024652 0.01695807 0.12139171 0.01646598\n",
      " 0.00395905 0.20696038 0.05741631 0.33282424]\n",
      "The last layer activations are: \n",
      "[0.13205321 0.00199041 0.01194715 0.01042248 0.46408621 0.04557617\n",
      " 0.01809935 0.44943374 0.00323108 0.30577661]\n",
      "The last layer activations are: \n",
      "[0.05541722 0.00212954 0.01139333 0.01349211 0.27497855 0.03350324\n",
      " 0.00688428 0.30464017 0.00339351 0.29227404]\n",
      "The last layer activations are: \n",
      "[0.06721891 0.00200624 0.00896162 0.01039623 0.3272461  0.03517435\n",
      " 0.00882419 0.35945054 0.00367126 0.32745862]\n",
      "The last layer activations are: \n",
      "[0.08266618 0.00210659 0.01010708 0.01189849 0.33190718 0.03530585\n",
      " 0.01116825 0.35036579 0.00419986 0.31597987]\n",
      "The last layer activations are: \n",
      "[0.09847387 0.00214116 0.01009736 0.01179154 0.33824032 0.03527841\n",
      " 0.01139495 0.35605845 0.00551772 0.32031671]\n",
      "The last layer activations are: \n",
      "[0.08064473 0.00268309 0.01058199 0.01284872 0.29823946 0.03205933\n",
      " 0.01104595 0.33051315 0.01158193 0.31918391]\n",
      "The last layer activations are: \n",
      "[0.11359298 0.00226555 0.01159287 0.01295151 0.35981795 0.03605378\n",
      " 0.01460522 0.37448944 0.00677512 0.31426811]\n",
      "The last layer activations are: \n",
      "[0.01429998 0.0085773  0.01019919 0.01969251 0.12503591 0.01786799\n",
      " 0.00376384 0.22706725 0.03660536 0.38793126]\n",
      "The last layer activations are: \n",
      "[0.20138386 0.0019008  0.01110755 0.00872838 0.45497436 0.04351704\n",
      " 0.01858545 0.4309254  0.0049982  0.27153907]\n",
      "The last layer activations are: \n",
      "[0.00544755 0.00466712 0.01074534 0.01730779 0.18349158 0.02005596\n",
      " 0.00214016 0.28378389 0.01048273 0.35533139]\n",
      "The last layer activations are: \n",
      "[0.00887811 0.0032322  0.01367175 0.01436448 0.22912487 0.02535145\n",
      " 0.00291957 0.2718457  0.01345576 0.25556513]\n",
      "The last layer activations are: \n",
      "[0.02250725 0.00220983 0.00583232 0.00969774 0.37238636 0.03422126\n",
      " 0.00529248 0.37247008 0.005491   0.28602194]\n",
      "The last layer activations are: \n",
      "[0.01192754 0.00291064 0.00771205 0.01805503 0.25637911 0.02964606\n",
      " 0.00516276 0.32072906 0.01220494 0.35297913]\n",
      "The last layer activations are: \n",
      "[0.02655014 0.00214495 0.00943376 0.007098   0.37035621 0.03485844\n",
      " 0.0100656  0.37581187 0.00682412 0.29371927]\n",
      "The last layer activations are: \n",
      "[0.00203404 0.00963484 0.00848735 0.01810533 0.08483272 0.01722283\n",
      " 0.00289126 0.19401464 0.11690497 0.5300812 ]\n",
      "The last layer activations are: \n",
      "[0.03947263 0.00191571 0.01041394 0.01004865 0.41533376 0.03758591\n",
      " 0.0162309  0.4077046  0.00457392 0.23684323]\n",
      "The last layer activations are: \n",
      "[0.00504797 0.00743046 0.01173119 0.04911376 0.11537035 0.02218999\n",
      " 0.0020973  0.27494503 0.01263151 0.45999904]\n",
      "The last layer activations are: \n",
      "[0.04924796 0.00212857 0.00891634 0.00338116 0.37427444 0.03489219\n",
      " 0.00964041 0.35085847 0.01125439 0.25238789]\n",
      "The last layer activations are: \n",
      "[0.0472651  0.00246986 0.0129372  0.00547063 0.2509282  0.03296229\n",
      " 0.01181955 0.30515479 0.00893797 0.2844771 ]\n",
      "The last layer activations are: \n",
      "[2.87035835e-03 1.47490694e-02 2.91813496e-03 6.49910139e-03\n",
      " 5.32520548e-02 1.34341225e-02 5.14048873e-04 1.72324310e-01\n",
      " 2.81791900e-01 6.91911834e-01]\n",
      "The last layer activations are: \n",
      "[0.13468026 0.00181358 0.00826318 0.00960737 0.44165889 0.03564581\n",
      " 0.00823487 0.42536883 0.00343866 0.16202565]\n",
      "The last layer activations are: \n",
      "[0.02060182 0.00510406 0.01035315 0.03108366 0.16274943 0.02495255\n",
      " 0.0040375  0.34144707 0.00510042 0.33449774]\n",
      "The last layer activations are: \n",
      "[0.03899589 0.00420592 0.01008288 0.01505986 0.233287   0.02845845\n",
      " 0.00594065 0.35355303 0.00682936 0.40519069]\n",
      "The last layer activations are: \n",
      "[0.00708753 0.00902289 0.01092361 0.06616966 0.11424507 0.02341897\n",
      " 0.00206042 0.31695371 0.0301781  0.55748035]\n",
      "The last layer activations are: \n",
      "[0.16751304 0.00192572 0.00821905 0.00134089 0.52284047 0.03594106\n",
      " 0.01702298 0.3736344  0.00537506 0.14276725]\n",
      "The last layer activations are: \n",
      "[0.00442542 0.00776519 0.00493466 0.00040764 0.03371349 0.0171702\n",
      " 0.00152684 0.07784995 0.2833532  0.27449832]\n",
      "The last layer activations are: \n",
      "[0.04958202 0.00179672 0.00773871 0.00142775 0.26216749 0.03310179\n",
      " 0.00906118 0.40342023 0.00266837 0.2614966 ]\n",
      "The last layer activations are: \n",
      "[0.05767064 0.00209641 0.00891249 0.00147338 0.28859879 0.03258797\n",
      " 0.00934642 0.30464064 0.00295018 0.27526141]\n",
      "The last layer activations are: \n",
      "[0.06992658 0.00207282 0.00825364 0.00150038 0.32829125 0.03284823\n",
      " 0.0087222  0.35723193 0.00359481 0.31966063]\n",
      "The last layer activations are: \n",
      "[0.06516733 0.00256925 0.00845258 0.0015903  0.29701238 0.03091657\n",
      " 0.00770126 0.34748182 0.0065036  0.33509518]\n",
      "The last layer activations are: \n",
      "[0.00705363 0.0092239  0.00467846 0.00094568 0.08810351 0.01913923\n",
      " 0.00140284 0.21642466 0.14007653 0.63866008]\n",
      "The last layer activations are: \n",
      "[0.02717483 0.00491254 0.01028174 0.00253133 0.23463768 0.02719511\n",
      " 0.00408918 0.44325723 0.00696173 0.32384951]\n",
      "The last layer activations are: \n",
      "[1.75708112e-03 1.68641607e-02 4.15588535e-03 1.20863907e-03\n",
      " 5.05113873e-02 1.69750494e-02 5.74474491e-04 1.88005988e-01\n",
      " 2.99123235e-01 7.68982850e-01]\n",
      "The last layer activations are: \n",
      "[0.09544403 0.00261075 0.01068    0.00217783 0.44470885 0.03299766\n",
      " 0.01095311 0.43617697 0.00264386 0.13660825]\n",
      "The last layer activations are: \n",
      "[0.00449407 0.00788063 0.00746096 0.00468854 0.10842628 0.02540856\n",
      " 0.00082825 0.45684224 0.00445698 0.27201056]\n",
      "The last layer activations are: \n",
      "[0.00781285 0.00644951 0.01416931 0.0053549  0.16829859 0.03155477\n",
      " 0.00158403 0.26741943 0.00807109 0.35334593]\n",
      "The last layer activations are: \n",
      "[0.00758611 0.00851429 0.00833196 0.00610331 0.14089204 0.02804181\n",
      " 0.00159849 0.39387927 0.0199347  0.51823628]\n",
      "The last layer activations are: \n",
      "[0.00701092 0.00972557 0.01052328 0.0063115  0.13509296 0.02946881\n",
      " 0.00150978 0.35501471 0.03199658 0.44016519]\n",
      "The last layer activations are: \n",
      "[1.83234969e-03 1.44286146e-02 6.70709820e-03 6.17730530e-03\n",
      " 6.82737186e-02 2.57435974e-02 5.15790749e-04 2.73398847e-01\n",
      " 1.30329070e-01 6.94880154e-01]\n",
      "The last layer activations are: \n",
      "[0.16431994 0.00201435 0.00768001 0.00495729 0.65601103 0.03242789\n",
      " 0.01538807 0.27553898 0.00343246 0.11402662]\n",
      "The last layer activations are: \n",
      "[2.91013307e-04 1.80054548e-02 5.58497185e-03 2.27762860e-02\n",
      " 2.43513533e-02 2.21028703e-02 1.24678064e-04 2.27637573e-01\n",
      " 2.62917366e-01 5.84964323e-01]\n",
      "The last layer activations are: \n",
      "[0.00217451 0.00677992 0.02316931 0.02074746 0.12641736 0.03670223\n",
      " 0.00061622 0.35891495 0.00213009 0.13208144]\n",
      "The last layer activations are: \n",
      "[0.00243066 0.00703211 0.00497257 0.04759236 0.14728183 0.03744085\n",
      " 0.00077189 0.48114598 0.00440942 0.25371517]\n",
      "The last layer activations are: \n",
      "[5.45318605e-04 1.52251056e-02 2.61471660e-03 1.41574402e-02\n",
      " 5.52803001e-02 2.75024447e-02 3.32121379e-04 1.48757309e-01\n",
      " 2.30040890e-01 7.54118795e-01]\n",
      "The last layer activations are: \n",
      "[0.00517306 0.00629358 0.0118591  0.09093931 0.18884816 0.04141081\n",
      " 0.0021593  0.71685333 0.00256579 0.2061179 ]\n",
      "The last layer activations are: \n",
      "[0.00643852 0.00668003 0.01403034 0.01766866 0.11387844 0.03520657\n",
      " 0.00117242 0.47483631 0.0039947  0.35776347]\n",
      "The last layer activations are: \n",
      "[0.00301803 0.00888889 0.01277432 0.02310326 0.0440013  0.03457415\n",
      " 0.0012682  0.28639353 0.04548568 0.55969572]\n",
      "The last layer activations are: \n",
      "[2.37823342e-03 1.29569593e-02 2.67303233e-03 1.98796799e-02\n",
      " 6.99499175e-02 2.79101524e-02 2.42480937e-04 2.25395219e-01\n",
      " 9.58265543e-02 6.89500230e-01]\n",
      "The last layer activations are: \n",
      "[1.57691907e-03 1.25904853e-02 3.90699747e-03 3.14467061e-02\n",
      " 3.74236194e-02 3.04839775e-02 2.95962294e-04 2.30498712e-01\n",
      " 1.52328771e-01 6.44938698e-01]\n",
      "The last layer activations are: \n",
      "[0.05537823 0.00310911 0.00413844 0.00090282 0.5433789  0.02669462\n",
      " 0.0058321  0.09176491 0.0136063  0.33362612]\n",
      "The last layer activations are: \n",
      "[2.84237395e-04 1.78559534e-02 5.18747684e-03 2.64810257e-03\n",
      " 9.54600309e-03 2.73346210e-02 4.68464013e-05 2.88383517e-01\n",
      " 2.35705339e-01 8.21760711e-01]\n",
      "The last layer activations are: \n",
      "[2.77231401e-03 7.09704635e-03 2.04817975e-02 1.88321527e-02\n",
      " 5.74377348e-02 4.61633276e-02 2.55776530e-04 7.53256730e-01\n",
      " 2.99782906e-04 3.70133866e-02]\n",
      "The last layer activations are: \n",
      "[4.11927711e-03 6.07308405e-03 6.80718021e-03 3.49134396e-02\n",
      " 1.06997354e-01 4.03006702e-02 3.75724968e-04 4.41165832e-01\n",
      " 3.14038335e-04 5.15382267e-02]\n",
      "The last layer activations are: \n",
      "[5.42188321e-03 6.48685464e-03 9.50912612e-03 8.40564332e-02\n",
      " 8.68331683e-02 4.08532379e-02 5.05306730e-04 6.08629083e-01\n",
      " 3.07716567e-04 7.44861327e-02]\n",
      "The last layer activations are: \n",
      "[8.87664187e-03 6.37997884e-03 1.23545412e-02 1.42539359e-02\n",
      " 1.28445811e-01 3.94461696e-02 9.17409286e-04 4.42255198e-01\n",
      " 3.21047176e-04 1.25953829e-01]\n",
      "The last layer activations are: \n",
      "[5.51510022e-03 7.01245370e-03 1.11136038e-02 2.66334003e-02\n",
      " 7.56651453e-02 3.58072342e-02 5.13974078e-04 6.30743859e-01\n",
      " 3.08507670e-04 2.10576148e-01]\n",
      "The last layer activations are: \n",
      "[8.70063687e-03 6.54567616e-03 1.39247307e-02 5.25684634e-02\n",
      " 1.35876382e-01 4.31755129e-02 1.03696511e-03 3.47025313e-01\n",
      " 3.21992272e-04 3.78823826e-01]\n",
      "The last layer activations are: \n",
      "[5.68145712e-03 7.71112326e-03 9.95801933e-03 8.41566687e-02\n",
      " 7.29997940e-02 3.39195537e-02 3.59322676e-04 6.10418590e-01\n",
      " 3.12123357e-04 4.11509071e-01]\n",
      "The last layer activations are: \n",
      "[0.00981828 0.00626749 0.01583454 0.00555963 0.15660478 0.04280186\n",
      " 0.00083008 0.22096768 0.00034818 0.30433764]\n",
      "The last layer activations are: \n",
      "[5.62358436e-03 8.38705404e-03 8.53966285e-03 7.60957888e-03\n",
      " 5.07195845e-02 3.52023202e-02 4.49406547e-04 4.71281308e-01\n",
      " 3.13035614e-04 3.75962098e-01]\n",
      "The last layer activations are: \n",
      "[1.05356405e-02 6.44234566e-03 1.31634834e-02 1.04758950e-02\n",
      " 9.32610973e-02 5.21638147e-02 8.15120267e-04 5.30982618e-01\n",
      " 3.29000218e-04 4.41832080e-01]\n",
      "The last layer activations are: \n",
      "[5.64003986e-03 8.00780861e-03 1.04618732e-02 1.59704884e-02\n",
      " 8.72047903e-02 3.02771135e-02 4.74389504e-04 5.22730516e-01\n",
      " 3.24840968e-04 4.02386809e-01]\n",
      "The last layer activations are: \n",
      "[9.43547662e-03 6.39640055e-03 1.73542537e-02 3.08677007e-02\n",
      " 1.16215855e-01 4.82738312e-02 1.02655884e-03 4.32265142e-01\n",
      " 3.42447988e-04 4.01465662e-01]\n",
      "The last layer activations are: \n",
      "[6.51322095e-03 8.19277143e-03 7.18171074e-03 7.29612821e-02\n",
      " 7.83133152e-02 3.69437065e-02 2.27016607e-04 6.17916420e-01\n",
      " 3.33280633e-04 4.17666674e-01]\n",
      "The last layer activations are: \n",
      "[0.01247967 0.00621524 0.01198872 0.00468257 0.16651759 0.05319279\n",
      " 0.00041339 0.23898421 0.00036551 0.31938712]\n",
      "The last layer activations are: \n",
      "[5.07185701e-03 1.10657888e-02 1.40980230e-02 5.84084261e-03\n",
      " 3.80906780e-02 3.78917130e-02 5.31997027e-04 4.75941903e-01\n",
      " 3.31637921e-04 3.33830304e-01]\n",
      "The last layer activations are: \n",
      "[1.66360677e-02 5.66392901e-03 1.01507859e-02 7.89986392e-03\n",
      " 9.55733268e-02 7.58016146e-02 6.27064285e-04 5.67297289e-01\n",
      " 3.65003591e-04 5.00317665e-01]\n",
      "The last layer activations are: \n",
      "[4.12359763e-03 6.69388935e-03 1.42962086e-02 1.10131080e-02\n",
      " 8.31339341e-02 1.46730537e-02 4.39555244e-04 5.01799187e-01\n",
      " 3.72263479e-04 3.73006965e-01]\n",
      "The last layer activations are: \n",
      "[6.07068429e-03 5.59712089e-03 1.41153902e-02 1.72236371e-02\n",
      " 1.07927319e-01 1.90491149e-02 7.85286405e-04 4.85165003e-01\n",
      " 3.91282760e-04 4.49260635e-01]\n",
      "The last layer activations are: \n",
      "[9.21869645e-03 6.37108527e-03 1.59418437e-02 3.68096896e-02\n",
      " 7.48464917e-02 2.47376486e-02 3.55924723e-04 5.98386511e-01\n",
      " 4.11244205e-04 4.18364482e-01]\n",
      "The last layer activations are: \n",
      "[0.02752218 0.0044389  0.1122108  0.23710682 0.0057442  0.07481993\n",
      " 0.01203718 0.0326529  0.00097063 0.05013388]\n",
      "The last layer activations are: \n",
      "[6.79962713e-03 7.89211482e-03 3.58043247e-03 1.72799812e-02\n",
      " 5.99816989e-02 4.11430690e-02 4.53068473e-05 3.54559713e-01\n",
      " 1.25117818e-03 6.60408787e-01]\n",
      "The last layer activations are: \n",
      "[1.88088428e-02 7.21452022e-03 3.79192259e-02 1.34867301e-01\n",
      " 1.84923354e-04 1.77969040e-01 4.97252063e-04 8.29320870e-05\n",
      " 2.17019958e-02 4.92475519e-04]\n",
      "The last layer activations are: \n",
      "[3.02437152e-03 7.21639038e-03 5.25120560e-03 1.17309573e-02\n",
      " 2.92528765e-02 5.66251420e-02 4.22855500e-05 1.95782279e-02\n",
      " 6.42271739e-03 4.03994993e-02]\n",
      "The last layer activations are: \n",
      "[1.86142490e-03 9.67558746e-03 5.91318517e-03 2.11843620e-02\n",
      " 2.14277353e-02 5.52551757e-02 6.60016226e-05 4.92419380e-02\n",
      " 2.09647577e-02 6.02957975e-02]\n",
      "The last layer activations are: \n",
      "[3.13459939e-03 8.45198136e-03 2.41619957e-03 2.35664921e-03\n",
      " 2.89866741e-02 7.35864266e-02 6.64200909e-05 1.65422647e-01\n",
      " 1.44653578e-01 1.11846478e-01]\n",
      "The last layer activations are: \n",
      "[2.17716912e-04 1.48164073e-02 6.61239628e-03 6.08099381e-03\n",
      " 2.10703279e-02 1.36871969e-02 1.33093895e-04 2.97157082e-01\n",
      " 3.11032403e-02 1.61693132e-01]\n",
      "The last layer activations are: \n",
      "[3.18316993e-04 5.41294614e-03 3.31328832e-03 9.00928792e-03\n",
      " 4.38385382e-02 2.34221879e-02 4.72222558e-05 1.59027259e-01\n",
      " 7.79596411e-02 6.26627821e-01]\n",
      "The last layer activations are: \n",
      "[2.57779357e-03 3.78528783e-03 1.81907825e-02 6.22775705e-02\n",
      " 1.02833819e-01 4.08834893e-02 1.72963391e-04 7.95013879e-01\n",
      " 2.32455608e-03 2.53749137e-01]\n",
      "The last layer activations are: \n",
      "[6.34995388e-04 6.02515573e-03 1.48306400e-03 1.64485610e-03\n",
      " 3.33841876e-02 5.46093900e-02 4.87451781e-05 6.58192741e-02\n",
      " 1.36813733e-01 8.71876420e-01]\n",
      "The last layer activations are: \n",
      "[1.09265025e-02 3.41437275e-03 8.15798110e-03 1.33046415e-02\n",
      " 1.08010451e-01 6.80419527e-02 1.66914400e-04 7.93323449e-01\n",
      " 4.91258173e-04 7.58999885e-02]\n",
      "The last layer activations are: \n",
      "[7.68797324e-03 3.39848447e-03 1.57525475e-02 2.39415628e-02\n",
      " 6.63569733e-02 4.84016573e-02 1.24083593e-04 6.12875265e-01\n",
      " 5.16831278e-04 1.19909880e-01]\n",
      "The last layer activations are: \n",
      "[6.61532818e-03 4.08392607e-03 4.70445769e-03 2.19002615e-02\n",
      " 5.10782965e-02 7.26630467e-02 1.02356212e-04 2.32050487e-01\n",
      " 1.58311835e-03 3.85457608e-01]\n",
      "The last layer activations are: \n",
      "[8.83271698e-04 4.87278193e-03 5.80857190e-03 2.06453735e-02\n",
      " 2.09324098e-02 5.01395551e-02 2.75408636e-05 2.15768106e-01\n",
      " 5.26825724e-03 7.55628121e-01]\n",
      "The last layer activations are: \n",
      "[2.44951107e-02 2.12699821e-03 4.18001969e-04 9.22100519e-05\n",
      " 6.94924916e-01 2.91895136e-02 2.17169119e-03 7.46754996e-03\n",
      " 8.92696340e-03 2.61577142e-01]\n",
      "The last layer activations are: \n",
      "[8.48335867e-04 5.13063298e-03 6.22737801e-04 2.73220852e-04\n",
      " 2.19392749e-03 6.83549623e-02 1.13851285e-05 1.07918915e-01\n",
      " 3.59257626e-02 7.32779191e-01]\n",
      "The last layer activations are: \n",
      "[2.68237136e-03 5.04775802e-03 1.10578292e-03 5.27569354e-04\n",
      " 9.08291579e-03 4.61683543e-02 3.46801839e-05 5.32638703e-01\n",
      " 1.59371343e-02 2.11358246e-01]\n",
      "The last layer activations are: \n",
      "[9.07659243e-04 6.20462153e-03 7.90579520e-04 2.53355906e-04\n",
      " 6.54131453e-03 6.63337082e-02 2.05181473e-05 8.23974623e-02\n",
      " 1.47844505e-01 6.17038897e-01]\n",
      "The last layer activations are: \n",
      "[3.43113838e-03 4.50902814e-03 2.23576155e-03 6.91373740e-04\n",
      " 2.42282439e-02 8.61889693e-02 4.64639718e-05 4.79971625e-01\n",
      " 8.59495109e-03 5.54187827e-01]\n",
      "The last layer activations are: \n",
      "[8.46571171e-04 5.72503944e-03 2.00857373e-03 2.96272748e-04\n",
      " 1.27317535e-02 5.80856315e-02 2.22243517e-05 1.10307366e-01\n",
      " 1.01412193e-01 8.22934008e-01]\n",
      "The last layer activations are: \n",
      "[7.74580254e-03 3.78502151e-03 1.46796632e-02 1.66973920e-03\n",
      " 5.53897064e-02 8.80444866e-02 7.33832470e-05 7.68524528e-01\n",
      " 2.32295798e-03 2.01878796e-01]\n",
      "The last layer activations are: \n",
      "[6.27287666e-04 5.74054468e-03 2.25376523e-03 3.19415414e-04\n",
      " 1.53304338e-02 4.05293535e-02 2.11152259e-05 9.72803408e-02\n",
      " 9.12076699e-02 8.44558021e-01]\n",
      "The last layer activations are: \n",
      "[6.93026961e-03 3.64566081e-03 1.69027348e-02 1.88432285e-03\n",
      " 6.65123442e-02 6.21819607e-02 7.67400195e-05 7.92060096e-01\n",
      " 1.69185255e-03 1.51824674e-01]\n",
      "The last layer activations are: \n",
      "[5.80959636e-04 5.16006770e-03 1.64281653e-03 3.64622513e-04\n",
      " 1.68009030e-02 8.06478612e-02 1.78423575e-05 1.17652530e-01\n",
      " 5.77926604e-02 7.99494756e-01]\n",
      "The last layer activations are: \n",
      "[1.23409426e-03 4.95196105e-03 4.16537567e-03 3.96451904e-04\n",
      " 2.84473550e-02 7.51419563e-02 3.21840708e-05 1.55509881e-01\n",
      " 3.25669879e-02 5.81667602e-01]\n",
      "The last layer activations are: \n",
      "[7.10167944e-04 5.00066142e-03 2.61117800e-03 3.89706250e-04\n",
      " 1.41571335e-02 9.80068893e-02 1.00979276e-05 1.78185869e-01\n",
      " 8.15345900e-02 8.20038667e-01]\n",
      "The last layer activations are: \n",
      "[1.40483204e-03 5.66076620e-03 6.06006489e-03 3.87314316e-04\n",
      " 2.99788238e-02 3.07081734e-02 2.71957705e-05 8.06183842e-02\n",
      " 2.96160850e-02 5.29434487e-01]\n",
      "The last layer activations are: \n",
      "[8.34860188e-04 4.76571450e-03 8.23344937e-04 4.24881437e-04\n",
      " 1.47790631e-02 4.35733821e-02 8.10436290e-06 1.57994880e-01\n",
      " 7.45445528e-02 8.31254442e-01]\n",
      "The last layer activations are: \n",
      "[9.09539954e-04 5.78400260e-03 1.25166514e-03 4.79184597e-04\n",
      " 2.19518533e-02 8.46161761e-02 1.79785313e-05 1.24016460e-01\n",
      " 4.05962476e-02 6.30365580e-01]\n",
      "The last layer activations are: \n",
      "[6.88393365e-04 4.94853284e-03 2.47032312e-03 4.93410203e-04\n",
      " 1.99032224e-02 1.06480025e-01 1.27739810e-05 1.31242969e-01\n",
      " 6.90230834e-02 8.59474832e-01]\n",
      "The last layer activations are: \n",
      "[8.99382137e-04 5.43173446e-03 7.05995768e-03 5.72827405e-04\n",
      " 2.66182607e-02 8.60205796e-02 1.77149722e-05 1.30387354e-01\n",
      " 2.94534259e-02 7.56644627e-01]\n",
      "The last layer activations are: \n",
      "[6.73396627e-04 5.49499919e-03 5.59511459e-04 5.56998778e-04\n",
      " 2.08725336e-02 1.12328994e-01 1.22767084e-05 1.10808777e-01\n",
      " 7.91338281e-02 8.56048001e-01]\n",
      "The last layer activations are: \n",
      "[5.37893301e-04 5.58887033e-03 7.40512709e-04 6.77159857e-04\n",
      " 2.53224147e-02 2.19465487e-02 1.78288239e-05 1.31326488e-01\n",
      " 3.31646004e-02 7.38791250e-01]\n",
      "The last layer activations are: \n",
      "[5.82209926e-04 5.26638868e-03 1.15428303e-03 7.28329782e-04\n",
      " 2.36010542e-02 3.20948614e-02 1.24347217e-05 1.10583505e-01\n",
      " 5.72319940e-02 8.65611652e-01]\n",
      "The last layer activations are: \n",
      "[6.33511436e-04 5.12404067e-03 3.09947264e-03 9.25119170e-04\n",
      " 2.74677760e-02 6.81535512e-02 1.40664525e-05 1.21915060e-01\n",
      " 3.07017915e-02 8.13038345e-01]\n",
      "The last layer activations are: \n",
      "[5.23024013e-04 5.42673921e-03 5.50007217e-03 1.04797699e-03\n",
      " 2.47741246e-02 1.07352634e-01 1.22018739e-05 1.02460065e-01\n",
      " 6.27506750e-02 8.54028926e-01]\n",
      "The last layer activations are: \n",
      "[6.18340691e-04 5.20377448e-03 1.54227442e-03 1.42701355e-03\n",
      " 2.86404102e-02 8.24064980e-02 1.31103148e-05 1.11972016e-01\n",
      " 3.19305323e-02 8.32583398e-01]\n",
      "The last layer activations are: \n",
      "[2.87473689e-04 6.37477907e-03 3.67475885e-03 1.78010760e-03\n",
      " 2.10528909e-02 6.63547776e-02 1.18515725e-05 8.74369426e-02\n",
      " 1.02143581e-01 8.49750685e-01]\n",
      "The last layer activations are: \n",
      "[6.68070428e-04 4.88236913e-03 2.01634765e-03 3.71351425e-03\n",
      " 3.44703577e-02 1.63094266e-01 1.32037231e-05 1.30998356e-01\n",
      " 1.67644796e-02 8.40164458e-01]\n",
      "The last layer activations are: \n",
      "[1.43850375e-04 7.87465618e-03 3.99476639e-03 6.82367304e-03\n",
      " 1.72744087e-02 8.66126197e-03 1.34988514e-05 6.57360838e-02\n",
      " 1.19776629e-01 8.60852675e-01]\n",
      "The last layer activations are: \n",
      "[4.76536070e-04 4.42180419e-03 6.00907847e-04 1.47461589e-02\n",
      " 3.63611487e-02 1.69131421e-02 1.11193427e-05 1.28793800e-01\n",
      " 1.26103997e-02 8.69083244e-01]\n",
      "The last layer activations are: \n",
      "[1.64832233e-03 3.94603030e-03 1.18299923e-03 5.62529758e-02\n",
      " 1.37871567e-04 4.80775821e-02 2.78837088e-04 5.98767197e-04\n",
      " 4.39000529e-01 6.78231960e-02]\n",
      "The last layer activations are: \n",
      "[3.14316961e-04 5.87624888e-03 9.97937511e-04 1.17643424e-02\n",
      " 2.43126545e-02 3.47683788e-02 1.07673201e-05 1.14081298e-01\n",
      " 2.21799694e-02 8.84635361e-01]\n",
      "The last layer activations are: \n",
      "[6.74051949e-03 3.13458287e-03 6.49761042e-03 9.46317834e-02\n",
      " 7.11959649e-05 1.82317946e-01 2.75699689e-04 2.22525589e-04\n",
      " 6.96212557e-01 2.48794309e-02]\n",
      "The last layer activations are: \n",
      "[3.26871958e-04 6.10513864e-03 6.27496211e-03 9.86426031e-03\n",
      " 2.46583738e-02 9.57852790e-02 9.14828671e-06 1.10082360e-01\n",
      " 1.48233664e-02 8.70473721e-01]\n",
      "The last layer activations are: \n",
      "[4.09864845e-04 5.55611654e-03 4.14801684e-03 2.25947847e-02\n",
      " 1.24825053e-02 1.20541053e-01 1.39220629e-05 2.64872553e-02\n",
      " 4.42183347e-02 6.75663527e-01]\n",
      "The last layer activations are: \n",
      "[2.78281040e-04 6.42556360e-03 3.90226415e-03 5.32557226e-03\n",
      " 2.05684960e-02 8.09568820e-02 8.02577948e-06 8.98415318e-02\n",
      " 3.99405073e-02 8.72053729e-01]\n",
      "The last layer activations are: \n",
      "[2.67895243e-04 6.10012226e-03 5.32489807e-03 1.62753631e-02\n",
      " 2.24451880e-02 1.01052211e-01 9.59356562e-06 8.07076890e-02\n",
      " 5.40755761e-02 8.35590695e-01]\n",
      "The last layer activations are: \n",
      "[2.31843554e-04 6.30496501e-03 2.79095019e-03 6.66154649e-03\n",
      " 2.04059651e-02 8.80991313e-02 7.66617157e-06 8.75881471e-02\n",
      " 4.53006701e-02 8.70925434e-01]\n",
      "The last layer activations are: \n",
      "[2.56396280e-04 5.98666528e-03 7.57350939e-03 1.82129744e-02\n",
      " 2.59057229e-02 1.03749039e-01 9.54768849e-06 9.50633218e-02\n",
      " 3.36732165e-02 8.33988102e-01]\n",
      "The last layer activations are: \n",
      "[1.70277125e-04 6.92805882e-03 1.01721411e-03 3.60673757e-03\n",
      " 1.76502734e-02 8.21707271e-02 6.12487052e-06 7.21575506e-02\n",
      " 1.05268200e-01 8.90133616e-01]\n",
      "The last layer activations are: \n",
      "[1.80900210e-03 5.04428941e-03 2.14620061e-02 9.84650796e-02\n",
      " 5.50125038e-02 3.38337251e-02 3.32857545e-05 9.07698285e-01\n",
      " 6.17240921e-04 6.20116107e-02]\n",
      "The last layer activations are: \n",
      "[2.11754509e-04 5.70853709e-03 3.90522387e-03 3.67875074e-03\n",
      " 2.36482072e-02 1.22674072e-01 8.43933090e-06 5.47244655e-02\n",
      " 1.49614680e-02 8.69796557e-01]\n",
      "The last layer activations are: \n",
      "[2.19064253e-04 5.58749131e-03 5.31338678e-03 1.05312042e-02\n",
      " 2.29982169e-02 5.00694874e-02 7.40421120e-06 9.73626620e-02\n",
      " 4.56717827e-02 7.51707526e-01]\n",
      "The last layer activations are: \n",
      "[2.28335425e-04 5.51769560e-03 2.23635796e-03 8.52049779e-03\n",
      " 2.49521287e-02 1.07137384e-01 7.78880357e-06 7.29155741e-02\n",
      " 5.32176788e-02 8.81159921e-01]\n",
      "The last layer activations are: \n",
      "[1.60693977e-04 5.74481325e-03 7.80039653e-03 1.76153661e-02\n",
      " 2.20059485e-02 6.74989283e-02 7.79628304e-06 1.04102353e-01\n",
      " 2.31128195e-02 8.43204694e-01]\n",
      "The last layer activations are: \n",
      "[2.45674169e-04 5.49644008e-03 6.07384972e-04 2.44360839e-03\n",
      " 2.76026557e-02 1.12976519e-01 9.27651578e-06 5.49237267e-02\n",
      " 5.68753341e-02 8.93914016e-01]\n",
      "The last layer activations are: \n",
      "[8.81985729e-05 5.38923617e-03 9.33014781e-04 6.01850255e-03\n",
      " 9.46717604e-03 5.02071349e-02 5.23528039e-06 1.13855334e-01\n",
      " 1.77008002e-02 8.11145220e-01]\n",
      "The last layer activations are: \n",
      "[2.04589843e-04 5.40033604e-03 1.89404581e-03 7.18389513e-03\n",
      " 1.75228815e-02 9.99134852e-02 1.03689853e-05 6.68220490e-02\n",
      " 4.06799633e-02 8.15675335e-01]\n",
      "The last layer activations are: \n",
      "[9.25853005e-05 5.68389556e-03 4.90082888e-03 2.14145521e-02\n",
      " 6.18396597e-03 8.39661863e-02 3.78957477e-06 1.01071797e-01\n",
      " 5.59229602e-02 9.02491072e-01]\n",
      "The last layer activations are: \n",
      "[1.23273958e-02 2.00704584e-03 9.45992023e-05 7.95839114e-05\n",
      " 6.41830366e-01 3.82315638e-02 8.56862037e-04 1.91505021e-03\n",
      " 7.98993340e-03 5.18536229e-01]\n",
      "The last layer activations are: \n",
      "[6.12906078e-05 5.44514174e-03 2.10574678e-03 8.42719779e-04\n",
      " 1.58201169e-03 9.80607276e-02 2.11130404e-06 1.65161420e-01\n",
      " 4.52790064e-02 8.73159022e-01]\n",
      "The last layer activations are: \n",
      "[1.15842352e-03 6.30872307e-03 4.79514518e-02 1.04133748e-02\n",
      " 1.30585290e-02 1.10455802e-02 2.61538059e-05 9.66822822e-01\n",
      " 2.93703289e-04 1.05152143e-02]\n",
      "The last layer activations are: \n",
      "[1.99683164e-03 3.92285917e-03 3.72926587e-03 2.25413859e-02\n",
      " 1.73777463e-02 2.15014885e-02 1.85426830e-05 9.23229377e-01\n",
      " 3.20375769e-04 1.31395227e-02]\n",
      "The last layer activations are: \n",
      "[3.50959674e-04 4.69067262e-03 1.87682489e-03 1.97613984e-02\n",
      " 1.06973690e-02 4.87768078e-02 1.38387708e-05 3.69504687e-01\n",
      " 9.60739059e-04 6.38789780e-02]\n",
      "The last layer activations are: \n",
      "[1.18169667e-04 4.44452709e-03 1.71493887e-03 6.66570151e-03\n",
      " 5.85604768e-03 1.19597388e-01 6.79575942e-06 3.26505717e-02\n",
      " 6.41225666e-03 4.03514439e-01]\n",
      "The last layer activations are: \n",
      "[1.03974759e-04 5.03579338e-03 4.18521318e-03 8.31067680e-03\n",
      " 5.82603669e-03 6.42317943e-02 5.82966503e-06 5.13035256e-02\n",
      " 4.22734239e-02 8.43268347e-01]\n",
      "The last layer activations are: \n",
      "[1.41328661e-04 4.50165738e-03 2.44944656e-03 8.84351346e-03\n",
      " 8.26962134e-03 1.64669276e-01 6.20658622e-06 7.80112706e-02\n",
      " 1.54833313e-02 8.45664585e-01]\n",
      "The last layer activations are: \n",
      "[8.70214918e-05 5.70868144e-03 3.95772596e-03 4.44033348e-03\n",
      " 6.82877674e-03 2.60100910e-02 5.48085379e-06 6.04871163e-02\n",
      " 5.99855700e-02 8.65900628e-01]\n",
      "The last layer activations are: \n",
      "[1.42179579e-04 4.28857678e-03 2.44000359e-03 1.31998865e-02\n",
      " 1.01606536e-02 9.86401152e-02 5.75909979e-06 9.52965862e-02\n",
      " 1.20630635e-02 8.65304358e-01]\n",
      "The last layer activations are: \n",
      "[8.76511528e-05 5.47993510e-03 4.51291900e-03 1.75968560e-03\n",
      " 8.61808363e-03 8.41031845e-02 5.86585838e-06 6.55254150e-02\n",
      " 4.32907317e-02 8.75808619e-01]\n",
      "The last layer activations are: \n",
      "[1.35953387e-04 4.13660171e-03 2.61586280e-03 6.48372798e-03\n",
      " 1.17223562e-02 1.54605408e-01 5.19275875e-06 1.08433042e-01\n",
      " 1.20105815e-02 8.44302792e-01]\n",
      "The last layer activations are: \n",
      "[8.80748682e-05 5.25989328e-03 3.96321845e-03 3.74401438e-03\n",
      " 9.61593855e-03 4.34319425e-02 5.11686861e-06 6.97018109e-02\n",
      " 4.49977623e-02 8.72496104e-01]\n",
      "The last layer activations are: \n",
      "[1.26486988e-04 4.17904475e-03 3.07343592e-03 1.09651513e-02\n",
      " 1.29151191e-02 1.67328843e-01 5.13844938e-06 9.90837655e-02\n",
      " 1.26072889e-02 8.66907782e-01]\n",
      "The last layer activations are: \n",
      "[8.03638662e-05 5.38035876e-03 3.88130401e-03 1.88782088e-03\n",
      " 1.02728260e-02 4.23239368e-02 4.88641121e-06 6.72181572e-02\n",
      " 4.20346669e-02 8.81300032e-01]\n",
      "The last layer activations are: \n",
      "[1.25064581e-04 4.01963986e-03 3.52184773e-03 8.16594866e-03\n",
      " 1.41761697e-02 1.71292775e-01 4.92632381e-06 1.04128305e-01\n",
      " 1.06947004e-02 8.52013281e-01]\n",
      "The last layer activations are: \n",
      "[7.92529629e-05 5.09861378e-03 3.80641877e-03 2.33977989e-03\n",
      " 1.11799762e-02 4.67761583e-02 4.56295963e-06 7.12180866e-02\n",
      " 3.73852491e-02 8.82293430e-01]\n",
      "The last layer activations are: \n",
      "[1.19683265e-04 4.02654417e-03 3.83130624e-03 8.89238417e-03\n",
      " 1.53163039e-02 1.84406006e-01 4.93658174e-06 9.95416329e-02\n",
      " 1.07415451e-02 8.47619599e-01]\n",
      "The last layer activations are: \n",
      "[7.57177265e-05 4.95604623e-03 3.68429030e-03 2.18666753e-03\n",
      " 1.17816273e-02 4.87449754e-02 4.19515766e-06 7.16593198e-02\n",
      " 3.62618204e-02 8.85826432e-01]\n",
      "The last layer activations are: \n",
      "[1.19275435e-04 3.98261391e-03 4.34982141e-03 8.21131916e-03\n",
      " 1.70140136e-02 1.93445203e-01 5.00716621e-06 1.02267600e-01\n",
      " 9.28962894e-03 8.29955480e-01]\n",
      "The last layer activations are: \n",
      "[7.60463114e-05 4.71083535e-03 3.38199169e-03 2.10489039e-03\n",
      " 1.28173262e-02 6.38902194e-02 3.97575042e-06 7.14848066e-02\n",
      " 3.11068402e-02 8.93798544e-01]\n",
      "The last layer activations are: \n",
      "[1.40782749e-04 3.95646969e-03 6.28367828e-03 8.10365139e-03\n",
      " 1.95088461e-02 1.53993916e-01 5.47620213e-06 1.30169578e-01\n",
      " 6.23735492e-03 7.39436391e-01]\n",
      "The last layer activations are: \n",
      "[8.65781811e-05 4.26674598e-03 2.72985984e-03 1.87184103e-03\n",
      " 1.47005673e-02 1.31462783e-01 3.96016569e-06 7.29199819e-02\n",
      " 2.20400808e-02 8.92001112e-01]\n",
      "The last layer activations are: \n",
      "[1.70741328e-04 4.30366480e-03 9.37719572e-03 9.03245211e-03\n",
      " 2.06486462e-02 3.61095148e-02 5.94904032e-06 2.04918800e-01\n",
      " 5.07196124e-03 5.89321928e-01]\n",
      "The last layer activations are: \n",
      "[1.02004214e-04 3.94838874e-03 2.02866199e-03 1.88420273e-03\n",
      " 1.62060038e-02 8.83726644e-02 3.84393537e-06 6.39912054e-02\n",
      " 1.90822070e-02 8.91099241e-01]\n",
      "The last layer activations are: \n",
      "[1.07744578e-04 4.12399569e-03 6.30884394e-03 5.16785993e-03\n",
      " 1.79175829e-02 7.22483253e-02 4.49616931e-06 1.13994544e-01\n",
      " 8.15625057e-03 6.93288776e-01]\n",
      "The last layer activations are: \n",
      "[1.01312808e-04 3.86168964e-03 2.93685790e-03 2.81700062e-03\n",
      " 1.71725334e-02 1.58155436e-01 3.84559795e-06 8.81211294e-02\n",
      " 1.95147086e-02 8.87370542e-01]\n",
      "The last layer activations are: \n",
      "[8.67390294e-05 4.45833916e-03 5.48342521e-03 4.86953139e-03\n",
      " 1.75578039e-02 5.61695438e-02 4.31039000e-06 8.79347947e-02\n",
      " 1.38515122e-02 8.44847481e-01]\n",
      "The last layer activations are: \n",
      "[9.62230000e-05 4.04353464e-03 3.02080038e-03 3.85311500e-03\n",
      " 1.76940479e-02 1.61562394e-01 3.61913159e-06 8.52066749e-02\n",
      " 1.65548792e-02 8.87688552e-01]\n",
      "The last layer activations are: \n",
      "[7.42490991e-05 4.71816526e-03 4.79318661e-03 3.49847055e-03\n",
      " 1.65875290e-02 5.42673596e-02 3.93431108e-06 7.67836890e-02\n",
      " 1.93848645e-02 8.65032437e-01]\n",
      "The last layer activations are: \n",
      "[9.27303458e-05 4.03515013e-03 3.17998888e-03 4.88531717e-03\n",
      " 1.81087609e-02 1.79228344e-01 3.45086195e-06 8.73533912e-02\n",
      " 1.31327479e-02 8.90242014e-01]\n",
      "The last layer activations are: \n",
      "[5.90549377e-05 5.24981708e-03 4.57231034e-03 2.48302587e-03\n",
      " 1.54930591e-02 3.47613005e-02 3.79872970e-06 6.77252359e-02\n",
      " 2.71017952e-02 8.63879789e-01]\n",
      "The last layer activations are: \n",
      "[9.15412371e-05 3.88763753e-03 2.91107396e-03 5.97280581e-03\n",
      " 1.85225710e-02 1.74056324e-01 3.14716506e-06 9.22149714e-02\n",
      " 9.93150678e-03 8.98610921e-01]\n",
      "The last layer activations are: \n",
      "[5.33870635e-05 5.32571210e-03 4.51908493e-03 1.72623977e-03\n",
      " 1.57475375e-02 3.14586403e-02 3.65464360e-06 6.38079883e-02\n",
      " 2.65683341e-02 8.62142829e-01]\n",
      "The last layer activations are: \n",
      "[8.24951330e-05 3.77158322e-03 2.76829352e-03 5.85847626e-03\n",
      " 1.82593679e-02 1.60244569e-01 2.89717049e-06 9.08846517e-02\n",
      " 9.76065961e-03 9.01532941e-01]\n",
      "The last layer activations are: \n",
      "[5.50876193e-05 5.08969429e-03 4.15544887e-03 1.40276323e-03\n",
      " 1.67810082e-02 4.34603244e-02 3.43827570e-06 6.01891574e-02\n",
      " 2.30704848e-02 8.68401538e-01]\n",
      "The last layer activations are: \n",
      "[7.25445722e-05 3.77180375e-03 3.48069423e-03 5.17411370e-03\n",
      " 1.74426662e-02 1.76196121e-01 2.65991858e-06 8.62643450e-02\n",
      " 9.99063970e-03 8.98807143e-01]\n",
      "The last layer activations are: \n",
      "[5.76575274e-05 5.00515300e-03 3.95349805e-03 1.52543081e-03\n",
      " 1.71719265e-02 3.86395445e-02 3.15972849e-06 6.17062884e-02\n",
      " 2.04093174e-02 8.63537198e-01]\n",
      "The last layer activations are: \n",
      "[6.61798462e-05 3.90864603e-03 3.83509394e-03 4.53592985e-03\n",
      " 1.60553249e-02 1.58803813e-01 2.55942952e-06 7.89228263e-02\n",
      " 1.20552406e-02 9.04041314e-01]\n",
      "The last layer activations are: \n",
      "[6.29408829e-05 4.81662468e-03 3.88998493e-03 1.82121198e-03\n",
      " 1.87262953e-02 4.96391825e-02 3.16075244e-06 6.72269597e-02\n",
      " 1.49390062e-02 8.52849160e-01]\n",
      "The last layer activations are: \n",
      "[5.52468445e-05 4.10050618e-03 3.42150620e-03 3.65801071e-03\n",
      " 1.43871669e-02 1.38053610e-01 2.40738750e-06 7.18621921e-02\n",
      " 1.75734586e-02 9.05406584e-01]\n",
      "The last layer activations are: \n",
      "[7.65207509e-05 4.55207660e-03 4.19455186e-03 2.12678179e-03\n",
      " 2.25175891e-02 6.01980844e-02 3.74642149e-06 7.31632534e-02\n",
      " 7.95378058e-03 8.29855500e-01]\n",
      "The last layer activations are: \n",
      "[4.58428681e-05 4.40725288e-03 3.04090266e-03 2.89132277e-03\n",
      " 1.20096096e-02 1.03383985e-01 2.22775750e-06 6.51771196e-02\n",
      " 2.39972301e-02 9.07184076e-01]\n",
      "The last layer activations are: \n",
      "[6.03933666e-04 3.34206789e-03 1.73232615e-03 1.39635688e-03\n",
      " 1.22231615e-01 3.56255778e-02 2.75612803e-05 5.38491092e-02\n",
      " 2.28645269e-03 5.03847599e-01]\n",
      "The last layer activations are: \n",
      "[4.66962238e-05 4.08732919e-03 3.57759851e-03 2.72063794e-03\n",
      " 1.09817198e-02 9.78113359e-02 2.24241382e-06 7.49291621e-02\n",
      " 9.93560755e-03 8.84966901e-01]\n",
      "The last layer activations are: \n",
      "[7.86604884e-05 4.52896102e-03 4.02009828e-03 2.72105534e-03\n",
      " 1.93298021e-02 6.66513990e-02 4.29729689e-06 9.37099263e-02\n",
      " 4.83940669e-03 6.68677911e-01]\n",
      "The last layer activations are: \n",
      "[4.90124158e-05 4.34697434e-03 3.50958320e-03 2.63974363e-03\n",
      " 1.05145466e-02 1.13858728e-01 2.33114051e-06 8.27421014e-02\n",
      " 1.42615436e-02 8.62781171e-01]\n",
      "The last layer activations are: \n",
      "[6.84907051e-05 4.34612532e-03 4.16701441e-03 2.96921270e-03\n",
      " 1.77677851e-02 7.69435401e-02 3.71831219e-06 9.12429443e-02\n",
      " 6.64468797e-03 7.22359707e-01]\n",
      "The last layer activations are: \n",
      "[4.83953519e-05 4.43041381e-03 3.32264691e-03 2.78107371e-03\n",
      " 1.33447438e-02 1.13539774e-01 2.66523476e-06 8.14358321e-02\n",
      " 1.58328315e-02 8.79509996e-01]\n",
      "The last layer activations are: \n",
      "[5.83616344e-05 4.49561303e-03 4.05845685e-03 3.07571025e-03\n",
      " 1.71938514e-02 8.29128479e-02 2.90921036e-06 8.95945846e-02\n",
      " 7.96263931e-03 8.36937626e-01]\n",
      "The last layer activations are: \n",
      "[5.15284563e-05 4.59262887e-03 3.21131037e-03 2.62274940e-03\n",
      " 1.59006829e-02 1.04933650e-01 2.59227086e-06 7.54753051e-02\n",
      " 1.49958095e-02 8.77321519e-01]\n",
      "The last layer activations are: \n",
      "[5.37393371e-05 4.55362299e-03 3.65848284e-03 2.74284252e-03\n",
      " 1.76962610e-02 1.03143609e-01 2.57107837e-06 7.85459660e-02\n",
      " 1.01617681e-02 8.78338955e-01]\n",
      "The last layer activations are: \n",
      "[5.09932984e-05 4.62474943e-03 3.29144039e-03 2.46643147e-03\n",
      " 1.75295033e-02 9.95452530e-02 2.48350414e-06 6.80126855e-02\n",
      " 1.33919317e-02 8.85918245e-01]\n",
      "The last layer activations are: \n",
      "[5.15608532e-05 4.58904938e-03 3.60362315e-03 2.79868141e-03\n",
      " 1.89350508e-02 9.72151699e-02 2.43227216e-06 7.32643772e-02\n",
      " 1.04880321e-02 8.81750966e-01]\n",
      "The last layer activations are: \n",
      "[4.82903068e-05 4.67856661e-03 3.11369821e-03 2.17438869e-03\n",
      " 1.84360006e-02 9.91865342e-02 2.36362038e-06 6.01774754e-02\n",
      " 1.46088653e-02 8.93980711e-01]\n",
      "The last layer activations are: \n",
      "[5.00412294e-05 4.56653471e-03 3.85405292e-03 3.02720830e-03\n",
      " 1.99319951e-02 8.85279882e-02 2.30686214e-06 7.19420311e-02\n",
      " 9.02309718e-03 8.81159200e-01]\n",
      "The last layer activations are: \n",
      "[4.67338367e-05 4.74867163e-03 2.94886448e-03 2.03471531e-03\n",
      " 1.92958377e-02 9.66370770e-02 2.26862687e-06 5.89714509e-02\n",
      " 1.47176215e-02 8.91659794e-01]\n",
      "The last layer activations are: \n",
      "[4.90014317e-05 4.50999541e-03 3.92437151e-03 2.95438534e-03\n",
      " 2.08019303e-02 9.59759670e-02 2.20397395e-06 7.11949187e-02\n",
      " 8.39148206e-03 8.85418152e-01]\n",
      "The last layer activations are: \n",
      "[4.42942623e-05 4.83179335e-03 2.84447138e-03 1.88185614e-03\n",
      " 1.98819319e-02 8.85500977e-02 2.15149384e-06 5.67450738e-02\n",
      " 1.53259232e-02 8.93638834e-01]\n",
      "The last layer activations are: \n",
      "[4.85167909e-05 4.41922598e-03 4.06834685e-03 3.02410702e-03\n",
      " 2.16535436e-02 1.04796667e-01 2.10969236e-06 7.25621439e-02\n",
      " 7.21651806e-03 8.85838272e-01]\n",
      "The last layer activations are: \n",
      "[4.11639768e-05 4.94689448e-03 2.68445129e-03 1.57844061e-03\n",
      " 2.00322888e-02 7.84084737e-02 2.02124983e-06 5.14703684e-02\n",
      " 1.70510966e-02 9.00335583e-01]\n",
      "The last layer activations are: \n",
      "[3.64180314e-04 4.35124989e-03 2.20857496e-02 4.50403131e-02\n",
      " 3.17893117e-02 6.88694921e-03 4.71226768e-06 7.88915441e-01\n",
      " 9.67314683e-04 1.93706893e-01]\n",
      "The last layer activations are: \n",
      "[4.10981698e-05 4.79527678e-03 2.54157056e-03 1.01825913e-03\n",
      " 2.06181089e-02 1.00599810e-01 1.94461700e-06 3.79878621e-02\n",
      " 1.42521192e-02 9.20431589e-01]\n",
      "The last layer activations are: \n",
      "[4.95083424e-05 4.60985127e-03 6.85186057e-03 2.53278107e-03\n",
      " 2.13257249e-02 2.68660451e-02 2.08158126e-06 7.34889173e-02\n",
      " 3.39361933e-03 7.46842993e-01]\n",
      "The last layer activations are: \n",
      "[4.60846309e-05 4.07869585e-03 2.11183750e-03 1.72603111e-03\n",
      " 1.99662819e-02 7.58862632e-02 1.79372951e-06 5.47237189e-02\n",
      " 1.05163950e-02 9.01865958e-01]\n",
      "The last layer activations are: \n",
      "[5.68847271e-05 4.71055042e-03 6.12530682e-03 3.22281783e-03\n",
      " 2.67571207e-02 4.83510223e-02 2.95603382e-06 1.10236735e-01\n",
      " 3.89384272e-03 7.01870671e-01]\n",
      "The last layer activations are: \n",
      "[4.52625871e-05 4.13628717e-03 2.51869909e-03 1.58843168e-03\n",
      " 2.07871146e-02 1.27435568e-01 1.78001547e-06 6.43080991e-02\n",
      " 1.08718963e-02 9.09974016e-01]\n",
      "The last layer activations are: \n",
      "[4.44419161e-05 4.86919504e-03 5.11065323e-03 2.04236234e-03\n",
      " 2.65224992e-02 5.03554920e-02 2.59039598e-06 7.08367001e-02\n",
      " 6.08177003e-03 8.00362885e-01]\n",
      "The last layer activations are: \n",
      "[4.37922939e-05 4.16805667e-03 2.72758121e-03 2.04917395e-03\n",
      " 2.09369796e-02 1.12867856e-01 1.68409818e-06 6.69045746e-02\n",
      " 1.15962587e-02 9.01038750e-01]\n",
      "The last layer activations are: \n",
      "[3.92478120e-05 4.91632322e-03 4.40922550e-03 1.60466658e-03\n",
      " 2.59561683e-02 6.39988063e-02 2.31619318e-06 6.36623611e-02\n",
      " 7.47680803e-03 8.37841391e-01]\n",
      "The last layer activations are: \n",
      "[4.06765752e-05 4.25307610e-03 3.21834154e-03 2.48462242e-03\n",
      " 2.23744952e-02 1.09507641e-01 1.67756568e-06 7.05592997e-02\n",
      " 9.97498706e-03 8.94004292e-01]\n",
      "The last layer activations are: \n",
      "[3.52954951e-05 4.93509187e-03 3.88855583e-03 1.47724684e-03\n",
      " 2.32551586e-02 7.28977229e-02 1.92729210e-06 6.13232199e-02\n",
      " 9.54876942e-03 8.76842164e-01]\n",
      "The last layer activations are: \n",
      "[3.94916485e-05 4.44064086e-03 3.27332250e-03 2.23402091e-03\n",
      " 2.34305172e-02 1.10789072e-01 1.65679600e-06 6.54848951e-02\n",
      " 8.78534952e-03 8.93820875e-01]\n",
      "The last layer activations are: \n",
      "[3.31076072e-05 4.92783360e-03 3.34447070e-03 1.54446746e-03\n",
      " 2.27645161e-02 6.57425493e-02 1.69598746e-06 5.73784422e-02\n",
      " 1.21578230e-02 8.88015549e-01]\n",
      "The last layer activations are: \n",
      "[3.87577045e-05 4.37824025e-03 3.42603921e-03 2.21592891e-03\n",
      " 2.47906239e-02 1.24017207e-01 1.61022795e-06 6.58524220e-02\n",
      " 7.22746251e-03 8.94731836e-01]\n",
      "The last layer activations are: \n",
      "[2.95514190e-05 5.04240093e-03 3.03786971e-03 1.45575042e-03\n",
      " 2.23252939e-02 5.33541502e-02 1.55880657e-06 5.39419237e-02\n",
      " 1.49878869e-02 8.90311277e-01]\n",
      "The last layer activations are: \n",
      "[3.92912699e-05 4.23878156e-03 3.54292879e-03 2.26343010e-03\n",
      " 2.67684556e-02 1.47175337e-01 1.55978416e-06 6.84139850e-02\n",
      " 4.99529897e-03 8.96692752e-01]\n",
      "The last layer activations are: \n",
      "[2.31268992e-05 5.52398470e-03 2.88228460e-03 1.24144493e-03\n",
      " 2.07344853e-02 3.22755515e-02 1.48391064e-06 4.85215659e-02\n",
      " 1.96720039e-02 8.87616253e-01]\n",
      "The last layer activations are: \n",
      "[3.87893359e-05 4.15060699e-03 3.22572621e-03 2.22242387e-03\n",
      " 2.85218074e-02 1.59561192e-01 1.47388762e-06 6.94739893e-02\n",
      " 3.73363718e-03 9.06020969e-01]\n",
      "The last layer activations are: \n",
      "[1.45451376e-05 6.64813484e-03 2.60237839e-03 8.93155102e-04\n",
      " 1.67650945e-02 1.40248548e-02 1.41237920e-06 3.82745512e-02\n",
      " 3.52359160e-02 8.80729607e-01]\n",
      "The last layer activations are: \n",
      "[3.68849142e-05 4.09991375e-03 2.73560326e-03 2.12768492e-03\n",
      " 2.88546762e-02 1.46449546e-01 1.36420693e-06 7.02933300e-02\n",
      " 3.66841725e-03 9.14844442e-01]\n",
      "The last layer activations are: \n",
      "[1.79614858e-05 5.76833311e-03 3.67703139e-03 9.88381847e-04\n",
      " 2.07441023e-02 2.82010570e-02 1.62718180e-06 4.45158347e-02\n",
      " 1.45963728e-02 8.88335335e-01]\n",
      "The last layer activations are: \n",
      "[3.47232983e-05 4.06290272e-03 2.39091732e-03 2.13785749e-03\n",
      " 2.87523500e-02 1.42197888e-01 1.26183154e-06 6.83767955e-02\n",
      " 3.83430969e-03 9.21477476e-01]\n",
      "The last layer activations are: \n",
      "[2.13440733e-05 5.25314919e-03 3.38669767e-03 1.08511902e-03\n",
      " 2.28346119e-02 3.67791340e-02 1.52302047e-06 4.74998243e-02\n",
      " 9.96630795e-03 8.87937389e-01]\n",
      "The last layer activations are: \n",
      "[3.02764749e-05 4.15365585e-03 2.71931916e-03 1.80575679e-03\n",
      " 2.76569629e-02 1.27861863e-01 1.21287857e-06 6.17195284e-02\n",
      " 4.46661597e-03 9.19194888e-01]\n",
      "The last layer activations are: \n",
      "[2.51524271e-05 4.84898135e-03 3.59133361e-03 1.45553269e-03\n",
      " 2.56053166e-02 4.49681251e-02 1.36297866e-06 5.34198938e-02\n",
      " 7.14759169e-03 8.85791617e-01]\n",
      "The last layer activations are: \n",
      "[2.70310953e-05 4.43784560e-03 3.18036629e-03 1.49701236e-03\n",
      " 2.64824641e-02 1.03151660e-01 1.23203143e-06 5.50528146e-02\n",
      " 6.15501310e-03 9.09467856e-01]\n",
      "The last layer activations are: \n",
      "[2.59935629e-05 4.69304601e-03 3.38787051e-03 1.67670333e-03\n",
      " 2.68369970e-02 5.93938137e-02 1.25406858e-06 5.74490777e-02\n",
      " 6.24502360e-03 8.87799890e-01]\n",
      "The last layer activations are: \n",
      "[2.57759799e-05 4.44445677e-03 2.99484751e-03 1.41508501e-03\n",
      " 2.57177257e-02 9.27728948e-02 1.15082116e-06 5.13068799e-02\n",
      " 7.41345222e-03 9.11521371e-01]\n",
      "The last layer activations are: \n",
      "[2.56834922e-05 4.76164459e-03 3.61458405e-03 1.81660084e-03\n",
      " 2.76920925e-02 5.43307391e-02 1.24139593e-06 5.87567885e-02\n",
      " 5.16005057e-03 8.74710345e-01]\n",
      "The last layer activations are: \n",
      "[2.42972974e-05 4.51538629e-03 2.70242768e-03 1.34918630e-03\n",
      " 2.43088713e-02 8.65646762e-02 1.06866456e-06 4.78282662e-02\n",
      " 8.33745455e-03 9.16371022e-01]\n",
      "The last layer activations are: \n",
      "[2.66572736e-05 4.84987545e-03 4.17705008e-03 1.94743392e-03\n",
      " 2.93170480e-02 4.97752007e-02 1.34420660e-06 6.37833309e-02\n",
      " 3.57718614e-03 8.43110569e-01]\n",
      "The last layer activations are: \n",
      "[2.29302041e-05 4.42335096e-03 2.43556854e-03 1.30354119e-03\n",
      " 2.29906638e-02 8.39938403e-02 1.02295508e-06 4.52613985e-02\n",
      " 8.40390261e-03 9.18904183e-01]\n",
      "The last layer activations are: \n",
      "[3.02574110e-05 4.83049252e-03 4.26967330e-03 1.71030849e-03\n",
      " 3.41249326e-02 4.26496846e-02 1.62459171e-06 5.94274832e-02\n",
      " 2.67412548e-03 8.01020623e-01]\n",
      "The last layer activations are: \n",
      "[2.37570077e-05 4.21468770e-03 2.59817202e-03 1.60755081e-03\n",
      " 2.11939533e-02 8.45383795e-02 9.63947878e-07 5.65590611e-02\n",
      " 6.94698263e-03 9.11325412e-01]\n",
      "The last layer activations are: \n",
      "[4.13350813e-05 4.67271033e-03 2.86220365e-03 8.91112019e-04\n",
      " 4.76082727e-02 6.65763321e-02 2.36867636e-06 4.20946360e-02\n",
      " 2.68065447e-03 7.91978902e-01]\n",
      "The last layer activations are: \n",
      "[2.12497354e-05 4.22365684e-03 3.14304137e-03 1.92703353e-03\n",
      " 1.65153517e-02 7.71344058e-02 9.13961734e-07 6.64227048e-02\n",
      " 6.94235417e-03 9.20054941e-01]\n",
      "The last layer activations are: \n",
      "[2.50359350e-04 3.51103856e-03 3.68204928e-04 1.56031643e-04\n",
      " 2.25662894e-01 7.57235240e-02 1.74712299e-05 8.78485306e-03\n",
      " 2.48626920e-03 6.33984056e-01]\n",
      "The last layer activations are: \n",
      "[1.24941279e-05 4.38582438e-03 3.69880089e-03 2.00074780e-03\n",
      " 9.85778501e-03 6.82132297e-02 6.85175036e-07 7.61236188e-02\n",
      " 7.08973718e-03 9.30067051e-01]\n",
      "The last layer activations are: \n",
      "[2.61325316e-05 5.23424772e-03 1.50433837e-03 4.64105284e-04\n",
      " 2.73023607e-02 1.10364381e-01 1.88173794e-06 4.53063801e-02\n",
      " 1.24975230e-03 8.04259319e-01]\n",
      "The last layer activations are: \n",
      "[1.54913966e-05 4.37373781e-03 3.11224536e-03 1.94331132e-03\n",
      " 8.88086010e-03 7.93022297e-02 7.35737727e-07 6.52008637e-02\n",
      " 5.31628953e-03 9.10169783e-01]\n",
      "The last layer activations are: \n",
      "[1.74367116e-05 5.20153520e-03 2.89600126e-03 1.01426781e-03\n",
      " 1.32874847e-02 7.76317719e-02 9.61480788e-07 6.94705825e-02\n",
      " 2.33098224e-03 8.72362144e-01]\n",
      "The last layer activations are: \n",
      "[1.65893681e-05 4.90222685e-03 2.64270742e-03 1.35267834e-03\n",
      " 1.21778262e-02 7.60861868e-02 8.65286153e-07 6.05079586e-02\n",
      " 4.72374344e-03 8.94151814e-01]\n",
      "The last layer activations are: \n",
      "[1.70417481e-05 4.95969276e-03 2.86140889e-03 1.31695846e-03\n",
      " 1.38950751e-02 7.57429354e-02 8.86443580e-07 6.51128256e-02\n",
      " 3.56891475e-03 8.96125310e-01]\n",
      "The last layer activations are: \n",
      "[1.66356599e-05 5.00543252e-03 2.60219862e-03 1.22330120e-03\n",
      " 1.42991175e-02 7.47025029e-02 8.72810741e-07 5.56320281e-02\n",
      " 4.63776762e-03 9.02206757e-01]\n",
      "The last layer activations are: \n",
      "[1.65972758e-05 5.04922267e-03 2.72368192e-03 1.35285629e-03\n",
      " 1.59320753e-02 6.95014728e-02 8.63405796e-07 5.89444217e-02\n",
      " 4.04780705e-03 8.95365536e-01]\n",
      "The last layer activations are: \n",
      "[1.66867018e-05 4.97919475e-03 2.58376031e-03 1.27037509e-03\n",
      " 1.64269097e-02 7.80198534e-02 8.42359990e-07 5.30979948e-02\n",
      " 4.55357764e-03 9.03955108e-01]\n",
      "The last layer activations are: \n",
      "[1.60579066e-05 5.11157040e-03 2.63414859e-03 1.27607952e-03\n",
      " 1.79565323e-02 6.76906215e-02 8.31762480e-07 5.56637679e-02\n",
      " 4.28105334e-03 8.99146013e-01]\n",
      "The last layer activations are: \n",
      "[1.65034416e-05 4.98706851e-03 2.50391339e-03 1.26579868e-03\n",
      " 1.80524680e-02 7.80895253e-02 8.02785755e-07 5.11474173e-02\n",
      " 4.70913532e-03 9.06998579e-01]\n",
      "The last layer activations are: \n",
      "[1.54711301e-05 5.15621116e-03 2.59866896e-03 1.21427518e-03\n",
      " 1.95216333e-02 6.63284563e-02 7.89383601e-07 5.47416744e-02\n",
      " 4.30684083e-03 9.04750131e-01]\n",
      "The last layer activations are: \n",
      "[1.63668491e-05 4.96585642e-03 2.48571788e-03 1.26257147e-03\n",
      " 1.94106323e-02 7.65472187e-02 7.74002240e-07 4.92512505e-02\n",
      " 4.81420388e-03 9.07475376e-01]\n",
      "The last layer activations are: \n",
      "[1.51805593e-05 5.13563940e-03 2.53688330e-03 1.18240117e-03\n",
      " 2.09690569e-02 6.64532983e-02 7.41038469e-07 5.50449780e-02\n",
      " 4.18706827e-03 9.09563972e-01]\n",
      "The last layer activations are: \n",
      "[1.60165948e-05 4.93817755e-03 2.50553775e-03 1.21624137e-03\n",
      " 2.06193431e-02 7.46868880e-02 7.37753992e-07 4.78755772e-02\n",
      " 4.93892767e-03 9.09950150e-01]\n",
      "The last layer activations are: \n",
      "[1.52304710e-05 5.10524753e-03 2.57884846e-03 1.22801366e-03\n",
      " 2.23770181e-02 6.68580556e-02 7.05360495e-07 5.46749011e-02\n",
      " 3.86424305e-03 9.10284070e-01]\n",
      "The last layer activations are: \n",
      "[1.49662640e-05 5.07795599e-03 2.35878002e-03 1.04480029e-03\n",
      " 2.15747325e-02 6.88131083e-02 6.90683877e-07 4.38638728e-02\n",
      " 5.59469531e-03 9.15085120e-01]\n",
      "The last layer activations are: \n",
      "[1.54322986e-05 5.07419898e-03 2.88182811e-03 1.38677427e-03\n",
      " 2.34625144e-02 6.72225501e-02 6.82711303e-07 5.60241106e-02\n",
      " 2.89879305e-03 9.03299428e-01]\n",
      "The last layer activations are: \n",
      "[1.38362621e-05 5.27489598e-03 2.13312079e-03 8.32502382e-04\n",
      " 2.21495557e-02 6.48956600e-02 6.59612972e-07 3.73874354e-02\n",
      " 5.76607707e-03 9.25164400e-01]\n",
      "The last layer activations are: \n",
      "[1.44387364e-05 5.17568320e-03 3.39037080e-03 1.29985405e-03\n",
      " 2.43849259e-02 6.20945492e-02 7.10501278e-07 5.24119727e-02\n",
      " 2.31982421e-03 8.85377413e-01]\n",
      "The last layer activations are: \n",
      "[1.50236865e-05 4.94415374e-03 2.04735941e-03 9.64872084e-04\n",
      " 2.30282735e-02 7.58317735e-02 6.23743040e-07 4.20339427e-02\n",
      " 4.66961664e-03 9.17039822e-01]\n",
      "The last layer activations are: \n",
      "[1.31568497e-05 5.33205057e-03 3.27630346e-03 1.08804769e-03\n",
      " 2.54549195e-02 6.11062199e-02 7.44753630e-07 4.90191303e-02\n",
      " 2.93694576e-03 8.85565933e-01]\n",
      "The last layer activations are: \n",
      "[1.51277133e-05 4.77401549e-03 2.03375818e-03 1.09578235e-03\n",
      " 2.39137470e-02 8.24255137e-02 5.86902557e-07 4.69588995e-02\n",
      " 4.34117259e-03 9.19509604e-01]\n",
      "The last layer activations are: \n",
      "[1.17285308e-05 5.55385563e-03 3.15806348e-03 8.60456211e-04\n",
      " 2.66943066e-02 5.67557449e-02 7.77003266e-07 4.52203677e-02\n",
      " 3.72707929e-03 8.87667259e-01]\n",
      "The last layer activations are: \n",
      "[1.51704589e-05 4.62604943e-03 1.98421585e-03 1.19311053e-03\n",
      " 2.47156261e-02 9.01535203e-02 5.56459233e-07 5.05903336e-02\n",
      " 4.29692142e-03 9.21951638e-01]\n",
      "The last layer activations are: \n",
      "[1.05440425e-05 5.80323465e-03 3.29466419e-03 6.85441628e-04\n",
      " 2.73997972e-02 4.81506873e-02 8.16330880e-07 4.32377436e-02\n",
      " 4.13995349e-03 8.83666561e-01]\n",
      "The last layer activations are: \n",
      "[1.53870231e-05 4.50585619e-03 1.95666326e-03 1.30854049e-03\n",
      " 2.52371564e-02 9.86522834e-02 5.26311245e-07 5.34101872e-02\n",
      " 3.83667118e-03 9.24844059e-01]\n",
      "The last layer activations are: \n",
      "[1.01386705e-05 5.99721977e-03 2.76091147e-03 5.90515597e-04\n",
      " 2.77206449e-02 3.24192152e-02 7.41863729e-07 4.21195874e-02\n",
      " 5.45080594e-03 8.75651223e-01]\n",
      "The last layer activations are: \n",
      "[1.49055536e-05 4.40011448e-03 2.08018748e-03 1.24560519e-03\n",
      " 2.62790321e-02 9.88293660e-02 5.32435829e-07 5.35853589e-02\n",
      " 3.38384209e-03 9.24307431e-01]\n",
      "The last layer activations are: \n",
      "[8.79961491e-06 6.24486454e-03 2.40777867e-03 5.83232295e-04\n",
      " 2.52827444e-02 2.43014410e-02 6.49421426e-07 4.03110599e-02\n",
      " 7.51447644e-03 8.83207513e-01]\n",
      "The last layer activations are: \n",
      "[1.42662111e-05 4.51966985e-03 2.35955735e-03 1.17040113e-03\n",
      " 2.86707779e-02 1.02720922e-01 5.39026325e-07 5.35632135e-02\n",
      " 2.61137094e-03 9.23846149e-01]\n",
      "The last layer activations are: \n",
      "[1.01093715e-05 5.69792011e-03 2.25272935e-03 7.74877764e-04\n",
      " 2.51554774e-02 3.35690003e-02 5.38603745e-07 4.37199770e-02\n",
      " 5.80236737e-03 9.03487701e-01]\n",
      "The last layer activations are: \n",
      "[1.32617240e-05 4.80459606e-03 2.38200018e-03 1.05462042e-03\n",
      " 2.93764441e-02 9.12199897e-02 5.32284143e-07 5.24865413e-02\n",
      " 2.40814179e-03 9.18074255e-01]\n",
      "The last layer activations are: \n",
      "[1.11124595e-05 5.29982623e-03 2.26273162e-03 8.77038800e-04\n",
      " 2.69330799e-02 5.20224351e-02 5.19392655e-07 4.64559462e-02\n",
      " 4.10725155e-03 9.13577614e-01]\n",
      "The last layer activations are: \n",
      "[1.23292488e-05 4.98492192e-03 2.48760111e-03 1.07430723e-03\n",
      " 2.87820757e-02 7.76735712e-02 5.18153035e-07 5.20610993e-02\n",
      " 2.68788086e-03 9.14343740e-01]\n",
      "The last layer activations are: \n",
      "[1.08570259e-05 5.26569026e-03 2.21467839e-03 8.34856805e-04\n",
      " 2.69221140e-02 6.18207171e-02 4.95094265e-07 4.49651130e-02\n",
      " 4.04161860e-03 9.20591391e-01]\n",
      "The last layer activations are: \n",
      "[1.15006608e-05 5.17184490e-03 2.58077669e-03 1.09075455e-03\n",
      " 2.79022441e-02 6.13479392e-02 5.03730788e-07 5.19138836e-02\n",
      " 2.81131966e-03 9.07205900e-01]\n",
      "The last layer activations are: \n",
      "[1.08839090e-05 5.18281316e-03 2.12258466e-03 8.15907032e-04\n",
      " 2.67388564e-02 6.70028227e-02 4.70377626e-07 4.33683686e-02\n",
      " 4.03438068e-03 9.24698488e-01]\n",
      "The last layer activations are: \n",
      "[1.56451489e-05 5.29414421e-03 3.77297493e-03 1.74578576e-03\n",
      " 2.75000209e-02 3.63230526e-02 5.81912175e-07 9.23241379e-02\n",
      " 2.02631745e-03 8.43068262e-01]\n",
      "The last layer activations are: \n",
      "[1.11449513e-05 4.95248665e-03 1.84243542e-03 7.46740030e-04\n",
      " 2.64132409e-02 7.62975145e-02 4.41936044e-07 3.66851802e-02\n",
      " 4.38440734e-03 9.35847681e-01]\n",
      "The last layer activations are: \n",
      "[1.04679158e-05 5.46615386e-03 3.69711501e-03 1.06100620e-03\n",
      " 2.80126743e-02 4.19653318e-02 5.57847758e-07 5.18127233e-02\n",
      " 1.82752850e-03 8.69087804e-01]\n",
      "The last layer activations are: \n",
      "[1.19397222e-05 4.62298451e-03 1.75500442e-03 9.39948607e-04\n",
      " 2.54759640e-02 8.24146081e-02 4.33370466e-07 4.10697346e-02\n",
      " 3.61832284e-03 9.28237098e-01]\n",
      "The last layer activations are: \n",
      "[1.14852003e-05 5.41046365e-03 3.59582419e-03 1.07163667e-03\n",
      " 3.09832172e-02 3.96410391e-02 6.35067736e-07 5.34050168e-02\n",
      " 1.70266125e-03 8.55274339e-01]\n",
      "The last layer activations are: \n",
      "[1.14722455e-05 4.59083506e-03 1.92547055e-03 9.95413597e-04\n",
      " 2.48274738e-02 8.29900755e-02 4.20322632e-07 4.27995419e-02\n",
      " 3.44386930e-03 9.29644499e-01]\n",
      "The last layer activations are: \n",
      "[1.21179691e-05 5.44352222e-03 3.29874487e-03 9.10715323e-04\n",
      " 3.23731024e-02 3.64899492e-02 6.32786851e-07 5.25623443e-02\n",
      " 1.92190934e-03 8.38690367e-01]\n",
      "The last layer activations are: \n",
      "[1.25392145e-05 4.47626712e-03 2.07194027e-03 1.03609337e-03\n",
      " 2.42563553e-02 7.75372479e-02 4.24393733e-07 4.87077478e-02\n",
      " 3.31900951e-03 9.19706425e-01]\n",
      "The last layer activations are: \n",
      "[1.15732317e-05 5.36573060e-03 2.75202290e-03 7.48505571e-04\n",
      " 3.26870253e-02 5.60237668e-02 5.73058845e-07 4.63045554e-02\n",
      " 2.20631704e-03 8.70174257e-01]\n",
      "The last layer activations are: \n",
      "[1.11767435e-05 4.86674566e-03 2.22181100e-03 9.33588055e-04\n",
      " 2.47719941e-02 6.37404281e-02 4.02219191e-07 4.86308612e-02\n",
      " 3.56657587e-03 9.16603118e-01]\n",
      "The last layer activations are: \n",
      "[1.13204686e-05 5.27864662e-03 2.42557387e-03 8.63909827e-04\n",
      " 2.87446388e-02 6.79094631e-02 4.52957705e-07 4.90800378e-02\n",
      " 2.31341782e-03 9.01973373e-01]\n",
      "The last layer activations are: \n",
      "[1.05820567e-05 5.20607666e-03 2.39338196e-03 8.95812031e-04\n",
      " 2.62188366e-02 6.05608825e-02 4.13968526e-07 4.79289655e-02\n",
      " 2.98539469e-03 9.14390396e-01]\n",
      "The last layer activations are: \n",
      "[1.01734272e-05 5.48232009e-03 2.43157377e-03 8.71013630e-04\n",
      " 2.73993469e-02 5.75570200e-02 4.21071324e-07 4.88860489e-02\n",
      " 2.84420877e-03 9.07935330e-01]\n",
      "The last layer activations are: \n",
      "[1.02120155e-05 5.34865439e-03 2.34722401e-03 8.67134951e-04\n",
      " 2.69295225e-02 6.38570793e-02 4.00838794e-07 4.65973944e-02\n",
      " 3.07323943e-03 9.16043427e-01]\n",
      "The last layer activations are: \n",
      "[9.70141067e-06 5.57658700e-03 2.52792309e-03 9.40665425e-04\n",
      " 2.76213439e-02 5.64328241e-02 4.03824387e-07 4.93603647e-02\n",
      " 2.76296681e-03 9.08159924e-01]\n",
      "The last layer activations are: \n",
      "[9.66409797e-06 5.50655890e-03 2.23490108e-03 7.88646108e-04\n",
      " 2.69406796e-02 6.43244050e-02 3.85852480e-07 4.23755804e-02\n",
      " 3.30958984e-03 9.20769635e-01]\n",
      "The last layer activations are: \n",
      "[9.13538314e-06 5.67766430e-03 2.48640738e-03 9.29281559e-04\n",
      " 2.72650676e-02 5.50052305e-02 3.83666310e-07 4.75949034e-02\n",
      " 2.84814142e-03 9.09843835e-01]\n",
      "The last layer activations are: \n",
      "[9.54564404e-06 5.52091646e-03 2.15063030e-03 7.66718810e-04\n",
      " 2.69366007e-02 6.58278782e-02 3.71937147e-07 4.01024639e-02\n",
      " 3.36176067e-03 9.22470490e-01]\n",
      "The last layer activations are: \n",
      "[8.82849599e-06 5.76065516e-03 2.49540784e-03 8.75459091e-04\n",
      " 2.74248984e-02 5.49675399e-02 3.77662162e-07 4.54514166e-02\n",
      " 2.77162462e-03 9.12088428e-01]\n",
      "The last layer activations are: \n",
      "[9.19325396e-06 5.59571860e-03 2.06536013e-03 7.51881051e-04\n",
      " 2.65427271e-02 6.29412440e-02 3.55244816e-07 3.91465650e-02\n",
      " 3.57098205e-03 9.23286148e-01]\n",
      "The last layer activations are: \n",
      "[8.64771764e-06 5.72789024e-03 2.47261139e-03 8.47266422e-04\n",
      " 2.79482020e-02 6.05230432e-02 3.64035507e-07 4.47000373e-02\n",
      " 2.60789037e-03 9.17023606e-01]\n",
      "The last layer activations are: \n",
      "[9.08875567e-06 5.57475891e-03 2.10650996e-03 8.12884191e-04\n",
      " 2.65518432e-02 6.05549239e-02 3.46189308e-07 4.03199021e-02\n",
      " 3.41747506e-03 9.20985611e-01]\n",
      "The last layer activations are: \n",
      "[8.39377887e-06 5.72988925e-03 2.31138995e-03 7.85329411e-04\n",
      " 2.79725992e-02 5.94947713e-02 3.45331924e-07 4.59883391e-02\n",
      " 2.66319165e-03 9.22067774e-01]\n",
      "The last layer activations are: \n",
      "[8.35129046e-06 5.70153572e-03 2.09968782e-03 8.53181439e-04\n",
      " 2.44311544e-02 5.21242890e-02 3.32770952e-07 4.03545167e-02\n",
      " 4.09272395e-03 9.15165065e-01]\n",
      "The last layer activations are: \n",
      "[8.88433201e-06 5.68338398e-03 2.20474018e-03 8.57201574e-04\n",
      " 2.85592849e-02 6.48975668e-02 3.31491526e-07 5.00603166e-02\n",
      " 2.07104544e-03 9.21482736e-01]\n",
      "The last layer activations are: \n",
      "[1.43587168e-06 1.23343376e-02 5.76754942e-04 1.76715219e-04\n",
      " 7.96449980e-03 8.98866842e-04 2.50255664e-07 1.16925533e-02\n",
      " 2.85937040e-01 8.78555450e-01]\n",
      "The last layer activations are: \n",
      "[9.90328241e-06 5.27602756e-03 1.94011699e-03 9.72468042e-04\n",
      " 3.03133820e-02 9.40456703e-02 3.03429280e-07 5.33352884e-02\n",
      " 1.04727533e-03 9.32068544e-01]\n",
      "Testing on batch data:\n",
      "Average cost is:  0.013887219750450855\n",
      "Percentage of correct is:  0.9221666666666667\n",
      "Testing on test data:\n",
      "Average cost is:  0.022153029917169127\n",
      "Percentage of correct is:  0.8625\n",
      "Label for the data is: 9\n",
      "The last layer activations are: \n",
      "[5.72316809e-06 6.39716135e-03 2.63738473e-03 5.49211762e-04\n",
      " 2.31741050e-02 3.95242265e-02 3.94196814e-07 3.37976883e-02\n",
      " 3.18185850e-03 9.12979795e-01]\n",
      "The cost is: 0.0010872308747753583\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[2.68308479e-02 1.66987630e-03 6.30194143e-02 8.46143965e-01\n",
      " 2.62487006e-05 4.63777439e-02 1.35890038e-04 1.62041594e-02\n",
      " 1.57735534e-02 2.47793868e-04]\n",
      "The cost is: 0.003102816443811477\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[1.79460052e-04 5.22493302e-03 2.22954735e-02 2.89910474e-02\n",
      " 2.24326224e-02 7.12716869e-04 1.45024411e-06 8.69073559e-01\n",
      " 6.38666947e-04 1.07527161e-01]\n",
      "The cost is: 0.0030572862899261284\n",
      "Label for the data is: 9\n",
      "The last layer activations are: \n",
      "[8.49545194e-06 5.32504234e-03 2.41457273e-03 7.71525827e-04\n",
      " 2.40038170e-02 5.73703322e-02 3.37467539e-07 3.87750684e-02\n",
      " 2.55171742e-03 9.21000328e-01]\n",
      "The cost is: 0.0011653285138379023\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[1.45437137e-04 4.85925875e-03 5.73135422e-04 1.81111075e-02\n",
      " 7.29509532e-06 3.16558335e-02 1.03035978e-05 1.56908710e-05\n",
      " 9.55498560e-01 2.81621515e-02]\n",
      "The cost is: 0.00041275513730554226\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[2.02679127e-03 3.03484507e-03 4.45488865e-01 3.78150212e-01\n",
      " 3.98689295e-05 2.12114159e-02 1.58021765e-03 8.42063432e-03\n",
      " 1.61573713e-02 1.05406554e-04]\n",
      "The cost is: 0.058595520694738334\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[3.11980198e-04 4.25223561e-03 2.50626831e-02 4.70496807e-02\n",
      " 2.47386560e-02 5.85610787e-04 1.52890418e-06 9.21569879e-01\n",
      " 5.63377983e-04 8.24315692e-02]\n",
      "The cost is: 0.001641889831667662\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[6.12056441e-06 1.39017898e-02 6.76840895e-03 1.34241979e-03\n",
      " 9.95424673e-06 9.12746064e-03 1.18627085e-04 6.91959410e-06\n",
      " 9.51236077e-01 1.08450109e-02]\n",
      "The cost is: 0.00028197324792626245\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[7.15107308e-05 6.92260438e-03 9.45927776e-01 3.50091510e-02\n",
      " 6.44191627e-05 7.30968378e-03 3.29710500e-02 3.46889990e-03\n",
      " 1.67896613e-02 3.51923850e-05]\n",
      "The cost is: 0.0005631826628512971\n",
      "Label for the data is: 6\n",
      "The last layer activations are: \n",
      "[1.86039796e-02 3.25395840e-03 3.43016062e-02 1.56618145e-04\n",
      " 1.92676854e-02 1.83741589e-03 9.16455530e-01 1.99007155e-05\n",
      " 7.23000387e-02 1.78382311e-06]\n",
      "The cost is: 0.0014114915221834507\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[4.89509452e-02 1.15326458e-03 1.59440846e-04 1.70742146e-04\n",
      " 1.30938592e-02 7.69208401e-01 4.93145452e-02 4.12991387e-07\n",
      " 5.13387468e-02 1.07961279e-03]\n",
      "The cost is: 0.006090254772926101\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[6.65060514e-05 6.65743391e-03 9.37425378e-01 2.78867202e-02\n",
      " 7.02188388e-05 9.18082706e-03 3.32050327e-02 2.59670663e-03\n",
      " 1.79723383e-02 4.28938961e-05]\n",
      "The cost is: 0.0006254194656974048\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[3.10544712e-01 1.36032231e-03 4.95619671e-03 4.47480539e-03\n",
      " 1.15506133e-02 1.81279446e-02 3.42789715e-01 8.73642502e-05\n",
      " 1.57039771e-02 6.23206994e-06]\n",
      "The cost is: 0.059360848232491915\n",
      "Label for the data is: 9\n",
      "The last layer activations are: \n",
      "[6.26360710e-06 6.14519742e-03 2.70061107e-03 5.87476544e-04\n",
      " 2.40663859e-02 4.54026904e-02 3.94927283e-07 3.51772917e-02\n",
      " 2.76722120e-03 9.14225841e-01]\n",
      "The cost is: 0.0011288302848583257\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[7.12231017e-05 6.96192154e-03 9.45906026e-01 3.50231651e-02\n",
      " 6.41829214e-05 7.29164517e-03 3.28903290e-02 3.46614863e-03\n",
      " 1.67674507e-02 3.51583688e-05]\n",
      "The cost is: 0.0005629362274084878\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[1.28800598e-04 6.18080730e-03 9.06555055e-01 5.69911865e-02\n",
      " 5.73591230e-05 8.55938944e-03 1.86442904e-02 4.01388527e-03\n",
      " 1.69699021e-02 4.26919589e-05]\n",
      "The cost is: 0.17118708758242585\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[1.62665226e-06 1.59372838e-02 3.73216704e-02 3.41007681e-04\n",
      " 1.75936749e-05 7.43462980e-03 8.12655352e-04 4.81087044e-06\n",
      " 9.42078571e-01 5.72449222e-03]\n",
      "The cost is: 0.0005090616586165217\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[9.39594017e-04 1.93611079e-03 8.18308270e-03 9.82267722e-02\n",
      " 2.83401500e-05 7.81757196e-01 3.62466010e-05 9.04632483e-05\n",
      " 6.16068177e-02 2.26545504e-02]\n",
      "The cost is: 0.006165865336112031\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "[4.96485436e-04 6.48313795e-03 1.69657469e-02 3.12814249e-06\n",
      " 9.59824266e-01 9.77305178e-05 9.58816066e-02 2.21077861e-02\n",
      " 5.39763490e-04 7.48056935e-04]\n",
      "The cost is: 0.001162710089767186\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[6.79825314e-05 6.89251918e-03 9.42264219e-01 3.16950580e-02\n",
      " 6.54121411e-05 7.93631401e-03 3.25907259e-02 3.04671416e-03\n",
      " 1.73503946e-02 3.82621473e-05]\n",
      "The cost is: 0.0005820973436143871\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "# net = Network(4, 3, 3, 2)\n",
    "# net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = False\n",
    "\n",
    "if training:\n",
    "    # #\n",
    "    # # 4 3 3 2 Test run\n",
    "    # #\n",
    "\n",
    "    # dummyData = [[0.125, 0.25, 0.675, 0.885]]\n",
    "    # dummyLabel = [1]\n",
    "\n",
    "    # for i in range(100):\n",
    "    #     net.trainBatch(dummyData, dummyLabel, 5)\n",
    "    #     net.coutLastLayer()\n",
    "    \n",
    "    # net.test(dummyData, dummyLabel)\n",
    "\n",
    "    # #\n",
    "    # # Training on the actual data\n",
    "    # #\n",
    "\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "        data_batch_02 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "        label_batch_02 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "        data_batch_03 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "        label_batch_03 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "        data_batch_04 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "        label_batch_04 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "        data_batch_05 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "        label_batch_05 = pickle.load(infile)\n",
    "\n",
    "    # with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_first_100 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    print(\"Testing on batch data:\")\n",
    "    net.test(data_batch_02, label_batch_02)\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    print(\"Testing on the batch\")\n",
    "    for i in range(400):\n",
    "        # Number in one batch is 6000 so a learning rate of 6000 will result in an npm of 1\n",
    "        net.trainBatch(data_batch_02, label_batch_02, 60)\n",
    "        net.coutLastLayer()\n",
    "        # net.cout()\n",
    "        \n",
    "        # net.test(data_batch_01, label_batch_01)\n",
    "        #net.test(data_test, label_test)\n",
    "\n",
    "    net.checkRandomExamples(data_batch_01, label_batch_01)\n",
    "\n",
    "    print(\"Testing on batch data:\")\n",
    "    net.test(data_batch_01, label_batch_01)\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "    # #\n",
    "    # #\n",
    "    # ### CHECKING\n",
    "    # #\n",
    "    # #\n",
    "\n",
    "    # \n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    net.test(data_test, label_test)\n",
    "    net.checkRandomExamples(data_batch_01, label_batch_01)\n",
    "\n",
    "    net.cout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
