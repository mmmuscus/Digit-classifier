{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 0.1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(-1.0, 0.0) for i in range(currentLayerLen)])\n",
    "        #self.biasVector = np.zeros(currentLayerLen)\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetLayer(self):\n",
    "        self.zVector = np.zeros(self.zVector.size)\n",
    "        self.errorVector = np.zeros(self.errorVector.size)\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen) * 0.001\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # print(\"Sigmoid called with x = \", str(x))\n",
    "    if x <= -700:\n",
    "        x = -700\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    # print(\"Sigmoid deriv called with x = \", str(x))\n",
    "    if x <= -350:\n",
    "        x = -350\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # # FOR 4 3 3 2 test\n",
    "\n",
    "        # for idx in range(0, 4):\n",
    "        #     self.Layers[0].activationVector[idx] = dataset[idx]\n",
    "\n",
    "        #\n",
    "        # FOR MINST\n",
    "        #\n",
    "\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += (1.0 - self.Layers[self.Layers.size - 1].activationVector[i]) * (1.0 - self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "            else:\n",
    "                sum += (self.Layers[self.Layers.size - 1].activationVector[i]) * (self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "        return sum / (self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        # The desired value for target is 1\n",
    "        CGradient = np.zeros(self.Layers[self.Layers.size - 1].size)\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            CGradient[i] = self.Layers[self.Layers.size - 1].activationVector[i]\n",
    "            if i == target:\n",
    "                CGradient[i] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def resetNetwork(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetLayer()\n",
    "\n",
    "    def resetAdjs(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        self.resetAdjs()\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nRESET NETWORK\\n\\n\")\n",
    "            self.resetNetwork()\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nSET START LAYER ACTIVATIONS\\n\\n\")\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL FORWARD PROPAGATION\\n\\n\")\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL BACKWARD PROPAGATION\\n\\n\")\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nADJUST BASED ON GRADIENT DESCENT FOR CURRENT EXAMPLE\\n\\n\")\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "        #     self.cout()\n",
    "\n",
    "        # print(\"\\n\\nADJUST WITH ADJUST VARIABLES\\n\\n\")\n",
    "        self.adjustWithAdjustVariables()\n",
    "        # self.cout()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[self.Layers.size - 1].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def checkRandomExamples(self, data, labels):\n",
    "        numberOfData = len(data)\n",
    "\n",
    "        for num in range (0, 20):\n",
    "            randIdx = random.randint(0, numberOfData)\n",
    "            \n",
    "            self.setStartLayerActivations(data[randIdx])\n",
    "            self.fullForwardPropagation(labels[randIdx])\n",
    "\n",
    "            print(\"Label for the data is: \" + str(labels[randIdx]))\n",
    "            self.coutLastLayer()\n",
    "            print(\"The cost is: \" + str(self.cost(labels[randIdx])))\n",
    "\n",
    "    def coutLastLayer(self):\n",
    "        print(\"The last layer activations are: \")\n",
    "        print(self.Layers[self.Layers.size - 1].activationVector)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on test data:\n",
      "Average cost is:  0.011785253845161815\n",
      "Percentage of correct is:  0.9275\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[9.88678734e-01 1.81148552e-04 1.33024358e-05 8.11018916e-03\n",
      " 2.18093044e-04 2.24673056e-02 9.69837594e-03 3.06396099e-04\n",
      " 6.43998423e-03 1.25832692e-09]\n",
      "The cost is: 8.344323771567187e-05\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[1.83979608e-05 6.32550863e-04 1.41125307e-03 8.98272659e-01\n",
      " 1.04475533e-06 8.48096893e-02 2.03961780e-08 1.53762856e-06\n",
      " 1.20168680e-02 1.02034932e-02]\n",
      "The cost is: 0.16447179831039113\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[9.89080740e-01 1.59617744e-04 1.36633789e-05 8.80419823e-03\n",
      " 2.31995337e-04 2.19837062e-02 9.79150823e-03 3.18600636e-04\n",
      " 6.79891016e-03 1.35987824e-09]\n",
      "The cost is: 8.22307281707184e-05\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[7.09705921e-08 6.25809806e-04 1.07713914e-05 4.51788361e-03\n",
      " 2.04585663e-07 1.37166559e-06 8.31133669e-10 8.69612136e-07\n",
      " 9.94354316e-01 3.97913999e-04]\n",
      "The cost is: 5.2835107143618195e-06\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[2.19849448e-04 1.08382534e-03 8.02767714e-03 2.67451753e-04\n",
      " 1.55754421e-04 2.06526993e-05 4.42951739e-10 9.90563064e-01\n",
      " 8.26650747e-06 5.81634669e-03]\n",
      "The cost is: 1.8864853675950236e-05\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[8.55387168e-14 9.99999976e-01 1.38935263e-10 1.26272229e-09\n",
      " 7.45760664e-10 2.57844119e-14 3.55101438e-13 4.05790888e-07\n",
      " 5.28827996e-06 3.37457881e-08]\n",
      "The cost is: 2.8132265092932935e-12\n",
      "Label for the data is: 6\n",
      "The last layer activations are: \n",
      "[5.88677471e-03 2.91815888e-04 1.31403743e-02 2.35833234e-06\n",
      " 7.92874552e-03 5.77943708e-04 9.75765460e-01 5.53913250e-07\n",
      " 1.38664544e-03 1.38778270e-10]\n",
      "The cost is: 8.598434461423994e-05\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[3.23245901e-05 2.73225434e-04 1.41546127e-03 3.08003340e-05\n",
      " 1.01645787e-04 1.14207168e-06 1.49730338e-10 9.63071938e-01\n",
      " 1.71128931e-03 4.03315706e-03]\n",
      "The cost is: 0.0001384967112187414\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[3.46812163e-04 3.59099915e-04 1.27269643e-02 4.25547566e-04\n",
      " 2.52563786e-04 2.60405087e-05 4.73145788e-10 9.94640586e-01\n",
      " 1.31803895e-05 1.10108472e-02]\n",
      "The cost is: 3.124326606735184e-05\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[2.05870090e-05 1.66188540e-04 9.93460027e-01 1.13601190e-04\n",
      " 1.47341048e-06 5.63140257e-03 4.14652457e-03 2.10988106e-04\n",
      " 7.56747726e-04 1.42000608e-07]\n",
      "The cost is: 9.233573430043061e-06\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[2.26246524e-05 1.76048677e-04 9.90162273e-01 2.06057718e-04\n",
      " 1.26818699e-06 4.08955535e-03 2.59243806e-03 2.15074025e-04\n",
      " 8.70287870e-04 1.69980442e-07]\n",
      "The cost is: 1.211036875172556e-05\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[9.01789039e-03 1.89505067e-04 2.61331123e-02 2.20107963e-06\n",
      " 6.01292661e-03 8.49991389e-04 9.70693753e-01 1.39653032e-06\n",
      " 1.46725373e-03 1.45839738e-10]\n",
      "The cost is: 0.1890783465295399\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[9.32906163e-06 2.20014906e-04 9.87820078e-01 4.30771978e-04\n",
      " 1.64713442e-06 1.83085709e-03 3.71738126e-03 8.81027594e-05\n",
      " 9.45791473e-04 2.18216633e-07]\n",
      "The cost is: 1.6665779893476565e-05\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[1.26390907e-11 9.99999742e-01 3.27708785e-08 5.07670171e-07\n",
      " 1.18990514e-10 3.64611081e-11 6.99895349e-12 2.03225346e-06\n",
      " 7.35508272e-07 3.14542097e-08]\n",
      "The cost is: 4.997616311830964e-13\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[9.85451324e-01 3.20692700e-04 1.13832780e-05 6.47081660e-03\n",
      " 1.65422833e-04 2.14971244e-02 9.25835789e-03 2.40196947e-04\n",
      " 5.15855039e-03 9.30626346e-10]\n",
      "The cost is: 8.281776741495567e-05\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[2.37533085e-05 1.68361083e-04 9.89924757e-01 2.30684686e-04\n",
      " 1.28376768e-06 4.07236225e-03 2.48326732e-03 2.23804103e-04\n",
      " 9.13329980e-04 1.79240954e-07]\n",
      "The cost is: 1.2522766175421484e-05\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[3.30675681e-04 1.38709231e-04 4.15797927e-03 1.67247667e-02\n",
      " 2.88034786e-06 9.91089817e-01 2.74500838e-08 4.62924723e-06\n",
      " 6.45914329e-03 1.09280131e-02]\n",
      "The cost is: 5.376685994844053e-05\n",
      "Label for the data is: 6\n",
      "The last layer activations are: \n",
      "[6.79411557e-03 3.27789598e-04 1.96659385e-02 1.88504981e-06\n",
      " 5.13131247e-03 7.16311462e-04 9.78463136e-01 8.87398000e-07\n",
      " 1.02752019e-03 9.96528095e-11]\n",
      "The cost is: 9.247523748431808e-05\n",
      "Label for the data is: 6\n",
      "The last layer activations are: \n",
      "[1.69773607e-03 6.03510763e-04 9.96396471e-03 1.54596620e-05\n",
      " 4.52155792e-03 9.40289029e-05 9.76537401e-01 2.73205821e-07\n",
      " 1.28393395e-03 1.49692479e-10]\n",
      "The cost is: 6.751227250903873e-05\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[1.70836883e-12 9.99999954e-01 3.20518688e-09 2.57655147e-09\n",
      " 5.39124302e-10 1.53816227e-12 7.87553537e-13 7.26600672e-06\n",
      " 1.99647999e-07 3.84062147e-08]\n",
      "The cost is: 5.283834882959704e-12\n",
      "Testing on test data:\n",
      "Average cost is:  0.012255575622968024\n",
      "Percentage of correct is:  0.9279\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "# net = Network(4, 3, 3, 2)\n",
    "# net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = False\n",
    "\n",
    "if training:\n",
    "    # #\n",
    "    # # 4 3 3 2 Test run\n",
    "    # #\n",
    "\n",
    "    # dummyData = [[0.125, 0.25, 0.675, 0.885]]\n",
    "    # dummyLabel = [1]\n",
    "\n",
    "    # for i in range(100):\n",
    "    #     net.trainBatch(dummyData, dummyLabel, 5)\n",
    "    #     net.coutLastLayer()\n",
    "    \n",
    "    # net.test(dummyData, dummyLabel)\n",
    "\n",
    "    # #\n",
    "    # # Training on the actual data\n",
    "    # #\n",
    "\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "        data_batch_02 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "        label_batch_02 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "        data_batch_03 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "        label_batch_03 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "        data_batch_04 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "        label_batch_04 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "        data_batch_05 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "        label_batch_05 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_06 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_06 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "        data_batch_07 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "        label_batch_07 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "        data_batch_08 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "        label_batch_08 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "        data_batch_09 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "        label_batch_09 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "        data_batch_10 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "        label_batch_10 = pickle.load(infile)\n",
    "\n",
    "    # with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_first_100 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_03, label_batch_03)\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    for i in range(200):\n",
    "        # Number in one batch is 6000 so a learning rate of 6000 will result in an npm of 1\n",
    "        # net.trainBatch(data_batch_01, label_batch_01, 60)\n",
    "        net.trainBatch(data_batch_02, label_batch_02, 60)\n",
    "        net.trainBatch(data_batch_03, label_batch_03, 60)\n",
    "        net.trainBatch(data_batch_04, label_batch_04, 60)\n",
    "        net.trainBatch(data_batch_05, label_batch_05, 60)\n",
    "        net.trainBatch(data_batch_06, label_batch_06, 60)\n",
    "        net.trainBatch(data_batch_07, label_batch_07, 60)\n",
    "        net.trainBatch(data_batch_08, label_batch_08, 60)\n",
    "        net.trainBatch(data_batch_09, label_batch_09, 60)\n",
    "        net.trainBatch(data_batch_10, label_batch_10, 60)\n",
    "        # net.cout()\n",
    "        \n",
    "        # net.test(data_batch_01, label_batch_01)\n",
    "        #net.test(data_test, label_test)\n",
    "\n",
    "    net.checkRandomExamples(data_test, label_test)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_03, label_batch_03)\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "    # #\n",
    "    # #\n",
    "    # ### CHECKING\n",
    "    # #\n",
    "    # #\n",
    "\n",
    "    # \n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    net.test(data_test, label_test)\n",
    "    net.checkRandomExamples(data_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
