{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 0.1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(-1.0, 0.0) for i in range(currentLayerLen)])\n",
    "        #self.biasVector = np.zeros(currentLayerLen)\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetLayer(self):\n",
    "        self.zVector = np.zeros(self.zVector.size)\n",
    "        self.errorVector = np.zeros(self.errorVector.size)\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen) * 0.001\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # print(\"Sigmoid called with x = \", str(x))\n",
    "    if x <= -700:\n",
    "        x = -700\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    # print(\"Sigmoid deriv called with x = \", str(x))\n",
    "    if x <= -350:\n",
    "        x = -350\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # # FOR 4 3 3 2 test\n",
    "\n",
    "        # for idx in range(0, 4):\n",
    "        #     self.Layers[0].activationVector[idx] = dataset[idx]\n",
    "\n",
    "        #\n",
    "        # FOR MINST\n",
    "        #\n",
    "\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += (1.0 - self.Layers[self.Layers.size - 1].activationVector[i]) * (1.0 - self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "            else:\n",
    "                sum += (self.Layers[self.Layers.size - 1].activationVector[i]) * (self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "        return sum / (self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        # The desired value for target is 1\n",
    "        CGradient = np.zeros(self.Layers[self.Layers.size - 1].size)\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            CGradient[i] = self.Layers[self.Layers.size - 1].activationVector[i]\n",
    "            if i == target:\n",
    "                CGradient[i] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def resetNetwork(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetLayer()\n",
    "\n",
    "    def resetAdjs(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        self.resetAdjs()\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nRESET NETWORK\\n\\n\")\n",
    "            self.resetNetwork()\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nSET START LAYER ACTIVATIONS\\n\\n\")\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL FORWARD PROPAGATION\\n\\n\")\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL BACKWARD PROPAGATION\\n\\n\")\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nADJUST BASED ON GRADIENT DESCENT FOR CURRENT EXAMPLE\\n\\n\")\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "        #     self.cout()\n",
    "\n",
    "        # print(\"\\n\\nADJUST WITH ADJUST VARIABLES\\n\\n\")\n",
    "        self.adjustWithAdjustVariables()\n",
    "        # self.cout()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[self.Layers.size - 1].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def checkRandomExamples(self, data, labels):\n",
    "        numberOfData = len(data)\n",
    "\n",
    "        for num in range (0, 20):\n",
    "            randIdx = random.randint(0, numberOfData)\n",
    "            \n",
    "            self.setStartLayerActivations(data[randIdx])\n",
    "            self.fullForwardPropagation(labels[randIdx])\n",
    "\n",
    "            print(\"Label for the data is: \" + str(labels[randIdx]))\n",
    "            self.coutLastLayer()\n",
    "            print(\"The cost is: \" + str(self.cost(labels[randIdx])))\n",
    "\n",
    "    def coutLastLayer(self):\n",
    "        print(\"The last layer activations are: \")\n",
    "        print(self.Layers[self.Layers.size - 1].activationVector)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on the batch\n",
      "The last layer activations are: \n",
      "[0.28371671 0.29959164 0.34073454 0.30840412 0.39878743 0.3600489\n",
      " 0.29022348 0.35838388 0.27012863 0.30623098]\n",
      "The last layer activations are: \n",
      "[1.65976615e-04 1.23707124e-04 6.19969094e-06 4.81204166e-05\n",
      " 2.95160539e-07 1.17978397e-06 1.30835115e-04 3.85631008e-06\n",
      " 2.57568199e-04 5.16015555e-05]\n",
      "The last layer activations are: \n",
      "[1.68612342e-04 1.25680905e-04 6.31353460e-06 4.88753758e-05\n",
      " 3.02128612e-07 1.20467100e-06 1.32885303e-04 3.93043507e-06\n",
      " 2.61834898e-04 5.24076103e-05]\n",
      "The last layer activations are: \n",
      "[1.71327500e-04 1.27714541e-04 6.43119024e-06 4.96533593e-05\n",
      " 3.09367445e-07 1.23046112e-06 1.34997036e-04 4.00710765e-06\n",
      " 2.66232620e-04 5.32381712e-05]\n",
      "The last layer activations are: \n",
      "[1.74125930e-04 1.29810948e-04 6.55285910e-06 5.04555112e-05\n",
      " 3.16892697e-07 1.25720439e-06 1.37173300e-04 4.08646445e-06\n",
      " 2.70767701e-04 5.40944534e-05]\n",
      "The last layer activations are: \n",
      "[1.77011732e-04 1.31973245e-04 6.67875711e-06 5.12830557e-05\n",
      " 3.24721253e-07 1.28495464e-06 1.39417281e-04 4.16865214e-06\n",
      " 2.75446902e-04 5.49777564e-05]\n",
      "The last layer activations are: \n",
      "[1.79989289e-04 1.34204764e-04 6.80911628e-06 5.21373038e-05\n",
      " 3.32871339e-07 1.31376987e-06 1.41732388e-04 4.25382847e-06\n",
      " 2.80277449e-04 5.58894722e-05]\n",
      "The last layer activations are: \n",
      "[1.83063293e-04 1.36509075e-04 6.94418623e-06 5.30196617e-05\n",
      " 3.41362662e-07 1.34371262e-06 1.44122271e-04 4.34216327e-06\n",
      " 2.85267070e-04 5.68310932e-05]\n",
      "The last layer activations are: \n",
      "[1.86238772e-04 1.38890008e-04 7.08423587e-06 5.39316392e-05\n",
      " 3.50216554e-07 1.37485045e-06 1.46590844e-04 4.43383968e-06\n",
      " 2.90424044e-04 5.78042220e-05]\n",
      "The last layer activations are: \n",
      "[1.89521122e-04 1.41351670e-04 7.22955530e-06 5.48748599e-05\n",
      " 3.59456151e-07 1.40725645e-06 1.49142306e-04 4.52905541e-06\n",
      " 2.95757252e-04 5.88105813e-05]\n",
      "The last layer activations are: \n",
      "[1.92916139e-04 1.43898482e-04 7.38045797e-06 5.58510721e-05\n",
      " 3.69106575e-07 1.44100979e-06 1.51781171e-04 4.62802428e-06\n",
      " 3.01276228e-04 5.98520262e-05]\n",
      "The last layer activations are: \n",
      "[1.96430064e-04 1.46535199e-04 7.53728305e-06 5.68621609e-05\n",
      " 3.79195157e-07 1.47619637e-06 1.54512301e-04 4.73097785e-06\n",
      " 3.06991225e-04 6.09305565e-05]\n",
      "The last layer activations are: \n",
      "[2.00069616e-04 1.49266949e-04 7.70039814e-06 5.79101620e-05\n",
      " 3.89751677e-07 1.51290957e-06 1.57340933e-04 4.83816731e-06\n",
      " 3.12913286e-04 6.20483315e-05]\n",
      "The last layer activations are: \n",
      "[2.03842051e-04 1.52099270e-04 7.87020227e-06 5.89972776e-05\n",
      " 4.00808644e-07 1.55125105e-06 1.60272723e-04 4.94986561e-06\n",
      " 3.19054319e-04 6.32076867e-05]\n",
      "The last layer activations are: \n",
      "[2.07755209e-04 1.55038150e-04 8.04712936e-06 6.01258930e-05\n",
      " 4.12401608e-07 1.59133168e-06 1.63313785e-04 5.06636981e-06\n",
      " 3.25427184e-04 6.44111514e-05]\n",
      "The last layer activations are: \n",
      "[2.11817577e-04 1.58090075e-04 8.23165207e-06 6.12985970e-05\n",
      " 4.24569521e-07 1.63327259e-06 1.66470743e-04 5.18800382e-06\n",
      " 3.32045795e-04 6.56614703e-05]\n",
      "The last layer activations are: \n",
      "[2.16038360e-04 1.61262084e-04 8.42428625e-06 6.25182036e-05\n",
      " 4.37355146e-07 1.67720642e-06 1.69750780e-04 5.31512149e-06\n",
      " 3.38925227e-04 6.69616263e-05]\n",
      "The last layer activations are: \n",
      "[2.20427556e-04 1.64561825e-04 8.62559592e-06 6.37877773e-05\n",
      " 4.50805528e-07 1.72327864e-06 1.73161704e-04 5.44811013e-06\n",
      " 3.46081842e-04 6.83148673e-05]\n",
      "The last layer activations are: \n",
      "[2.24996045e-04 1.67997628e-04 8.83619904e-06 6.51106618e-05\n",
      " 4.64972532e-07 1.77164917e-06 1.76712016e-04 5.58739449e-06\n",
      " 3.53533434e-04 6.97247364e-05]\n",
      "The last layer activations are: \n",
      "[2.29755690e-04 1.71578582e-04 9.05677403e-06 6.64905124e-05\n",
      " 4.79913468e-07 1.82249416e-06 1.80410988e-04 5.73344143e-06\n",
      " 3.61299383e-04 7.11951064e-05]\n",
      "The last layer activations are: \n",
      "[2.34719450e-04 1.75314620e-04 9.28806735e-06 6.79313333e-05\n",
      " 4.95691812e-07 1.87600810e-06 1.84268754e-04 5.88676515e-06\n",
      " 3.69400844e-04 7.27302188e-05]\n",
      "The last layer activations are: \n",
      "[2.39901513e-04 1.79216625e-04 9.53090216e-06 6.94375206e-05\n",
      " 5.12378038e-07 1.93240626e-06 1.88296414e-04 6.04793336e-06\n",
      " 3.77860952e-04 7.43347292e-05]\n",
      "The last layer activations are: \n",
      "[2.45317442e-04 1.83296542e-04 9.78618834e-06 7.10139108e-05\n",
      " 5.30050593e-07 1.99192744e-06 1.92506151e-04 6.21757430e-06\n",
      " 3.86705063e-04 7.60137590e-05]\n",
      "The last layer activations are: \n",
      "[2.50984350e-04 1.87567518e-04 1.00549341e-05 7.26658382e-05\n",
      " 5.48797034e-07 2.05483729e-06 1.96911367e-04 6.39638498e-06\n",
      " 3.95961026e-04 7.77729552e-05]\n",
      "The last layer activations are: \n",
      "[2.56921098e-04 1.92044048e-04 1.03382598e-05 7.43991997e-05\n",
      " 5.68715362e-07 2.12143207e-06 2.01526842e-04 6.58514072e-06\n",
      " 4.05659503e-04 7.96185593e-05]\n",
      "The last layer activations are: \n",
      "[2.63148523e-04 1.96742162e-04 1.06374131e-05 7.62205317e-05\n",
      " 5.89915589e-07 2.19204318e-06 2.06368913e-04 6.78470638e-06\n",
      " 4.15834332e-04 8.15574879e-05]\n",
      "The last layer activations are: \n",
      "[2.69689707e-04 2.01679629e-04 1.09537883e-05 7.81370985e-05\n",
      " 6.12521598e-07 2.26704241e-06 2.11455689e-04 6.99604943e-06\n",
      " 4.26522948e-04 8.35974262e-05]\n",
      "The last layer activations are: \n",
      "[2.76570289e-04 2.06876200e-04 1.12889477e-05 8.01569961e-05\n",
      " 6.36673342e-07 2.34684815e-06 2.16807294e-04 7.22025550e-06\n",
      " 4.37766882e-04 8.57469377e-05]\n",
      "The last layer activations are: \n",
      "[2.83818825e-04 2.12353894e-04 1.16446474e-05 8.22892749e-05\n",
      " 6.62529481e-07 2.43193284e-06 2.22446159e-04 7.45854666e-06\n",
      " 4.49612335e-04 8.80155926e-05]\n",
      "The last layer activations are: \n",
      "[2.91467221e-04 2.18137331e-04 1.20228681e-05 8.45440834e-05\n",
      " 6.90270527e-07 2.52283178e-06 2.28397362e-04 7.71230329e-06\n",
      " 4.62110854e-04 9.04141197e-05]\n",
      "The last layer activations are: \n",
      "[2.99551239e-04 2.24254129e-04 1.24258518e-05 8.69328393e-05\n",
      " 7.20102645e-07 2.62015374e-06 2.34689027e-04 7.98309009e-06\n",
      " 4.75320132e-04 9.29545867e-05]\n",
      "The last layer activations are: \n",
      "[3.08111092e-04 2.30735374e-04 1.28561453e-05 8.94684332e-05\n",
      " 7.52262253e-07 2.72459372e-06 2.41352805e-04 8.27268744e-06\n",
      " 4.89304950e-04 9.56506140e-05]\n",
      "The last layer activations are: \n",
      "[3.17192163e-04 2.37616178e-04 1.33166534e-05 9.21654719e-05\n",
      " 7.87021619e-07 2.83694850e-06 2.48424439e-04 8.58312919e-06\n",
      " 5.04138303e-04 9.85176316e-05]\n",
      "The last layer activations are: \n",
      "[3.26845860e-04 2.44936355e-04 1.38107029e-05 9.50405724e-05\n",
      " 8.24695731e-07 2.95813544e-06 2.55944451e-04 8.91674859e-06\n",
      " 5.19902743e-04 1.01573187e-04]\n",
      "The last layer activations are: \n",
      "[3.37130649e-04 2.52741224e-04 1.43421205e-05 9.81127161e-05\n",
      " 8.65650752e-07 3.08921572e-06 2.63958956e-04 9.27623439e-06\n",
      " 5.36691990e-04 1.04837320e-04]\n",
      "The last layer activations are: \n",
      "[3.48113299e-04 2.61082599e-04 1.49153291e-05 1.01403681e-04\n",
      " 9.10314531e-07 3.23142289e-06 2.72520669e-04 9.66469979e-06\n",
      " 5.54612888e-04 1.08333015e-04]\n",
      "The last layer activations are: \n",
      "[3.59870410e-04 2.70019981e-04 1.55354661e-05 1.04938572e-04\n",
      " 9.59189730e-07 3.38619841e-06 2.81690111e-04 1.00857678e-05\n",
      " 5.73787775e-04 1.12086756e-04]\n",
      "The last layer activations are: \n",
      "[3.72490273e-04 2.79622030e-04 1.62085306e-05 1.08746472e-04\n",
      " 1.01287036e-06 3.55523621e-06 2.91537107e-04 1.05436776e-05\n",
      " 5.94357385e-04 1.16129218e-04]\n",
      "The last layer activations are: \n",
      "[3.86075181e-04 2.89968386e-04 1.69415679e-05 1.12861253e-04\n",
      " 1.07206280e-06 3.74053880e-06 3.02142628e-04 1.10434183e-05\n",
      " 6.16484424e-04 1.20496111e-04]\n",
      "The last layer activations are: \n",
      "[4.00744290e-04 3.01151929e-04 1.77429028e-05 1.17322589e-04\n",
      " 1.13761266e-06 3.94448875e-06 3.13601088e-04 1.15908978e-05\n",
      " 6.40358007e-04 1.25229248e-04]\n",
      "The last layer activations are: \n",
      "[4.16637217e-04 3.13281624e-04 1.86224369e-05 1.22177238e-04\n",
      " 1.21053957e-06 4.16994048e-06 3.26023224e-04 1.21931590e-05\n",
      " 6.66199199e-04 1.30377886e-04]\n",
      "The last layer activations are: \n",
      "[4.33918577e-04 3.26486119e-04 1.95920297e-05 1.27480668e-04\n",
      " 1.29208257e-06 4.42033912e-06 3.39539744e-04 1.28586584e-05\n",
      " 6.94268006e-04 1.36000435e-04]\n",
      "The last layer activations are: \n",
      "[4.52783777e-04 3.40918335e-04 2.06659954e-05 1.33299146e-04\n",
      " 1.38375997e-06 4.69987631e-06 3.54305983e-04 1.35976283e-05\n",
      " 7.24872256e-04 1.42166644e-04]\n",
      "The last layer activations are: \n",
      "[4.73466471e-04 3.56761391e-04 2.18617527e-05 1.39712450e-04\n",
      " 1.48744927e-06 5.01369637e-06 3.70507902e-04 1.44225542e-05\n",
      " 7.58379004e-04 1.48960447e-04]\n",
      "The last layer activations are: \n",
      "[4.96248256e-04 3.74236326e-04 2.32006888e-05 1.46817423e-04\n",
      " 1.60549514e-06 5.36817262e-06 3.88369907e-04 1.53488098e-05\n",
      " 7.95229306e-04 1.56483678e-04]\n",
      "The last layer activations are: \n",
      "[5.21471429e-04 3.93612263e-04 2.47093199e-05 1.54732690e-04\n",
      " 1.74085732e-06 5.77128243e-06 4.08165150e-04 1.63955131e-05\n",
      " 8.35957591e-04 1.64861010e-04]\n",
      "The last layer activations are: \n",
      "[5.49555968e-04 4.15219975e-04 2.64208713e-05 1.63604986e-04\n",
      " 1.89731611e-06 6.23312363e-06 4.30229255e-04 1.75866972e-05\n",
      " 8.81217348e-04 1.74246574e-04]\n",
      "The last layer activations are: \n",
      "[5.81022435e-04 4.39470218e-04 2.83774621e-05 1.73617782e-04\n",
      " 2.07976260e-06 6.76663624e-06 4.54978877e-04 1.89529323e-05\n",
      " 9.31815622e-04 1.84832972e-04]\n",
      "The last layer activations are: \n",
      "[6.16523309e-04 4.66878860e-04 3.06331722e-05 1.85003202e-04\n",
      " 2.29461559e-06 7.38862864e-06 4.82937125e-04 2.05336150e-05\n",
      " 9.88760010e-04 1.96863724e-04]\n",
      "The last layer activations are: \n",
      "[6.56886526e-04 4.98101886e-04 3.32584254e-05 1.98058769e-04\n",
      " 2.55043236e-06 8.12126322e-06 5.14768967e-04 2.23802510e-05\n",
      " 1.05332364e-03 2.10650741e-04]\n",
      "The last layer activations are: \n",
      "[7.03177017e-04 5.33984994e-04 3.63463755e-05 2.13171358e-04\n",
      " 2.85882232e-06 8.99425164e-06 5.51331380e-04 2.45612606e-05\n",
      " 1.12713651e-03 2.26599299e-04]\n",
      "The last layer activations are: \n",
      "[7.56785330e-04 5.75635209e-04 4.00224089e-05 2.30852134e-04\n",
      " 3.23584617e-06 1.00481721e-05 5.93745732e-04 2.71691643e-05\n",
      " 1.21231637e-03 2.45244441e-04]\n",
      "The last layer activations are: \n",
      "[8.19557924e-04 6.24526482e-04 4.44586240e-05 2.51788658e-04\n",
      " 3.70421542e-06 1.13396083e-05 6.43504478e-04 3.03315895e-05\n",
      " 1.31165979e-03 2.67305199e-04]\n",
      "The last layer activations are: \n",
      "[8.93993150e-04 6.82659024e-04 4.98964802e-05 2.76924479e-04\n",
      " 4.29685167e-06 1.29493353e-05 7.02632089e-04 3.42285870e-05\n",
      " 1.42892777e-03 2.93767344e-04]\n",
      "The last layer activations are: \n",
      "[9.83543629e-04 7.52806009e-04 5.66832821e-05 3.07584062e-04\n",
      " 5.06283878e-06 1.49957713e-05 7.73934121e-04 3.91207028e-05\n",
      " 1.56928288e-03 3.26013093e-04]\n",
      "The last layer activations are: \n",
      "[1.09309616e-03 8.38906598e-04 6.53329202e-05 3.45674852e-04\n",
      " 6.07775041e-06 1.76578723e-05 8.61393855e-04 4.53960437e-05\n",
      " 1.73997727e-03 3.66030654e-04]\n",
      "The last layer activations are: \n",
      "[1.22975774e-03 9.46712435e-04 7.66308310e-05 3.94025376e-04\n",
      " 7.46233739e-06 2.12156796e-05 9.70824460e-04 5.36522434e-05\n",
      " 1.95146872e-03 4.16764286e-04]\n",
      "The last layer activations are: \n",
      "[1.40418919e-03 1.08488965e-03 9.18231520e-05 4.56972829e-04\n",
      " 9.41799716e-06 2.61254433e-05 1.11098020e-03 6.48454869e-05\n",
      " 2.21929346e-03 4.82721648e-04]\n",
      "The last layer activations are: \n",
      "[1.63295769e-03 1.26697325e-03 1.12974239e-04 5.41428614e-04\n",
      " 1.22978815e-05 3.31661524e-05 1.29552642e-03 8.05750095e-05\n",
      " 2.56732842e-03 5.71072974e-04]\n",
      "The last layer activations are: \n",
      "[1.94286961e-03 1.51498801e-03 1.43679706e-04 6.58904291e-04\n",
      " 1.67586656e-05 4.37425631e-05 1.54668816e-03 1.03657087e-04\n",
      " 3.03371103e-03 6.93736111e-04]\n",
      "The last layer activations are: \n",
      "[2.37932663e-03 1.86647882e-03 1.90576510e-04 8.29568182e-04\n",
      " 2.41084670e-05 6.05547349e-05 1.90233325e-03 1.39357121e-04\n",
      " 3.68205625e-03 8.71539742e-04]\n",
      "The last layer activations are: \n",
      "[3.02321184e-03 2.38882483e-03 2.66756124e-04 1.09081945e-03\n",
      " 3.71709790e-05 8.91896965e-05 2.43039639e-03 1.98219502e-04\n",
      " 4.62364014e-03 1.14299231e-03]\n",
      "The last layer activations are: \n",
      "[4.02640219e-03 3.20959078e-03 4.00000292e-04 1.51634145e-03\n",
      " 6.26242161e-05 1.42205853e-04 3.25949220e-03 3.03030770e-04\n",
      " 6.06280228e-03 1.58368100e-03]\n",
      "The last layer activations are: \n",
      "[0.00568748 0.00458175 0.00065369 0.00225966 0.00011786 0.00025034\n",
      " 0.00464488 0.00050694 0.0083903  0.00235035]\n",
      "The last layer activations are: \n",
      "[0.00860222 0.00701384 0.00117958 0.00364935 0.00025193 0.00049386\n",
      " 0.00710126 0.00094072 0.01235911 0.00377624]\n",
      "The last layer activations are: \n",
      "[0.01386765 0.01144148 0.00232599 0.00634136 0.00060301 0.00107817\n",
      " 0.01158459 0.00191488 0.01929435 0.00652   ]\n",
      "The last layer activations are: \n",
      "[0.0228969  0.01901983 0.00467614 0.01124985 0.00147465 0.00240055\n",
      " 0.01931943 0.00397307 0.0307865  0.01148023]\n",
      "The last layer activations are: \n",
      "[0.03583609 0.02963654 0.00839126 0.01843511 0.00308703 0.00465758\n",
      " 0.03034892 0.0073008  0.04674556 0.01866168]\n",
      "The last layer activations are: \n",
      "[0.05037529 0.04115609 0.01232743 0.0261901  0.00491543 0.00709278\n",
      " 0.04258347 0.01084863 0.063838   0.0263066 ]\n",
      "The last layer activations are: \n",
      "[0.06450023 0.05252617 0.01554731 0.03343158 0.00633009 0.00895336\n",
      " 0.05461813 0.01372656 0.07840207 0.03334194]\n",
      "The last layer activations are: \n",
      "[0.07748499 0.06434416 0.01825026 0.04065327 0.00735078 0.01031538\n",
      " 0.06655661 0.01611975 0.08864248 0.04027042]\n",
      "The last layer activations are: \n",
      "[0.08829493 0.07676773 0.02081551 0.04848845 0.00815912 0.01142143\n",
      " 0.0781359  0.01838611 0.09435566 0.04771086]\n",
      "The last layer activations are: \n",
      "[0.09586436 0.08887666 0.02348896 0.05724409 0.00887025 0.01241895\n",
      " 0.0883051  0.02075958 0.09671574 0.05595997]\n",
      "The last layer activations are: \n",
      "[0.10013044 0.09902973 0.02643697 0.0668468  0.00954753 0.01338863\n",
      " 0.09591582 0.02340378 0.0972507  0.06497113]\n",
      "The last layer activations are: \n",
      "[0.10202009 0.10598261 0.02979779 0.0767561  0.01022921 0.01437966\n",
      " 0.10062162 0.02646413 0.0970144  0.07429908]\n",
      "The last layer activations are: \n",
      "[0.10260789 0.10982701 0.03370427 0.08598537 0.01094181 0.01542681\n",
      " 0.10302053 0.03009511 0.09653557 0.08311428]\n",
      "The last layer activations are: \n",
      "[0.10261185 0.11159475 0.03828917 0.09346197 0.01170643 0.01655831\n",
      " 0.10402101 0.03447442 0.09603426 0.09047152]\n",
      "The last layer activations are: \n",
      "[0.10240053 0.11229499 0.04367667 0.09862167 0.01254266 0.01780055\n",
      " 0.10432045 0.03980826 0.09558829 0.09578833]\n",
      "The last layer activations are: \n",
      "[0.10213683 0.11253066 0.04995342 0.10166555 0.01347087 0.01918081\n",
      " 0.10431475 0.04631891 0.09521622 0.0991195 ]\n",
      "The last layer activations are: \n",
      "[0.10188159 0.11258379 0.05710909 0.10322725 0.01451313 0.02072794\n",
      " 0.1041927  0.05419324 0.09491189 0.10095977]\n",
      "The last layer activations are: \n",
      "[0.10164973 0.11256809 0.06494986 0.10392082 0.01569391 0.02247253\n",
      " 0.10403287 0.06346305 0.09465988 0.10186534]\n",
      "The last layer activations are: \n",
      "[0.10143933 0.11252441 0.07302257 0.10415745 0.01704135 0.02444742\n",
      " 0.10386389 0.07380456 0.09444428 0.1022473 ]\n",
      "The last layer activations are: \n",
      "[0.10124578 0.11246688 0.0806374  0.10416829 0.0185895  0.02668863\n",
      " 0.10369581 0.08434438 0.09425341 0.10235498]\n",
      "The last layer activations are: \n",
      "[0.10106758 0.11240265 0.08707248 0.10407536 0.02038124 0.0292362\n",
      " 0.10353436 0.09375023 0.09408152 0.10232746]\n",
      "The last layer activations are: \n",
      "[0.10090744 0.11233901 0.09188818 0.10394421 0.02247222 0.03213446\n",
      " 0.10338605 0.10084557 0.09392836 0.10224191]\n",
      "The last layer activations are: \n",
      "[0.10076968 0.11228335 0.09509504 0.10381082 0.02493484 0.03542981\n",
      " 0.10325736 0.10532674 0.09379636 0.10214152]\n",
      "The last layer activations are: \n",
      "[0.10065609 0.11223999 0.09703122 0.10369233 0.02786166 0.03916392\n",
      " 0.10315126 0.1077553  0.0936867  0.10204801]\n",
      "The last layer activations are: \n",
      "[0.10056411 0.11220861 0.09811424 0.10359272 0.03136772 0.04336149\n",
      " 0.10306577 0.10893546 0.09359726 0.10196847]\n",
      "The last layer activations are: \n",
      "[0.10048842 0.11218592 0.0986817  0.10350883 0.03559112 0.04801316\n",
      " 0.10299583 0.10946854 0.09352358 0.10190188]\n",
      "The last layer activations are: \n",
      "[0.10042338 0.11216801 0.09895608 0.10343541 0.0406877  0.05305492\n",
      " 0.10293591 0.10969349 0.09346069 0.10184433]\n",
      "The last layer activations are: \n",
      "[0.10036437 0.11215164 0.09906882 0.10336771 0.04681082 0.05834891\n",
      " 0.10288153 0.10977657 0.09340439 0.10179181]\n",
      "The last layer activations are: \n",
      "[0.10030815 0.11213447 0.09909316 0.10330226 0.0540618  0.06367712\n",
      " 0.10282952 0.10979394 0.09335154 0.10174121]\n",
      "The last layer activations are: \n",
      "[0.10025266 0.11211492 0.09906872 0.10323689 0.06239495 0.06876257\n",
      " 0.10277789 0.10977933 0.0933     0.10169041]\n",
      "The last layer activations are: \n",
      "[0.10019684 0.112092   0.09901684 0.10317049 0.07148289 0.07332432\n",
      " 0.10272559 0.10974759 0.09324854 0.10163826]\n",
      "The last layer activations are: \n",
      "[0.10014053 0.11206542 0.09894953 0.10310304 0.08061799 0.07715038\n",
      " 0.10267248 0.1097058  0.09319678 0.10158457]\n",
      "The last layer activations are: \n",
      "[0.10008468 0.1120358  0.09887467 0.10303581 0.08881767 0.0801514\n",
      " 0.10261952 0.1096586  0.09314534 0.10153037]\n",
      "The last layer activations are: \n",
      "[0.10003135 0.11200498 0.09879893 0.10297146 0.09522983 0.08236539\n",
      " 0.10256881 0.10961074 0.09309588 0.10147807]\n",
      "The last layer activations are: \n",
      "[0.09998315 0.11197552 0.09872829 0.10291334 0.09957311 0.08391714\n",
      " 0.10252307 0.10956718 0.09305066 0.10143084]\n",
      "The last layer activations are: \n",
      "[0.09994208 0.11194956 0.09866699 0.10286391 0.10216772 0.08496259\n",
      " 0.10248433 0.10953155 0.09301153 0.10139113]\n",
      "The last layer activations are: \n",
      "[0.09990858 0.11192793 0.09861648 0.10282373 0.10358128 0.08564691\n",
      " 0.10245305 0.10950491 0.09297908 0.10135962]\n",
      "The last layer activations are: \n",
      "[0.0998818  0.11191025 0.0985759  0.10279165 0.10430779 0.08608551\n",
      " 0.10242833 0.10948615 0.09295277 0.10133538]\n",
      "The last layer activations are: \n",
      "[0.09986025 0.11189561 0.09854323 0.1027658  0.10466887 0.08636192\n",
      " 0.10240865 0.10947329 0.09293142 0.10131678]\n",
      "The last layer activations are: \n",
      "[0.09984248 0.1118831  0.09851637 0.10274441 0.10484495 0.08653332\n",
      " 0.10239257 0.10946439 0.09291381 0.10130219]\n",
      "The last layer activations are: \n",
      "[0.09982733 0.111872   0.09849359 0.10272607 0.10492977 0.08663757\n",
      " 0.10237895 0.10945801 0.09289888 0.10129031]\n",
      "The last layer activations are: \n",
      "[0.09981399 0.11186187 0.09847362 0.10270979 0.10497016 0.08669922\n",
      " 0.10236697 0.10945317 0.09288586 0.10128022]\n",
      "The last layer activations are: \n",
      "[0.09980188 0.11185239 0.0984556  0.10269495 0.1049891  0.08673403\n",
      " 0.10235612 0.10944926 0.09287419 0.10127132]\n",
      "The last layer activations are: \n",
      "[0.09979068 0.11184341 0.09843898 0.10268113 0.10499773 0.08675201\n",
      " 0.10234607 0.10944592 0.09286349 0.10126322]\n",
      "The last layer activations are: \n",
      "[0.09978014 0.1118348  0.09842339 0.10266807 0.10500141 0.0867595\n",
      " 0.10233662 0.10944295 0.09285352 0.1012557 ]\n",
      "The last layer activations are: \n",
      "[0.09977012 0.11182651 0.0984086  0.10265563 0.10500274 0.0867605\n",
      " 0.10232763 0.10944022 0.0928441  0.1012486 ]\n",
      "The last layer activations are: \n",
      "[0.09976055 0.11181848 0.09839447 0.10264371 0.10500295 0.08675753\n",
      " 0.10231902 0.10943767 0.09283514 0.10124184]\n",
      "The last layer activations are: \n",
      "[0.09975134 0.1118107  0.0983809  0.10263223 0.10500264 0.08675219\n",
      " 0.10231075 0.10943525 0.09282656 0.10123536]\n",
      "The last layer activations are: \n",
      "[0.09974248 0.11180314 0.09836782 0.10262116 0.10500209 0.08674546\n",
      " 0.10230278 0.10943295 0.09281832 0.10122912]\n",
      "The last layer activations are: \n",
      "[0.09973393 0.11179579 0.0983552  0.10261046 0.10500145 0.08673798\n",
      " 0.10229508 0.10943075 0.09281036 0.1012231 ]\n",
      "The last layer activations are: \n",
      "[0.09972566 0.11178863 0.09834299 0.10260011 0.10500077 0.08673011\n",
      " 0.10228764 0.10942863 0.09280268 0.10121728]\n",
      "The last layer activations are: \n",
      "[0.09971767 0.11178166 0.09833118 0.10259009 0.10500009 0.08672212\n",
      " 0.10228044 0.10942661 0.09279526 0.10121166]\n",
      "The last layer activations are: \n",
      "[0.09970993 0.11177485 0.09831974 0.10258038 0.10499942 0.08671414\n",
      " 0.10227347 0.10942466 0.09278807 0.10120621]\n",
      "The last layer activations are: \n",
      "[0.09970244 0.11176821 0.09830865 0.10257097 0.10499877 0.08670625\n",
      " 0.10226672 0.10942279 0.0927811  0.10120094]\n",
      "The last layer activations are: \n",
      "[0.09969518 0.11176172 0.0982979  0.10256185 0.10499815 0.08669853\n",
      " 0.10226019 0.109421   0.09277435 0.10119584]\n",
      "The last layer activations are: \n",
      "[0.09968814 0.11175538 0.09828747 0.102553   0.10499756 0.08669098\n",
      " 0.10225385 0.10941927 0.0927678  0.10119089]\n",
      "The last layer activations are: \n",
      "[0.09968133 0.11174918 0.09827735 0.10254442 0.10499699 0.08668364\n",
      " 0.1022477  0.10941763 0.09276146 0.1011861 ]\n",
      "The last layer activations are: \n",
      "[0.09967472 0.11174312 0.09826752 0.10253609 0.10499646 0.0866765\n",
      " 0.10224174 0.10941605 0.09275529 0.10118145]\n",
      "The last layer activations are: \n",
      "[0.09966831 0.11173718 0.09825799 0.102528   0.10499597 0.08666956\n",
      " 0.10223596 0.10941454 0.09274932 0.10117695]\n",
      "The last layer activations are: \n",
      "[0.09966209 0.11173136 0.09824873 0.10252015 0.1049955  0.08666283\n",
      " 0.10223035 0.1094131  0.09274351 0.10117259]\n",
      "The last layer activations are: \n",
      "[0.09965606 0.11172565 0.09823973 0.10251252 0.10499508 0.08665631\n",
      " 0.10222491 0.10941172 0.09273787 0.10116836]\n",
      "The last layer activations are: \n",
      "[0.09965021 0.11172006 0.098231   0.10250512 0.10499468 0.08664997\n",
      " 0.10221963 0.10941041 0.0927324  0.10116426]\n",
      "The last layer activations are: \n",
      "[0.09964453 0.11171456 0.0982225  0.10249792 0.10499433 0.08664383\n",
      " 0.1022145  0.10940917 0.09272708 0.10116029]\n",
      "The last layer activations are: \n",
      "[0.09963902 0.11170917 0.09821425 0.10249093 0.10499402 0.08663787\n",
      " 0.10220953 0.10940799 0.09272192 0.10115644]\n",
      "The last layer activations are: \n",
      "[0.09963367 0.11170388 0.09820623 0.10248413 0.10499374 0.08663209\n",
      " 0.10220469 0.10940687 0.0927169  0.10115271]\n",
      "The last layer activations are: \n",
      "[0.09962847 0.11169867 0.09819844 0.10247753 0.1049935  0.08662648\n",
      " 0.1022     0.10940581 0.09271202 0.10114909]\n",
      "The last layer activations are: \n",
      "[0.09962343 0.11169355 0.09819086 0.1024711  0.1049933  0.08662104\n",
      " 0.10219545 0.10940481 0.09270727 0.10114559]\n",
      "The last layer activations are: \n",
      "[0.09961854 0.11168851 0.09818348 0.10246486 0.10499314 0.08661576\n",
      " 0.10219102 0.10940387 0.09270266 0.1011422 ]\n",
      "The last layer activations are: \n",
      "[0.09961379 0.11168355 0.09817632 0.10245879 0.10499303 0.08661064\n",
      " 0.10218672 0.10940299 0.09269818 0.10113891]\n",
      "The last layer activations are: \n",
      "[0.09960917 0.11167867 0.09816935 0.10245289 0.10499295 0.08660567\n",
      " 0.10218255 0.10940216 0.09269383 0.10113573]\n",
      "The last layer activations are: \n",
      "[0.09960469 0.11167386 0.09816257 0.10244715 0.10499292 0.08660085\n",
      " 0.10217849 0.1094014  0.09268959 0.10113265]\n",
      "The last layer activations are: \n",
      "[0.09960034 0.11166911 0.09815597 0.10244156 0.10499292 0.08659617\n",
      " 0.10217456 0.10940069 0.09268547 0.10112967]\n",
      "The last layer activations are: \n",
      "[0.09959612 0.11166443 0.09814955 0.10243613 0.10499297 0.08659163\n",
      " 0.10217073 0.10940003 0.09268147 0.10112678]\n",
      "The last layer activations are: \n",
      "[0.09959202 0.11165982 0.09814331 0.10243085 0.10499307 0.08658722\n",
      " 0.10216702 0.10939943 0.09267757 0.10112399]\n",
      "The last layer activations are: \n",
      "[0.09958804 0.11165526 0.09813724 0.10242572 0.1049932  0.08658295\n",
      " 0.10216341 0.10939889 0.09267378 0.10112129]\n",
      "The last layer activations are: \n",
      "[0.09958418 0.11165076 0.09813134 0.10242072 0.10499339 0.0865788\n",
      " 0.1021599  0.10939839 0.0926701  0.10111868]\n",
      "The last layer activations are: \n",
      "[0.09958043 0.11164631 0.09812559 0.10241586 0.10499361 0.08657478\n",
      " 0.1021565  0.10939795 0.09266652 0.10111616]\n",
      "The last layer activations are: \n",
      "[0.09957679 0.11164191 0.09812    0.10241113 0.10499388 0.08657087\n",
      " 0.1021532  0.10939757 0.09266304 0.10111373]\n",
      "The last layer activations are: \n",
      "[0.09957325 0.11163756 0.09811457 0.10240654 0.10499419 0.08656709\n",
      " 0.10214999 0.10939723 0.09265965 0.10111137]\n",
      "The last layer activations are: \n",
      "[0.09956983 0.11163326 0.09810928 0.10240207 0.10499455 0.08656342\n",
      " 0.10214688 0.10939695 0.09265636 0.10110911]\n",
      "The last layer activations are: \n",
      "[0.0995665  0.111629   0.09810414 0.10239772 0.10499495 0.08655986\n",
      " 0.10214385 0.10939672 0.09265316 0.10110692]\n",
      "The last layer activations are: \n",
      "[0.09956328 0.11162479 0.09809913 0.10239349 0.1049954  0.0865564\n",
      " 0.10214092 0.10939654 0.09265005 0.10110481]\n",
      "The last layer activations are: \n",
      "[0.09956015 0.11162061 0.09809427 0.10238938 0.10499589 0.08655306\n",
      " 0.10213807 0.10939641 0.09264703 0.10110278]\n",
      "The last layer activations are: \n",
      "[0.09955712 0.11161647 0.09808954 0.10238539 0.10499643 0.08654982\n",
      " 0.10213531 0.10939633 0.09264409 0.10110082]\n",
      "The last layer activations are: \n",
      "[0.09955418 0.11161237 0.09808494 0.1023815  0.10499701 0.08654667\n",
      " 0.10213264 0.1093963  0.09264124 0.10109894]\n",
      "The last layer activations are: \n",
      "[0.09955134 0.1116083  0.09808047 0.10237773 0.10499764 0.08654363\n",
      " 0.10213004 0.10939632 0.09263847 0.10109714]\n",
      "The last layer activations are: \n",
      "[0.09954858 0.11160426 0.09807613 0.10237406 0.10499832 0.08654069\n",
      " 0.10212753 0.10939638 0.09263577 0.10109541]\n",
      "The last layer activations are: \n",
      "[0.09954591 0.11160026 0.09807191 0.1023705  0.10499905 0.08653783\n",
      " 0.10212509 0.1093965  0.09263316 0.10109374]\n",
      "The last layer activations are: \n",
      "[0.09954333 0.11159628 0.09806781 0.10236704 0.10499982 0.08653507\n",
      " 0.10212273 0.10939666 0.09263062 0.10109215]\n",
      "The last layer activations are: \n",
      "[0.09954083 0.11159232 0.09806382 0.10236368 0.10500064 0.08653241\n",
      " 0.10212044 0.10939688 0.09262815 0.10109063]\n",
      "The last layer activations are: \n",
      "[0.09953841 0.11158839 0.09805996 0.10236042 0.1050015  0.08652983\n",
      " 0.10211823 0.10939714 0.09262576 0.10108918]\n",
      "The last layer activations are: \n",
      "[0.09953608 0.11158449 0.0980562  0.10235725 0.10500242 0.08652733\n",
      " 0.10211609 0.10939745 0.09262344 0.10108779]\n",
      "The last layer activations are: \n",
      "[0.09953382 0.11158061 0.09805256 0.10235418 0.10500338 0.08652493\n",
      " 0.10211402 0.10939781 0.09262119 0.10108647]\n",
      "The last layer activations are: \n",
      "[0.09953164 0.11157674 0.09804902 0.1023512  0.10500439 0.0865226\n",
      " 0.10211202 0.10939821 0.09261901 0.10108521]\n",
      "The last layer activations are: \n",
      "[0.09952954 0.1115729  0.0980456  0.10234832 0.10500545 0.08652036\n",
      " 0.10211009 0.10939867 0.0926169  0.10108402]\n",
      "The last layer activations are: \n",
      "[0.09952752 0.11156907 0.09804227 0.10234552 0.10500656 0.0865182\n",
      " 0.10210823 0.10939917 0.09261486 0.10108289]\n",
      "The last layer activations are: \n",
      "[0.09952557 0.11156526 0.09803905 0.10234281 0.10500772 0.08651612\n",
      " 0.10210643 0.10939971 0.09261288 0.10108183]\n",
      "The last layer activations are: \n",
      "[0.09952369 0.11156147 0.09803594 0.10234018 0.10500893 0.08651411\n",
      " 0.1021047  0.10940031 0.09261096 0.10108083]\n",
      "The last layer activations are: \n",
      "[0.09952189 0.11155768 0.09803292 0.10233764 0.10501018 0.08651219\n",
      " 0.10210303 0.10940095 0.09260911 0.10107989]\n",
      "The last layer activations are: \n",
      "[0.09952015 0.11155391 0.09803    0.10233519 0.10501149 0.08651034\n",
      " 0.10210143 0.10940164 0.09260732 0.10107901]\n",
      "The last layer activations are: \n",
      "[0.09951849 0.11155015 0.09802717 0.10233281 0.10501285 0.08650856\n",
      " 0.10209988 0.10940238 0.09260559 0.10107819]\n",
      "The last layer activations are: \n",
      "[0.09951689 0.1115464  0.09802444 0.10233052 0.10501426 0.08650686\n",
      " 0.1020984  0.10940317 0.09260392 0.10107743]\n",
      "The last layer activations are: \n",
      "[0.09951537 0.11154265 0.09802181 0.10232831 0.10501573 0.08650523\n",
      " 0.10209698 0.109404   0.09260231 0.10107673]\n",
      "The last layer activations are: \n",
      "[0.09951391 0.11153892 0.09801926 0.10232617 0.10501724 0.08650367\n",
      " 0.10209562 0.10940488 0.09260076 0.10107609]\n",
      "The last layer activations are: \n",
      "[0.09951251 0.11153518 0.09801681 0.10232412 0.10501881 0.08650218\n",
      " 0.10209432 0.10940581 0.09259927 0.10107551]\n",
      "The last layer activations are: \n",
      "[0.09951119 0.11153146 0.09801445 0.10232213 0.10502043 0.08650076\n",
      " 0.10209308 0.10940679 0.09259784 0.10107499]\n",
      "The last layer activations are: \n",
      "[0.09950992 0.11152773 0.09801217 0.10232023 0.1050221  0.08649941\n",
      " 0.10209189 0.10940781 0.09259646 0.10107452]\n",
      "The last layer activations are: \n",
      "[0.09950873 0.11152401 0.09800999 0.1023184  0.10502383 0.08649812\n",
      " 0.10209076 0.10940888 0.09259513 0.10107411]\n",
      "The last layer activations are: \n",
      "[0.09950759 0.11152029 0.09800789 0.10231664 0.10502561 0.08649691\n",
      " 0.10208969 0.10941    0.09259387 0.10107376]\n",
      "The last layer activations are: \n",
      "[0.09950652 0.11151657 0.09800587 0.10231496 0.10502745 0.08649576\n",
      " 0.10208867 0.10941117 0.09259265 0.10107346]\n",
      "The last layer activations are: \n",
      "[0.09950551 0.11151284 0.09800394 0.10231335 0.10502934 0.08649468\n",
      " 0.10208771 0.10941239 0.09259149 0.10107322]\n",
      "The last layer activations are: \n",
      "[0.09950456 0.11150912 0.0980021  0.10231181 0.10503129 0.08649366\n",
      " 0.10208681 0.10941366 0.09259039 0.10107304]\n",
      "The last layer activations are: \n",
      "[0.09950368 0.11150539 0.09800033 0.10231034 0.10503329 0.0864927\n",
      " 0.10208596 0.10941498 0.09258934 0.10107292]\n",
      "The last layer activations are: \n",
      "[0.09950286 0.11150165 0.09799865 0.10230894 0.10503535 0.08649181\n",
      " 0.10208516 0.10941635 0.09258834 0.10107285]\n",
      "The last layer activations are: \n",
      "[0.09950209 0.11149791 0.09799705 0.1023076  0.10503747 0.08649099\n",
      " 0.10208441 0.10941776 0.09258739 0.10107283]\n",
      "The last layer activations are: \n",
      "[0.09950139 0.11149416 0.09799553 0.10230634 0.10503965 0.08649023\n",
      " 0.10208372 0.10941923 0.09258649 0.10107287]\n",
      "The last layer activations are: \n",
      "[0.09950074 0.11149041 0.09799409 0.10230515 0.10504188 0.08648953\n",
      " 0.10208309 0.10942075 0.09258564 0.10107297]\n",
      "The last layer activations are: \n",
      "[0.09950016 0.11148664 0.09799273 0.10230402 0.10504418 0.08648889\n",
      " 0.1020825  0.10942232 0.09258485 0.10107312]\n",
      "The last layer activations are: \n",
      "[0.09949964 0.11148287 0.09799145 0.10230297 0.10504654 0.08648831\n",
      " 0.10208197 0.10942394 0.0925841  0.10107333]\n",
      "The last layer activations are: \n",
      "[0.09949917 0.11147908 0.09799025 0.10230197 0.10504895 0.0864878\n",
      " 0.10208149 0.10942561 0.09258341 0.10107359]\n",
      "The last layer activations are: \n",
      "[0.09949876 0.11147528 0.09798913 0.10230105 0.10505143 0.08648735\n",
      " 0.10208106 0.10942734 0.09258277 0.10107391]\n",
      "The last layer activations are: \n",
      "[0.09949841 0.11147147 0.09798808 0.10230019 0.10505397 0.08648695\n",
      " 0.10208068 0.10942911 0.09258217 0.10107428]\n",
      "The last layer activations are: \n",
      "[0.09949812 0.11146764 0.09798711 0.1022994  0.10505658 0.08648662\n",
      " 0.10208036 0.10943094 0.09258162 0.10107471]\n",
      "The last layer activations are: \n",
      "[0.09949789 0.11146379 0.09798622 0.10229867 0.10505924 0.08648635\n",
      " 0.10208009 0.10943283 0.09258113 0.1010752 ]\n",
      "The last layer activations are: \n",
      "[0.09949772 0.11145993 0.0979854  0.10229801 0.10506198 0.08648615\n",
      " 0.10207986 0.10943477 0.09258068 0.10107574]\n",
      "The last layer activations are: \n",
      "[0.0994976  0.11145605 0.09798466 0.10229741 0.10506478 0.086486\n",
      " 0.10207969 0.10943676 0.09258028 0.10107633]\n",
      "The last layer activations are: \n",
      "[0.09949754 0.11145216 0.097984   0.10229688 0.10506764 0.08648591\n",
      " 0.10207957 0.10943881 0.09257993 0.10107699]\n",
      "The last layer activations are: \n",
      "[0.09949754 0.11144824 0.09798341 0.10229641 0.10507057 0.08648588\n",
      " 0.1020795  0.10944092 0.09257963 0.1010777 ]\n",
      "The last layer activations are: \n",
      "[0.0994976  0.1114443  0.09798289 0.10229601 0.10507358 0.08648591\n",
      " 0.10207949 0.10944308 0.09257938 0.10107846]\n",
      "The last layer activations are: \n",
      "[0.09949772 0.11144034 0.09798246 0.10229567 0.10507665 0.08648601\n",
      " 0.10207952 0.1094453  0.09257917 0.10107929]\n",
      "The last layer activations are: \n",
      "[0.09949789 0.11143635 0.09798209 0.1022954  0.10507979 0.08648616\n",
      " 0.1020796  0.10944758 0.09257901 0.10108017]\n",
      "The last layer activations are: \n",
      "[0.09949812 0.11143234 0.09798181 0.10229519 0.105083   0.08648637\n",
      " 0.10207974 0.10944992 0.09257891 0.1010811 ]\n",
      "The last layer activations are: \n",
      "[0.09949842 0.1114283  0.0979816  0.10229504 0.10508629 0.08648665\n",
      " 0.10207993 0.10945231 0.09257885 0.1010821 ]\n",
      "The last layer activations are: \n",
      "[0.09949876 0.11142424 0.09798146 0.10229496 0.10508964 0.08648698\n",
      " 0.10208017 0.10945477 0.09257884 0.10108315]\n",
      "The last layer activations are: \n",
      "[0.09949917 0.11142015 0.0979814  0.10229495 0.10509308 0.08648738\n",
      " 0.10208046 0.10945729 0.09257887 0.10108427]\n",
      "The last layer activations are: \n",
      "[0.09949964 0.11141602 0.09798142 0.102295   0.10509659 0.08648783\n",
      " 0.1020808  0.10945987 0.09257896 0.10108544]\n",
      "The last layer activations are: \n",
      "[0.09950017 0.11141187 0.09798151 0.10229511 0.10510018 0.08648835\n",
      " 0.10208119 0.10946252 0.09257909 0.10108667]\n",
      "Average cost is:  0.08998094847965524\n",
      "Percentage of correct is:  0.1135\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.09909139 0.11099185 0.09734347 0.10177287 0.10423308 0.08583418\n",
      " 0.10164714 0.10874289 0.09223456 0.10058136]\n",
      "The cost is: 0.09293164341708812\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[0.09939646 0.11130178 0.09781895 0.10216215 0.1048817  0.08632192\n",
      " 0.10197094 0.10928106 0.09249147 0.10095886]\n",
      "The cost is: 0.08792092607558119\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[0.09908525 0.11098562 0.09733391 0.10176504 0.10422004 0.08582437\n",
      " 0.10164063 0.10873207 0.09222939 0.10057377]\n",
      "The cost is: 0.08789969321833412\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[0.09937956 0.11128462 0.09779258 0.10214058 0.10484571 0.08629486\n",
      " 0.101953   0.10925121 0.09247723 0.10093794]\n",
      "The cost is: 0.09030077058647168\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.09936163 0.11126641 0.09776463 0.1021177  0.10480755 0.08626618\n",
      " 0.10193398 0.10921957 0.09246214 0.10091575]\n",
      "The cost is: 0.09061888088839168\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[0.09694459 0.10880528 0.09402    0.09904101 0.09971999 0.08243564\n",
      " 0.0993691  0.10498036 0.0904249  0.09793131]\n",
      "The cost is: 0.08776771725378363\n",
      "Label for the data is: 6\n",
      "The last layer activations are: \n",
      "[0.09948475 0.11139143 0.0979567  0.10227486 0.10506974 0.08646329\n",
      " 0.10206465 0.10943696 0.0925658  0.10106815]\n",
      "The cost is: 0.08979240144505853\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[0.09946586 0.11137226 0.09792722 0.10225074 0.1050295  0.08643304\n",
      " 0.1020446  0.1094036  0.0925499  0.10104477]\n",
      "The cost is: 0.09169020354247416\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.09893857 0.11083652 0.09710559 0.10157797 0.10390884 0.0855903\n",
      " 0.10148495 0.10847363 0.09210584 0.10039235]\n",
      "The cost is: 0.08974157916573212\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[0.09854251 0.1104337  0.09649    0.10107316 0.10307064 0.08495963\n",
      " 0.10106461 0.10777681 0.09177217 0.09990274]\n",
      "The cost is: 0.08786392087633554\n",
      "Label for the data is: 7\n",
      "The last layer activations are: \n",
      "[0.09688956 0.10874907 0.09393534 0.09897117 0.0996055  0.08234931\n",
      " 0.09931072 0.10488448 0.09037847 0.09786353]\n",
      "The cost is: 0.08853757015608986\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.09944837 0.11135449 0.09789993 0.10222841 0.10499223 0.08640503\n",
      " 0.10202604 0.1093727  0.09253517 0.10102311]\n",
      "The cost is: 0.0906154319255688\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.09824121 0.11012701 0.09602259 0.10068942 0.10243504 0.08448121\n",
      " 0.10074486 0.10724768 0.09151827 0.09953053]\n",
      "The cost is: 0.09066563259628627\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.09944446 0.11135052 0.09789382 0.10222342 0.1049839  0.08639876\n",
      " 0.10202189 0.1093658  0.09253188 0.10101827]\n",
      "The cost is: 0.09291459886124753\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.09950424 0.11141122 0.09798712 0.10229974 0.10511128 0.08649452\n",
      " 0.10208534 0.10947139 0.09258221 0.10109228]\n",
      "The cost is: 0.09061322317869594\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.0993629  0.1112677  0.0977666  0.10211931 0.10481025 0.0862682\n",
      " 0.10193532 0.1092218  0.09246321 0.10091732]\n",
      "The cost is: 0.0929185095053876\n",
      "Label for the data is: 6\n",
      "The last layer activations are: \n",
      "[0.0988327  0.11072888 0.09694091 0.10144299 0.10368449 0.08542153\n",
      " 0.10137259 0.10828723 0.09201666 0.10026144]\n",
      "The cost is: 0.08975410897148199\n",
      "Label for the data is: 2\n",
      "The last layer activations are: \n",
      "[0.09923701 0.11113982 0.09757033 0.10195865 0.10454245 0.08606684\n",
      " 0.1018017  0.10899967 0.0923572  0.10076153]\n",
      "The cost is: 0.09062387953584568\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[0.09836133 0.11024932 0.09620885 0.10084238 0.10268823 0.08467181\n",
      " 0.10087234 0.10745853 0.0916195  0.0996789 ]\n",
      "The cost is: 0.08785233168156653\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.09947572 0.11138227 0.09794261 0.10226333 0.1050505  0.08644883\n",
      " 0.10205507 0.10942101 0.0925582  0.10105697]\n",
      "The cost is: 0.0929131041540249\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.22352941 0.55294118 0.99215686\n",
      " 0.3372549  0.33333333 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6627451  0.87843137 0.99215686 0.98431373 0.99215686 0.98431373\n",
      " 0.44705882 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3372549  0.99215686 1.         0.99215686\n",
      " 0.99607843 0.99215686 0.99607843 0.99215686 0.99607843 0.99215686\n",
      " 0.33333333 0.33333333 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.77254902 0.98431373 0.99215686 0.98431373 0.99215686 0.98431373\n",
      " 0.99215686 0.98431373 0.99215686 0.98431373 0.99215686 0.98431373\n",
      " 0.6627451  0.21960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.22352941 1.         0.99215686\n",
      " 1.         0.99215686 0.         0.         0.         0.\n",
      " 0.99607843 0.99215686 0.99607843 0.99215686 0.99607843 0.76862745\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.22352941 0.87843137 0.99215686 0.98431373 0.99215686 0.98431373\n",
      " 0.         0.         0.         0.         0.32941176 0.3254902\n",
      " 0.32941176 0.3254902  0.32941176 0.10980392 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.99215686\n",
      " 1.         0.99215686 0.44705882 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.99215686 0.98431373 0.99215686 0.98431373\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.99607843 0.99215686 0.99607843 0.99215686 0.33333333 0.33333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.99215686 0.98431373\n",
      " 0.99215686 0.98431373 0.99215686 0.98431373 0.6627451  0.21960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.6627451  0.88235294 0.99607843 0.99215686\n",
      " 0.99607843 0.99215686 0.99607843 0.99215686 0.77647059 0.10980392\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.21960784 0.54509804 0.98431373 0.99215686 0.98431373\n",
      " 0.99215686 0.98431373 0.99215686 0.76862745 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.44705882 0.99607843 0.99215686\n",
      " 0.99607843 0.99215686 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54509804 0.98431373 0.99215686 0.98431373\n",
      " 0.44705882 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.22352941\n",
      " 0.22352941 0.         0.         0.         0.         0.\n",
      " 0.11372549 0.77254902 0.99607843 0.99215686 0.6627451  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.65882353 0.88235294 0.65882353\n",
      " 0.         0.         0.         0.         0.33333333 0.98431373\n",
      " 0.99215686 0.98431373 0.65882353 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.99607843 0.99215686 0.99607843 0.99215686\n",
      " 0.33333333 0.33333333 0.99607843 0.99215686 0.99607843 0.99215686\n",
      " 0.6627451  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.99215686 0.98431373 0.99215686 0.98431373 0.99215686 0.98431373\n",
      " 0.99215686 0.98431373 0.99215686 0.98431373 0.21960784 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.44705882\n",
      " 0.99607843 0.99215686 0.99607843 0.99215686 0.99607843 0.99215686\n",
      " 0.88627451 0.65882353 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.32941176 0.76862745\n",
      " 0.99215686 0.98431373 0.99215686 0.54117647 0.21960784 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Biases: \n",
      "[-0.81059092 -0.52333449 -0.0098939  -0.59209522 -0.98937187 -0.04193782\n",
      " -0.44071614 -0.10402282 -0.88675393 -0.92264784 -0.43435632 -0.79819657\n",
      " -0.77022766 -0.02587886 -0.27334204 -0.98630154 -0.76140321 -0.11718551\n",
      " -0.57295887 -0.0580304  -0.80190813 -0.90148061 -0.91202066 -0.2613117\n",
      " -0.78158364 -0.59222242 -0.5602912  -0.20247911 -0.49726514 -0.59694431\n",
      " -0.69259266 -0.31878682 -0.34717616 -0.66965407 -0.86854141 -0.24953775\n",
      " -0.44770138 -0.8618247  -0.95260363 -0.02102477 -0.29322279 -0.54675476\n",
      " -0.73690952 -0.9203162  -0.96853203 -0.49162459 -0.88876769 -0.70278179\n",
      " -0.37130856 -0.14550136 -0.20679881 -0.41943501 -0.97681844 -0.86401772\n",
      " -0.93551936 -0.13816336 -0.90412136 -0.56423722 -0.03703341 -0.33890746\n",
      " -0.58167633 -0.86058591 -0.11891009 -0.98026988 -0.38485371 -0.38991067\n",
      " -0.84643825 -0.60737475 -0.02922452 -0.74578903 -0.27216902 -0.98246019\n",
      " -0.26614301 -0.67322132 -0.31524751 -0.40901873 -0.09850077 -0.14097146\n",
      " -0.54102994 -0.491002   -0.23215556 -0.3391536  -0.88411796 -0.76956894\n",
      " -0.61865505 -0.08774118 -0.33570896 -0.15850754 -0.88312109 -0.83802972\n",
      " -0.32630283 -0.59382916 -0.75594784 -0.41045355 -0.37646516 -0.70314286\n",
      " -0.90193337 -0.40730011 -0.95526618 -0.036623   -0.37051135 -0.29749432\n",
      " -0.7113949  -0.29894835 -0.07346482 -0.16200147 -0.85509452 -0.90899194\n",
      " -0.04264623 -0.99870497 -0.54779074 -0.44501083 -0.24135562 -0.58092006\n",
      " -0.40305316 -0.19944736 -0.85777154 -0.66341571 -0.58602922 -0.93649506\n",
      " -0.83215207 -0.56215643 -0.30291879 -0.80293816 -0.48402472 -0.15969539\n",
      " -0.53575952 -0.52939073 -0.65954135 -0.99658333 -0.68962982 -0.03877699\n",
      " -0.42696438 -0.07581958 -0.69613317 -0.91930322 -0.48872203 -0.33592903\n",
      " -0.26540314 -0.69878503 -0.01979079 -0.51378904 -0.89206135 -0.18589399\n",
      " -0.16866514 -0.13425673 -0.27988608 -0.55539314 -0.2621873  -0.96730577\n",
      " -0.20515344 -0.98540531 -0.16423671 -0.16563092 -0.51246042 -0.41780538\n",
      " -0.81076112 -0.49489924 -0.81207145 -0.31205786 -0.90869782 -0.61833222\n",
      " -0.71981171 -0.78256066 -0.14505589 -0.05641234 -0.46006384 -0.47697687\n",
      " -0.13124661 -0.91564264 -0.26594746 -0.39427527 -0.38444651 -0.44637376\n",
      " -0.03757287 -0.07340924 -0.72102357 -0.81544591 -0.11096584 -0.34719164\n",
      " -0.38228995 -0.93845805 -0.57293351 -0.20021635 -0.50552255 -0.56982017\n",
      " -0.29222329 -0.5588375  -0.00118468 -0.23408922 -0.56772243 -0.90348996\n",
      " -0.59703125 -0.88047551 -0.77679279 -0.8144709  -0.11529048 -0.17019973\n",
      " -0.67077146 -0.01300325 -0.57500783 -0.09408466 -0.29140817 -0.420027\n",
      " -0.46709144 -0.14819374 -0.50788523 -0.78473656 -0.9938394  -0.01128273\n",
      " -0.74817028 -0.41827609 -0.04748742 -0.4099681  -0.67994532 -0.62412199\n",
      " -0.22173484 -0.2885719  -0.43872755 -0.6704019  -0.07881317 -0.82336535\n",
      " -0.12779079 -0.59956432 -0.95666946 -0.65327381 -0.80907263 -0.44036545\n",
      " -0.90482191 -0.58835343 -0.10482469 -0.03685465 -0.47649333 -0.84576334\n",
      " -0.88164051 -0.54660639 -0.52503137 -0.93122414 -0.56379279 -0.64310107\n",
      " -0.02877748 -0.40522243 -0.41655001 -0.04210089 -0.83425181 -0.73317656\n",
      " -0.77298165 -0.34274002 -0.90376232 -0.70653985 -0.97250541 -0.44888164\n",
      " -0.78294973 -0.64377561 -0.05616358 -0.41931248 -0.76336113 -0.06670209\n",
      " -0.99025115 -0.46512503 -0.91590139 -0.57539116 -0.6325219  -0.79801357\n",
      " -0.65185392 -0.24739083 -0.89029113 -0.68448281 -0.47042769 -0.77287477\n",
      " -0.68504659 -0.64906145 -0.39106146 -0.94532519 -0.00209353 -0.57187332\n",
      " -0.95878295 -0.13476811 -0.05936606 -0.40674903 -0.08300028 -0.03949664\n",
      " -0.85152732 -0.8612381  -0.9610333  -0.75234387 -0.10502586 -0.53260048\n",
      " -0.48440626 -0.47000023 -0.10768416 -0.36678457 -0.28627653 -0.05244297\n",
      " -0.31600246 -0.9505964  -0.07771184 -0.73552139 -0.92824596 -0.68667621\n",
      " -0.45005614 -0.86310804 -0.74130078 -0.22780922 -0.86643461 -0.77814029\n",
      " -0.48551069 -0.5755976  -0.43847134 -0.79824274 -0.24338803 -0.2961845\n",
      " -0.22244682 -0.99751597 -0.31792587 -0.01072502 -0.48437783 -0.59781079\n",
      " -0.66163761 -0.04581308 -0.13877891 -0.56357841 -0.00100415 -0.85177208\n",
      " -0.64825742 -0.90692711 -0.2230838  -0.27244905 -0.39813611 -0.10062194\n",
      " -0.45269194 -0.01184623 -0.55904603 -0.23484968 -0.6428136  -0.88772925\n",
      " -0.07672655 -0.81940806 -0.00205401 -0.74540175 -0.6987435  -0.51802508\n",
      " -0.75116244 -0.60851464 -0.96593423 -0.41101805 -0.94677814 -0.44405099\n",
      " -0.29634192 -0.39610921 -0.18480023 -0.93356602 -0.12612585 -0.44512284\n",
      " -0.23129059 -0.69270398 -0.31473941 -0.69724251 -0.97426231 -0.70176986\n",
      " -0.13423375 -0.92794775 -0.30290608 -0.27263487 -0.31587352 -0.70852583\n",
      " -0.0224428  -0.89919809 -0.42098785 -0.326354   -0.07910537 -0.49021553\n",
      " -0.48849187 -0.87899248 -0.684092   -0.93421412 -0.39596463 -0.18813623\n",
      " -0.165716   -0.07977847 -0.26839784 -0.95705139 -0.6452446  -0.04309552\n",
      " -0.32116785 -0.7299405  -0.58499715 -0.82478451 -0.49865836 -0.78770218\n",
      " -0.62375419 -0.07933045 -0.89984808 -0.21838444 -0.04812459 -0.15827796\n",
      " -0.05649644 -0.16625395 -0.70647865 -0.86136094 -0.20096227 -0.41292714\n",
      " -0.87093481 -0.36844944 -0.36312996 -0.61769717 -0.35732787 -0.13719634\n",
      " -0.69891818 -0.62589995 -0.9456339  -0.88042827 -0.80873264 -0.57476157\n",
      " -0.78534    -0.66281998 -0.47442408 -0.49894322 -0.33000948 -0.00900298\n",
      " -0.46200633 -0.81523733 -0.18427838 -0.15860961 -0.43400231 -0.44329926\n",
      " -0.1238957  -0.7832686  -0.91068837 -0.08998113 -0.96256112 -0.96406208\n",
      " -0.37922404 -0.62003933 -0.9620124  -0.40674562 -0.70029894 -0.06779755\n",
      " -0.58172298 -0.79871682 -0.00256259 -0.5509543  -0.67158224 -0.79152249\n",
      " -0.9963528  -0.52432373 -0.57432434 -0.65288177 -0.42447261 -0.7470635\n",
      " -0.3097059  -0.56314104 -0.41828319 -0.76650059 -0.9426322  -0.62885523\n",
      " -0.97733729 -0.58052168 -0.83749487 -0.80097983 -0.13944431 -0.21853072\n",
      " -0.61107078 -0.80164644 -0.7319158  -0.8720573  -0.23818864 -0.63058633\n",
      " -0.72961956 -0.37213366 -0.20965635 -0.16603027 -0.57919701 -0.71883523\n",
      " -0.51781497 -0.91682889 -0.68586122 -0.8242185  -0.7179241  -0.07429162\n",
      " -0.30564411 -0.10675254 -0.21618736 -0.89482515 -0.71391876 -0.64957337\n",
      " -0.73486044 -0.68166972 -0.48602132 -0.33524306 -0.69767541 -0.10404421\n",
      " -0.17226242 -0.55435669 -0.54295391 -0.41711093 -0.57905669 -0.91361664\n",
      " -0.71747546 -0.24355795 -0.12193792 -0.48628952 -0.00562037 -0.00857\n",
      " -0.71914906 -0.92765934 -0.41477654 -0.9194745  -0.17671596 -0.49808492\n",
      " -0.14622244 -0.81990964 -0.90137498 -0.08266541 -0.67462059 -0.95128237\n",
      " -0.80041193 -0.26817772 -0.19895361 -0.55397746 -0.70143912 -0.3060856\n",
      " -0.43986024 -0.16411444 -0.16848069 -0.48225824 -0.33550366 -0.287146\n",
      " -0.05296372 -0.44265434 -0.34326957 -0.07585525 -0.24816778 -0.19007032\n",
      " -0.89073034 -0.82487976 -0.42221694 -0.45948807 -0.0754388  -0.08961254\n",
      " -0.7155048  -0.8552898  -0.93809533 -0.65974419 -0.57231314 -0.23283127\n",
      " -0.97465229 -0.14154346 -0.02516157 -0.43809113 -0.58693539 -0.64323031\n",
      " -0.20472906 -0.37665823 -0.95520234 -0.73998229 -0.45110202 -0.36115512\n",
      " -0.59356716 -0.87953334 -0.53018059 -0.74902993 -0.83770682 -0.18659961\n",
      " -0.6218055  -0.56621227 -0.34138307 -0.6912311  -0.57620518 -0.1721962\n",
      " -0.81764965 -0.51135511 -0.56364422 -0.31723338 -0.50716339 -0.75911021\n",
      " -0.90008539 -0.26287379 -0.87892302 -0.53241253 -0.52178918 -0.03040794\n",
      " -0.76166193 -0.84532259 -0.82628378 -0.60319699 -0.49320308 -0.71578007\n",
      " -0.28890034 -0.82844175 -0.62221977 -0.5451606  -0.97616475 -0.12226336\n",
      " -0.63831233 -0.44929862 -0.46456453 -0.86204301 -0.9253302  -0.82365088\n",
      " -0.50772526 -0.56731381 -0.86365596 -0.69356931 -0.79671152 -0.44085587\n",
      " -0.61191632 -0.57615032 -0.39930652 -0.08286508 -0.03684755 -0.48463293\n",
      " -0.95499076 -0.7159346  -0.39138654 -0.4210621  -0.11212897 -0.92265304\n",
      " -0.0198319  -0.96968825 -0.95685251 -0.78210286 -0.33176915 -0.84440976\n",
      " -0.69649671 -0.00932075 -0.64903992 -0.22045105 -0.98700486 -0.37605902\n",
      " -0.63212836 -0.73681043 -0.96821843 -0.50996656 -0.80660147 -0.83476345\n",
      " -0.39404782 -0.33898062 -0.04860548 -0.12008465 -0.59126791 -0.46744211\n",
      " -0.33458335 -0.95605072 -0.89456034 -0.67539316 -0.37980182 -0.35509591\n",
      " -0.19491643 -0.12058519 -0.56089464 -0.28894477 -0.3800757  -0.79979441\n",
      " -0.7957978  -0.53167512 -0.4989921  -0.25758504 -0.08549318 -0.95359706\n",
      " -0.35033141 -0.80753299 -0.26879835 -0.20361151 -0.52748842 -0.6829247\n",
      " -0.93771665 -0.43602965 -0.43086683 -0.19526764 -0.54361179 -0.59572709\n",
      " -0.01737942 -0.2933928  -0.05595869 -0.1291752  -0.94673401 -0.96372209\n",
      " -0.87152205 -0.64366215 -0.42866615 -0.33303855 -0.7855627  -0.73739749\n",
      " -0.32278122 -0.76207542 -0.59922881 -0.3526425  -0.8072042  -0.13831227\n",
      " -0.53059325 -0.15406985 -0.02848803 -0.53432991 -0.31794688 -0.92626299\n",
      " -0.93273906 -0.12270305 -0.19528654 -0.05376204 -0.93262523 -0.01592539\n",
      " -0.68866652 -0.88477077 -0.33514955 -0.45801456 -0.52776873 -0.51552056\n",
      " -0.91846791 -0.87652713 -0.54949324 -0.63149003 -0.29888757 -0.10649095\n",
      " -0.84745425 -0.36062583 -0.23020568 -0.88404289 -0.56374189 -0.94098214\n",
      " -0.1541997  -0.17129316 -0.94245281 -0.53753351 -0.80934946 -0.3636135\n",
      " -0.97481817 -0.99433748 -0.35585291 -0.36406978 -0.68975577 -0.14434107\n",
      " -0.34655768 -0.91339665 -0.94769393 -0.44255777 -0.88480355 -0.82534864\n",
      " -0.35836755 -0.31884226 -0.16208451 -0.83780316 -0.69200982 -0.86463226\n",
      " -0.79494224 -0.92775123 -0.11239189 -0.65601955 -0.50890385 -0.40536787\n",
      " -0.61757634 -0.34489936 -0.04476859 -0.17223792 -0.87190842 -0.79739317\n",
      " -0.47882929 -0.84459982 -0.82732292 -0.97367517 -0.03017169 -0.93932517\n",
      " -0.85770888 -0.83337982 -0.78957669 -0.25454464 -0.2906462  -0.21094734\n",
      " -0.54422038 -0.97188688 -0.31647305 -0.38075067 -0.25788309 -0.82078183\n",
      " -0.9824616  -0.0236971  -0.13595069 -0.42780478 -0.19679944 -0.96715999\n",
      " -0.356853   -0.72217018 -0.39419502 -0.45793598]\n",
      "Z Vector: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Error: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "[[8.99081822e-04 2.69197334e-04 4.68394249e-04 ... 3.40266570e-05\n",
      "  1.23844324e-04 7.24484876e-04]\n",
      " [1.88024490e-04 1.72296246e-04 3.55469117e-04 ... 3.62994427e-04\n",
      "  9.26089950e-04 5.10219762e-04]\n",
      " [5.70203155e-04 8.53110611e-05 6.04223019e-04 ... 4.41054054e-04\n",
      "  8.77344943e-05 5.41732052e-04]\n",
      " ...\n",
      " [2.24212796e-05 7.38126541e-04 4.38343371e-04 ... 2.64761273e-04\n",
      "  2.84389790e-04 4.26051268e-04]\n",
      " [7.86060276e-05 1.23291018e-04 9.65467220e-04 ... 2.55707429e-04\n",
      "  4.40968058e-04 2.91082491e-04]\n",
      " [2.54280319e-04 4.00585801e-04 2.89419163e-04 ... 7.48146586e-04\n",
      "  6.40608970e-04 2.31857666e-05]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.99606835 0.99485914 0.99627161 0.99628571 0.99540482 0.99354633\n",
      " 0.99457225 0.99613621 0.99636808 0.99589492 0.99578211 0.99587245\n",
      " 0.99636717 0.99457325 0.99635305 0.99591995]\n",
      "Biases: \n",
      "[-0.29363991 -0.66867127 -0.10796954 -0.08381779 -0.5479198  -0.83580271\n",
      " -0.71478442 -0.24254816  0.09377158 -0.37318537 -0.41701294 -0.38370113\n",
      "  0.11667673 -0.71458022  0.04294697 -0.36197535]\n",
      "Z Vector: \n",
      "[5.53475627 5.2653816  5.5880442  5.59184739 5.37814052 5.03663187\n",
      " 5.21078898 5.55223657 5.6143549  5.49141628 5.46419269 5.48593508\n",
      " 5.6141025  5.21097253 5.61021141 5.4975566 ]\n",
      "Error: \n",
      "[2.21833978e-07 2.42597080e-07 2.25403316e-07 2.26859767e-07\n",
      " 2.30956745e-07 2.73470993e-07 2.49623818e-07 2.22242231e-07\n",
      " 2.34803636e-07 2.23585241e-07 2.26248912e-07 2.23923606e-07\n",
      " 2.36990204e-07 2.48365463e-07 2.31348909e-07 2.22968530e-07]\n",
      "\n",
      "[[-0.37355817 -0.32253298 -0.39428523 -0.39656602 -0.34092557 -0.29577065\n",
      "  -0.31513681 -0.37953129 -0.41312478 -0.3637872  -0.35761328 -0.36197004\n",
      "  -0.41481048 -0.31586937 -0.40872927 -0.36491744]\n",
      " [-0.28247741 -0.25101733 -0.29409132 -0.29599778 -0.26200621 -0.23304733\n",
      "  -0.24669672 -0.28598688 -0.30555272 -0.27653454 -0.27278134 -0.27531975\n",
      "  -0.30673211 -0.2467598  -0.30326204 -0.27772397]\n",
      " [-0.36168324 -0.31381872 -0.38042451 -0.38287937 -0.32998023 -0.28702296\n",
      "  -0.30694235 -0.36665645 -0.39936868 -0.35194159 -0.3462598  -0.35025186\n",
      "  -0.40022616 -0.30656276 -0.39459296 -0.35385676]\n",
      " [-0.30819335 -0.27172995 -0.32219265 -0.32353245 -0.28395011 -0.2513162\n",
      "  -0.26567952 -0.31153783 -0.33495042 -0.30085154 -0.29633093 -0.29971341\n",
      "  -0.33640376 -0.26573936 -0.33163421 -0.30195202]\n",
      " [-0.35761638 -0.31100368 -0.37649332 -0.37925277 -0.326866   -0.28522768\n",
      "  -0.30417516 -0.36330217 -0.39503521 -0.34950598 -0.34370056 -0.34742708\n",
      "  -0.39592001 -0.3046294  -0.39047912 -0.34969458]\n",
      " [-0.31857638 -0.27978056 -0.33381401 -0.33528138 -0.29379773 -0.25846996\n",
      "  -0.27398746 -0.32329622 -0.34784283 -0.31085381 -0.30611225 -0.30952274\n",
      "  -0.34933285 -0.27450599 -0.34490482 -0.31212032]\n",
      " [-0.36844176 -0.3186871  -0.38799203 -0.39049786 -0.33629118 -0.29253127\n",
      "  -0.31141524 -0.37400884 -0.40781302 -0.35938193 -0.35258049 -0.35698207\n",
      "  -0.40866445 -0.31217269 -0.403211   -0.3604189 ]\n",
      " [-0.28184249 -0.25076848 -0.29366355 -0.2944005  -0.26158168 -0.23242158\n",
      "  -0.24602688 -0.2847812  -0.30447122 -0.27572813 -0.27229972 -0.27515159\n",
      "  -0.30559821 -0.24564449 -0.30244522 -0.27637707]\n",
      " [-0.37534168 -0.32415944 -0.39517496 -0.39740655 -0.34148013 -0.29604405\n",
      "  -0.31597384 -0.38089301 -0.41544898 -0.36477816 -0.35856383 -0.36384242\n",
      "  -0.41637747 -0.31671132 -0.4100066  -0.36641589]\n",
      " [-0.33471714 -0.29312938 -0.35198683 -0.35339857 -0.30766741 -0.26916041\n",
      "  -0.28655252 -0.33975888 -0.36697354 -0.32763847 -0.322415   -0.32539265\n",
      "  -0.36884153 -0.28676197 -0.36347882 -0.32781491]\n",
      " [-0.33062082 -0.28946423 -0.3461361  -0.3478503  -0.30392222 -0.26624837\n",
      "  -0.28270447 -0.33526047 -0.3617248  -0.32219622 -0.31699181 -0.32066728\n",
      "  -0.36245838 -0.28376815 -0.35765632 -0.32378477]\n",
      " [-0.30282504 -0.26736589 -0.31618155 -0.31771181 -0.27981629 -0.2472947\n",
      "  -0.26254165 -0.30697068 -0.32973097 -0.29639988 -0.29182447 -0.29518848\n",
      "  -0.33035239 -0.26234774 -0.32595921 -0.29733119]\n",
      " [-0.32937422 -0.28831057 -0.3449106  -0.34712788 -0.30264006 -0.26576807\n",
      "  -0.28263443 -0.33342945 -0.36078283 -0.32156864 -0.31663611 -0.32003927\n",
      "  -0.36149267 -0.28237943 -0.35712525 -0.32209355]\n",
      " [-0.33322444 -0.29173471 -0.35029339 -0.35157937 -0.30601443 -0.26879406\n",
      "  -0.28588011 -0.33851759 -0.3653728  -0.3259074  -0.32049076 -0.3243555\n",
      "  -0.36649126 -0.28520973 -0.36201436 -0.32617475]\n",
      " [-0.29938032 -0.26493996 -0.31268893 -0.31483295 -0.27766108 -0.24551584\n",
      "  -0.2599217  -0.30356464 -0.32610915 -0.29368949 -0.28893493 -0.2918347\n",
      "  -0.32623638 -0.25976046 -0.32307173 -0.29448542]\n",
      " [-0.36601742 -0.31695203 -0.38480644 -0.38722931 -0.33388375 -0.29030998\n",
      "  -0.30979784 -0.37085025 -0.40394605 -0.35612191 -0.35038902 -0.35501334\n",
      "  -0.40570049 -0.31024542 -0.39931249 -0.35694819]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.00160591 0.00335698 0.00176059 0.00270891 0.00180717 0.00247755\n",
      " 0.00166888 0.00338087 0.0015907  0.00216588 0.00225146 0.00282334\n",
      " 0.00227006 0.00219417 0.00289755 0.00170456]\n",
      "Biases: \n",
      "[-0.63824509 -1.29633721 -0.73200203 -1.12341609 -0.75776298 -1.04713895\n",
      " -0.67742932 -1.30195192 -0.62837076 -0.92947899 -0.96456099 -1.1574729\n",
      " -0.97139445 -0.94000866 -1.17846584 -0.69962566]\n",
      "Z Vector: \n",
      "[-6.4324547  -5.69335111 -6.34034146 -5.90849558 -6.31418189 -5.99800489\n",
      " -6.39392987 -5.68623478 -6.44198895 -6.13276017 -6.09392137 -5.8670087\n",
      " -6.08567767 -6.11975528 -5.8409873  -6.3727436 ]\n",
      "Error: \n",
      "[-1.81930882e-05 -2.08512111e-05 -1.85779366e-05 -2.01407452e-05\n",
      " -1.86916361e-05 -1.97086159e-05 -1.83043050e-05 -2.06773897e-05\n",
      " -1.81079180e-05 -1.93343987e-05 -1.95495250e-05 -2.02011064e-05\n",
      " -1.95259646e-05 -1.93772803e-05 -2.02301442e-05 -1.84499580e-05]\n",
      "\n",
      "[[-0.9930983  -0.57781055 -0.93017082 -0.67869715 -0.91331773 -0.72454959\n",
      "  -0.96618231 -0.57563868 -0.99940526 -0.79947399 -0.77694226 -0.65820846\n",
      "  -0.77271525 -0.79223296 -0.64502118 -0.95139065]\n",
      " [-0.95704777 -0.4884292  -0.88696634 -0.60378977 -0.86767503 -0.65642155\n",
      "  -0.92693783 -0.48498468 -0.96482555 -0.74178814 -0.71630348 -0.5802507\n",
      "  -0.71093594 -0.73307342 -0.56588981 -0.91124357]\n",
      " [-1.55902912 -0.92469534 -1.46295618 -1.07867684 -1.43656583 -1.15014369\n",
      "  -1.5184135  -0.91984929 -1.56925638 -1.26417373 -1.23036249 -1.04774393\n",
      "  -1.22385106 -1.25316729 -1.02778163 -1.4960748 ]\n",
      " [-1.23129969 -0.72391356 -1.15486276 -0.84777263 -1.13386894 -0.9043675\n",
      "  -1.19923043 -0.72019137 -1.23929479 -0.99649751 -0.96820226 -0.82290948\n",
      "  -0.96285844 -0.98693924 -0.80735153 -1.18142932]\n",
      " [-2.00521681 -1.18184176 -1.88008218 -1.3823907  -1.84586138 -1.47446852\n",
      "  -1.95269748 -1.17614228 -2.01750878 -1.62332122 -1.57855107 -1.34216134\n",
      "  -1.56972865 -1.60779965 -1.31649569 -1.92239459]\n",
      " [-1.79373858 -1.05885073 -1.68247302 -1.2377843  -1.65129731 -1.31961315\n",
      "  -1.74625655 -1.05318448 -1.80448667 -1.45219644 -1.41275907 -1.20127241\n",
      "  -1.40524072 -1.43908575 -1.17848939 -1.71997848]\n",
      " [-1.03389002 -0.59597132 -0.96764039 -0.70239138 -0.94939599 -0.75111747\n",
      "  -1.00619859 -0.59288895 -1.04076944 -0.83078228 -0.80634751 -0.68110926\n",
      "  -0.80183458 -0.82220032 -0.66746676 -0.99047242]\n",
      " [-1.61157684 -0.9377549  -1.50926517 -1.10208414 -1.48132866 -1.17734917\n",
      "  -1.56826653 -0.93228419 -1.62178701 -1.29947634 -1.263015   -1.06885619\n",
      "  -1.25591293 -1.28740868 -1.04789174 -1.54417981]\n",
      " [-0.89280269 -0.51934948 -0.83602471 -0.60809174 -0.82048728 -0.65053376\n",
      "  -0.8682471  -0.51634996 -0.89900909 -0.71815273 -0.69732764 -0.59043744\n",
      "  -0.69335383 -0.71102224 -0.57876046 -0.85512843]\n",
      " [-1.21089221 -0.70531363 -1.13442207 -0.82865051 -1.11339362 -0.88584337\n",
      "  -1.17888021 -0.70233342 -1.21905252 -0.97680033 -0.94882432 -0.80418452\n",
      "  -0.94382391 -0.96753863 -0.78862291 -1.16046023]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[0.09947572 0.11138227 0.09794261 0.10226333 0.1050505  0.08644883\n",
      " 0.10205507 0.10942101 0.0925582  0.10105697]\n",
      "Biases: \n",
      "[-2.17513658 -2.0511366  -2.17608274 -2.13752508 -2.08560837 -2.30702562\n",
      " -2.1456285  -2.0513271  -2.25771504 -2.15144204]\n",
      "Z Vector: \n",
      "[-2.20306354 -2.07669903 -2.2202965  -2.17232563 -2.1423261  -2.35778669\n",
      " -2.17459619 -2.09666889 -2.28279178 -2.18553521]\n",
      "Error: \n",
      "[ 0.0089152   0.01102969  0.00865972  0.00939384  0.00988511  0.00683328\n",
      "  0.00935683  0.01067046  0.0077774  -0.0816826 ]\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "# net = Network(4, 3, 3, 2)\n",
    "# net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = True\n",
    "\n",
    "if training:\n",
    "    # #\n",
    "    # # 4 3 3 2 Test run\n",
    "    # #\n",
    "\n",
    "    # dummyData = [[0.125, 0.25, 0.675, 0.885]]\n",
    "    # dummyLabel = [1]\n",
    "\n",
    "    # for i in range(100):\n",
    "    #     net.trainBatch(dummyData, dummyLabel, 5)\n",
    "    #     net.coutLastLayer()\n",
    "    \n",
    "    # net.test(dummyData, dummyLabel)\n",
    "\n",
    "    # #\n",
    "    # # Training on the actual data\n",
    "    # #\n",
    "\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_01 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_05 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_05 = pickle.load(infile)\n",
    "\n",
    "    # with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_first_100 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    print(\"Testing on the batch\")\n",
    "    for i in range(400):\n",
    "        # Number in one batch is 6000 so a learning rate of 6000 will result in an npm of 1\n",
    "        net.trainBatch(data_batch_01, label_batch_01, 60)\n",
    "        net.coutLastLayer()\n",
    "        # net.cout()\n",
    "        \n",
    "        # net.test(data_batch_01, label_batch_01)\n",
    "        #net.test(data_test, label_test)\n",
    "\n",
    "    # print(\"Testing on batch data:\")\n",
    "    # net.test(data_batch_01, label_batch_01)\n",
    "    # print(\"Testing on test data:\")\n",
    "    # net.test(data_test, label_test)\n",
    "\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "    # #\n",
    "    # #\n",
    "    # ### CHECKING\n",
    "    # #\n",
    "    # #\n",
    "\n",
    "    # \n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    net.test(data_test, label_test)\n",
    "    net.checkRandomExamples(data_batch_01, label_batch_01)\n",
    "\n",
    "    net.cout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
