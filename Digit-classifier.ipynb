{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 0.1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(-1.0, 0.0) for i in range(currentLayerLen)])\n",
    "        #self.biasVector = np.zeros(currentLayerLen)\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetLayer(self):\n",
    "        self.zVector = np.zeros(self.zVector.size)\n",
    "        self.errorVector = np.zeros(self.errorVector.size)\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen) * 0.001\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # print(\"Sigmoid called with x = \", str(x))\n",
    "    if x <= -700:\n",
    "        x = -700\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    # print(\"Sigmoid deriv called with x = \", str(x))\n",
    "    if x <= -350:\n",
    "        x = -350\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) * (1 + np.exp(-x)))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # # FOR 4 3 3 2 test\n",
    "\n",
    "        # for idx in range(0, 4):\n",
    "        #     self.Layers[0].activationVector[idx] = dataset[idx]\n",
    "\n",
    "        #\n",
    "        # FOR MINST\n",
    "        #\n",
    "\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += (1.0 - self.Layers[self.Layers.size - 1].activationVector[i]) * (1.0 - self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "            else:\n",
    "                sum += (self.Layers[self.Layers.size - 1].activationVector[i]) * (self.Layers[self.Layers.size - 1].activationVector[i])\n",
    "        return sum / (self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        # The desired value for target is 1\n",
    "        CGradient = np.zeros(self.Layers[self.Layers.size - 1].size)\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            CGradient[i] = self.Layers[self.Layers.size - 1].activationVector[i]\n",
    "            if i == target:\n",
    "                CGradient[i] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def resetNetwork(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetLayer()\n",
    "\n",
    "    def resetAdjs(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        self.resetAdjs()\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nRESET NETWORK\\n\\n\")\n",
    "            self.resetNetwork()\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nSET START LAYER ACTIVATIONS\\n\\n\")\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL FORWARD PROPAGATION\\n\\n\")\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nFULL BACKWARD PROPAGATION\\n\\n\")\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            # self.cout()\n",
    "            # print(\"\\n\\nADJUST BASED ON GRADIENT DESCENT FOR CURRENT EXAMPLE\\n\\n\")\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "        #     self.cout()\n",
    "\n",
    "        # print(\"\\n\\nADJUST WITH ADJUST VARIABLES\\n\\n\")\n",
    "        self.adjustWithAdjustVariables()\n",
    "        # self.cout()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[self.Layers.size - 1].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def checkRandomExamples(self, data, labels):\n",
    "        numberOfData = len(data)\n",
    "\n",
    "        for num in range (0, 20):\n",
    "            randIdx = random.randint(0, numberOfData)\n",
    "            \n",
    "            self.setStartLayerActivations(data[randIdx])\n",
    "            self.fullForwardPropagation(labels[randIdx])\n",
    "\n",
    "            print(\"Label for the data is: \" + str(labels[randIdx]))\n",
    "            self.coutLastLayer()\n",
    "            print(\"The cost is: \" + str(self.cost(labels[randIdx])))\n",
    "\n",
    "    def coutLastLayer(self):\n",
    "        print(\"The last layer activations are: \")\n",
    "        print(self.Layers[self.Layers.size - 1].activationVector)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on the batch\n",
      "The last layer activations are: \n",
      "[0.09950075 0.11140768 0.09798167 0.10229529 0.10510384 0.08648893\n",
      " 0.10208164 0.10946523 0.09257928 0.10108796]\n",
      "The last layer activations are: \n",
      "[0.0995014  0.11140346 0.09798192 0.10229554 0.10510759 0.08648958\n",
      " 0.10208214 0.109468   0.09257951 0.10108932]\n",
      "The last layer activations are: \n",
      "[0.09950211 0.11139921 0.09798224 0.10229585 0.10511142 0.08649028\n",
      " 0.10208269 0.10947084 0.09257979 0.10109073]\n",
      "The last layer activations are: \n",
      "[0.09950287 0.11139492 0.09798263 0.10229622 0.10511533 0.08649105\n",
      " 0.1020833  0.10947374 0.09258012 0.10109221]\n",
      "The last layer activations are: \n",
      "[0.0995037  0.11139059 0.09798311 0.10229667 0.10511932 0.08649188\n",
      " 0.10208396 0.10947672 0.09258051 0.10109375]\n",
      "The last layer activations are: \n",
      "[0.09950459 0.11138622 0.09798366 0.10229718 0.1051234  0.08649277\n",
      " 0.10208467 0.10947976 0.09258094 0.10109535]\n",
      "The last layer activations are: \n",
      "[0.09950554 0.11138181 0.09798429 0.10229775 0.10512757 0.08649373\n",
      " 0.10208544 0.10948288 0.09258142 0.10109701]\n",
      "The last layer activations are: \n",
      "[0.09950656 0.11137736 0.097985   0.1022984  0.10513182 0.08649475\n",
      " 0.10208626 0.10948606 0.09258195 0.10109875]\n",
      "The last layer activations are: \n",
      "[0.09950763 0.11137287 0.09798579 0.10229911 0.10513617 0.08649584\n",
      " 0.10208714 0.10948932 0.09258253 0.10110054]\n",
      "The last layer activations are: \n",
      "[0.09950877 0.11136833 0.09798665 0.10229989 0.10514061 0.08649699\n",
      " 0.10208807 0.10949265 0.09258317 0.1011024 ]\n",
      "The last layer activations are: \n",
      "[0.09950998 0.11136375 0.0979876  0.10230074 0.10514514 0.08649821\n",
      " 0.10208906 0.10949606 0.09258386 0.10110433]\n",
      "The last layer activations are: \n",
      "[0.09951125 0.11135912 0.09798863 0.10230166 0.10514977 0.0864995\n",
      " 0.10209011 0.10949955 0.09258459 0.10110633]\n",
      "The last layer activations are: \n",
      "[0.09951259 0.11135443 0.09798974 0.10230265 0.10515449 0.08650085\n",
      " 0.10209121 0.10950311 0.09258539 0.1011084 ]\n",
      "The last layer activations are: \n",
      "[0.09951399 0.1113497  0.09799093 0.10230371 0.10515931 0.08650228\n",
      " 0.10209237 0.10950675 0.09258623 0.10111054]\n",
      "The last layer activations are: \n",
      "[0.09951546 0.11134491 0.09799221 0.10230484 0.10516424 0.08650377\n",
      " 0.10209359 0.10951048 0.09258713 0.10111275]\n",
      "The last layer activations are: \n",
      "[0.099517   0.11134007 0.09799357 0.10230605 0.10516927 0.08650533\n",
      " 0.10209488 0.10951428 0.09258808 0.10111503]\n",
      "The last layer activations are: \n",
      "[0.09951861 0.11133517 0.09799501 0.10230732 0.10517441 0.08650697\n",
      " 0.10209622 0.10951817 0.09258909 0.10111738]\n",
      "The last layer activations are: \n",
      "[0.09952028 0.11133022 0.09799655 0.10230868 0.10517965 0.08650867\n",
      " 0.10209762 0.10952215 0.09259015 0.10111981]\n",
      "The last layer activations are: \n",
      "[0.09952203 0.1113252  0.09799816 0.1023101  0.105185   0.08651045\n",
      " 0.10209909 0.10952622 0.09259127 0.10112231]\n",
      "The last layer activations are: \n",
      "[0.09952385 0.11132012 0.09799987 0.10231161 0.10519047 0.0865123\n",
      " 0.10210061 0.10953037 0.09259245 0.1011249 ]\n",
      "The last layer activations are: \n",
      "[0.09952575 0.11131498 0.09800167 0.10231319 0.10519605 0.08651423\n",
      " 0.1021022  0.10953462 0.09259369 0.10112756]\n",
      "The last layer activations are: \n",
      "[0.09952772 0.11130977 0.09800355 0.10231485 0.10520175 0.08651624\n",
      " 0.10210386 0.10953896 0.09259498 0.1011303 ]\n",
      "The last layer activations are: \n",
      "[0.09952976 0.11130449 0.09800553 0.10231658 0.10520757 0.08651832\n",
      " 0.10210558 0.1095434  0.09259633 0.10113312]\n",
      "The last layer activations are: \n",
      "[0.09953188 0.11129914 0.0980076  0.1023184  0.10521351 0.08652048\n",
      " 0.10210737 0.10954793 0.09259774 0.10113602]\n",
      "The last layer activations are: \n",
      "[0.09953408 0.11129372 0.09800977 0.1023203  0.10521958 0.08652272\n",
      " 0.10210923 0.10955256 0.09259922 0.10113901]\n",
      "The last layer activations are: \n",
      "[0.09953636 0.11128822 0.09801203 0.10232228 0.10522578 0.08652504\n",
      " 0.10211116 0.1095573  0.09260075 0.10114208]\n",
      "The last layer activations are: \n",
      "[0.09953872 0.11128264 0.09801439 0.10232435 0.10523211 0.08652745\n",
      " 0.10211315 0.10956214 0.09260235 0.10114524]\n",
      "The last layer activations are: \n",
      "[0.09954116 0.11127699 0.09801684 0.1023265  0.10523857 0.08652994\n",
      " 0.10211522 0.10956709 0.09260402 0.1011485 ]\n",
      "The last layer activations are: \n",
      "[0.09954368 0.11127125 0.0980194  0.10232874 0.10524518 0.08653252\n",
      " 0.10211736 0.10957215 0.09260574 0.10115184]\n",
      "The last layer activations are: \n",
      "[0.09954629 0.11126543 0.09802206 0.10233107 0.10525192 0.08653518\n",
      " 0.10211958 0.10957732 0.09260754 0.10115528]\n",
      "The last layer activations are: \n",
      "[0.09954899 0.11125951 0.09802483 0.10233349 0.10525881 0.08653794\n",
      " 0.10212187 0.10958261 0.0926094  0.10115881]\n",
      "The last layer activations are: \n",
      "[0.09955178 0.11125351 0.0980277  0.102336   0.10526585 0.08654078\n",
      " 0.10212424 0.10958801 0.09261133 0.10116244]\n",
      "The last layer activations are: \n",
      "[0.09955466 0.11124741 0.09803068 0.10233861 0.10527304 0.08654372\n",
      " 0.10212669 0.10959354 0.09261334 0.10116617]\n",
      "The last layer activations are: \n",
      "[0.09955763 0.11124122 0.09803377 0.10234131 0.10528039 0.08654675\n",
      " 0.10212922 0.10959919 0.09261541 0.10117   ]\n",
      "The last layer activations are: \n",
      "[0.09956069 0.11123492 0.09803697 0.10234411 0.1052879  0.08654989\n",
      " 0.10213183 0.10960496 0.09261755 0.10117393]\n",
      "The last layer activations are: \n",
      "[0.09956386 0.11122852 0.09804028 0.102347   0.10529558 0.08655312\n",
      " 0.10213453 0.10961087 0.09261978 0.10117797]\n",
      "The last layer activations are: \n",
      "[0.09956712 0.11122202 0.09804372 0.10235    0.10530342 0.08655645\n",
      " 0.10213731 0.10961691 0.09262207 0.10118213]\n",
      "The last layer activations are: \n",
      "[0.09957048 0.1112154  0.09804727 0.10235311 0.10531144 0.08655988\n",
      " 0.10214018 0.10962309 0.09262445 0.10118639]\n",
      "The last layer activations are: \n",
      "[0.09957395 0.11120867 0.09805095 0.10235632 0.10531964 0.08656343\n",
      " 0.10214315 0.10962941 0.0926269  0.10119077]\n",
      "The last layer activations are: \n",
      "[0.09957752 0.11120182 0.09805475 0.10235964 0.10532802 0.08656708\n",
      " 0.1021462  0.10963588 0.09262944 0.10119526]\n",
      "The last layer activations are: \n",
      "[0.0995812  0.11119485 0.09805868 0.10236307 0.10533659 0.08657084\n",
      " 0.10214935 0.10964249 0.09263205 0.10119988]\n",
      "The last layer activations are: \n",
      "[0.099585   0.11118776 0.09806274 0.10236661 0.10534536 0.08657472\n",
      " 0.10215259 0.10964926 0.09263476 0.10120462]\n",
      "The last layer activations are: \n",
      "[0.0995889  0.11118053 0.09806693 0.10237027 0.10535432 0.08657872\n",
      " 0.10215594 0.10965619 0.09263755 0.10120949]\n",
      "The last layer activations are: \n",
      "[0.09959293 0.11117317 0.09807126 0.10237405 0.1053635  0.08658283\n",
      " 0.10215939 0.10966328 0.09264043 0.10121449]\n",
      "The last layer activations are: \n",
      "[0.09959708 0.11116567 0.09807573 0.10237796 0.10537288 0.08658707\n",
      " 0.10216294 0.10967054 0.0926434  0.10121963]\n",
      "The last layer activations are: \n",
      "[0.09960135 0.11115802 0.09808035 0.10238199 0.10538248 0.08659144\n",
      " 0.1021666  0.10967797 0.09264646 0.1012249 ]\n",
      "The last layer activations are: \n",
      "[0.09960575 0.11115023 0.09808511 0.10238614 0.10539231 0.08659593\n",
      " 0.10217037 0.10968558 0.09264962 0.10123031]\n",
      "The last layer activations are: \n",
      "[0.09961027 0.11114228 0.09809003 0.10239044 0.10540237 0.08660057\n",
      " 0.10217425 0.10969337 0.09265288 0.10123587]\n",
      "The last layer activations are: \n",
      "[0.09961494 0.11113417 0.0980951  0.10239486 0.10541267 0.08660533\n",
      " 0.10217825 0.10970136 0.09265624 0.10124158]\n",
      "The last layer activations are: \n",
      "[0.09961974 0.11112589 0.09810033 0.10239943 0.10542322 0.08661025\n",
      " 0.10218237 0.10970953 0.09265971 0.10124745]\n",
      "The last layer activations are: \n",
      "[0.09962469 0.11111744 0.09810573 0.10240414 0.10543402 0.0866153\n",
      " 0.10218662 0.10971791 0.09266328 0.10125348]\n",
      "The last layer activations are: \n",
      "[0.09962978 0.11110882 0.09811129 0.10240901 0.10544508 0.08662051\n",
      " 0.10219099 0.1097265  0.09266697 0.10125967]\n",
      "The last layer activations are: \n",
      "[0.09963502 0.11110001 0.09811703 0.10241402 0.10545642 0.08662587\n",
      " 0.1021955  0.1097353  0.09267076 0.10126603]\n",
      "The last layer activations are: \n",
      "[0.09964042 0.11109101 0.09812295 0.10241919 0.10546804 0.0866314\n",
      " 0.10220014 0.10974432 0.09267467 0.10127257]\n",
      "The last layer activations are: \n",
      "[0.09964598 0.11108181 0.09812906 0.10242452 0.10547994 0.08663709\n",
      " 0.10220491 0.10975357 0.09267871 0.10127929]\n",
      "The last layer activations are: \n",
      "[0.09965171 0.1110724  0.09813535 0.10243002 0.10549215 0.08664294\n",
      " 0.10220984 0.10976306 0.09268287 0.10128619]\n",
      "The last layer activations are: \n",
      "[0.09965761 0.11106278 0.09814185 0.1024357  0.10550467 0.08664898\n",
      " 0.10221491 0.10977279 0.09268715 0.10129329]\n",
      "The last layer activations are: \n",
      "[0.09966368 0.11105294 0.09814854 0.10244155 0.10551751 0.08665519\n",
      " 0.10222014 0.10978278 0.09269157 0.10130059]\n",
      "The last layer activations are: \n",
      "[0.09966994 0.11104286 0.09815544 0.10244758 0.10553068 0.0866616\n",
      " 0.10222552 0.10979304 0.09269612 0.10130809]\n",
      "The last layer activations are: \n",
      "[0.09967639 0.11103255 0.09816256 0.10245381 0.1055442  0.0866682\n",
      " 0.10223107 0.10980356 0.09270082 0.10131581]\n",
      "The last layer activations are: \n",
      "[0.09968304 0.11102199 0.09816991 0.10246023 0.10555808 0.086675\n",
      " 0.1022368  0.10981437 0.09270566 0.10132375]\n",
      "The last layer activations are: \n",
      "[0.09968989 0.11101116 0.09817748 0.10246685 0.10557232 0.086682\n",
      " 0.10224269 0.10982547 0.09271065 0.10133192]\n",
      "The last layer activations are: \n",
      "[0.09969695 0.11100007 0.0981853  0.10247369 0.10558695 0.08668923\n",
      " 0.10224877 0.10983687 0.0927158  0.10134033]\n",
      "The last layer activations are: \n",
      "[0.09970423 0.1109887  0.09819336 0.10248074 0.10560198 0.08669667\n",
      " 0.10225504 0.10984859 0.0927211  0.10134898]\n",
      "The last layer activations are: \n",
      "[0.09971174 0.11097703 0.09820167 0.10248803 0.10561742 0.08670435\n",
      " 0.10226151 0.10986064 0.09272658 0.1013579 ]\n",
      "The last layer activations are: \n",
      "[0.09971948 0.11096506 0.09821026 0.10249554 0.1056333  0.08671227\n",
      " 0.10226818 0.10987303 0.09273223 0.10136707]\n",
      "The last layer activations are: \n",
      "[0.09972746 0.11095277 0.09821912 0.1025033  0.10564962 0.08672044\n",
      " 0.10227506 0.10988577 0.09273806 0.10137652]\n",
      "The last layer activations are: \n",
      "[0.0997357  0.11094014 0.09822826 0.10251131 0.1056664  0.08672887\n",
      " 0.10228216 0.10989888 0.09274407 0.10138626]\n",
      "The last layer activations are: \n",
      "[0.09974421 0.11092717 0.0982377  0.10251958 0.10568368 0.08673757\n",
      " 0.1022895  0.10991237 0.09275028 0.1013963 ]\n",
      "The last layer activations are: \n",
      "[0.09975298 0.11091384 0.09824745 0.10252813 0.10570145 0.08674655\n",
      " 0.10229706 0.10992627 0.09275669 0.10140665]\n",
      "The last layer activations are: \n",
      "[0.09976205 0.11090013 0.09825752 0.10253696 0.10571975 0.08675582\n",
      " 0.10230488 0.10994058 0.0927633  0.10141732]\n",
      "The last layer activations are: \n",
      "[0.09977141 0.11088603 0.09826792 0.10254609 0.1057386  0.08676539\n",
      " 0.10231296 0.10995532 0.09277014 0.10142833]\n",
      "The last layer activations are: \n",
      "[0.09978108 0.11087151 0.09827867 0.10255552 0.10575802 0.08677529\n",
      " 0.1023213  0.10997052 0.0927772  0.10143969]\n",
      "The last layer activations are: \n",
      "[0.09979108 0.11085656 0.09828978 0.10256527 0.10577804 0.08678551\n",
      " 0.10232993 0.10998619 0.0927845  0.10145142]\n",
      "The last layer activations are: \n",
      "[0.09980141 0.11084116 0.09830127 0.10257536 0.10579868 0.08679608\n",
      " 0.10233885 0.11000236 0.09279204 0.10146353]\n",
      "The last layer activations are: \n",
      "[0.0998121  0.11082528 0.09831315 0.1025858  0.10581998 0.08680701\n",
      " 0.10234807 0.11001904 0.09279984 0.10147604]\n",
      "The last layer activations are: \n",
      "[0.09982316 0.1108089  0.09832544 0.10259661 0.10584195 0.08681831\n",
      " 0.10235762 0.11003627 0.09280791 0.10148897]\n",
      "The last layer activations are: \n",
      "[0.0998346  0.110792   0.09833816 0.10260779 0.10586464 0.08683001\n",
      " 0.1023675  0.11005406 0.09281626 0.10150234]\n",
      "The last layer activations are: \n",
      "[0.09984645 0.11077456 0.09835133 0.10261938 0.10588807 0.08684213\n",
      " 0.10237773 0.11007244 0.0928249  0.10151616]\n",
      "The last layer activations are: \n",
      "[0.09985873 0.11075653 0.09836496 0.10263138 0.10591229 0.08685467\n",
      " 0.10238832 0.11009144 0.09283384 0.10153047]\n",
      "The last layer activations are: \n",
      "[0.09987144 0.1107379  0.09837909 0.10264383 0.10593732 0.08686767\n",
      " 0.10239931 0.1101111  0.09284311 0.10154527]\n",
      "The last layer activations are: \n",
      "[0.09988463 0.11071863 0.09839373 0.10265673 0.10596322 0.08688114\n",
      " 0.1024107  0.11013144 0.09285271 0.10156061]\n",
      "The last layer activations are: \n",
      "[0.0998983  0.11069869 0.09840891 0.10267011 0.10599002 0.08689512\n",
      " 0.10242251 0.11015251 0.09286267 0.1015765 ]\n",
      "The last layer activations are: \n",
      "[0.09991249 0.11067805 0.09842465 0.102684   0.10601777 0.08690961\n",
      " 0.10243477 0.11017432 0.09287299 0.10159298]\n",
      "The last layer activations are: \n",
      "[0.09992722 0.11065665 0.09844099 0.10269842 0.10604653 0.08692466\n",
      " 0.1024475  0.11019694 0.09288371 0.10161006]\n",
      "The last layer activations are: \n",
      "[0.09994252 0.11063447 0.09845795 0.10271341 0.10607633 0.08694029\n",
      " 0.10246072 0.11022039 0.09289483 0.1016278 ]\n",
      "The last layer activations are: \n",
      "[0.09995842 0.11061145 0.09847557 0.10272898 0.10610725 0.08695652\n",
      " 0.10247446 0.11024473 0.09290638 0.10164621]\n",
      "The last layer activations are: \n",
      "[0.09997496 0.11058756 0.09849388 0.10274517 0.10613934 0.08697341\n",
      " 0.10248876 0.11027001 0.09291839 0.10166535]\n",
      "The last layer activations are: \n",
      "[0.09999217 0.11056273 0.09851292 0.10276203 0.10617266 0.08699097\n",
      " 0.10250363 0.11029627 0.09293087 0.10168524]\n",
      "The last layer activations are: \n",
      "[0.10001008 0.11053691 0.09853273 0.10277957 0.10620729 0.08700925\n",
      " 0.10251912 0.11032357 0.09294386 0.10170593]\n",
      "The last layer activations are: \n",
      "[0.10002874 0.11051005 0.09855335 0.10279785 0.1062433  0.08702829\n",
      " 0.10253526 0.11035198 0.09295737 0.10172748]\n",
      "The last layer activations are: \n",
      "[0.1000482  0.11048207 0.09857483 0.1028169  0.10628077 0.08704813\n",
      " 0.10255208 0.11038155 0.09297146 0.10174992]\n",
      "The last layer activations are: \n",
      "[0.1000685  0.11045292 0.09859722 0.10283678 0.1063198  0.08706883\n",
      " 0.10256964 0.11041237 0.09298614 0.10177331]\n",
      "The last layer activations are: \n",
      "[0.10008969 0.1104225  0.09862057 0.10285753 0.10636046 0.08709043\n",
      " 0.10258797 0.1104445  0.09300145 0.10179771]\n",
      "The last layer activations are: \n",
      "[0.10011184 0.11039075 0.09864495 0.1028792  0.10640288 0.08711299\n",
      " 0.10260712 0.11047803 0.09301743 0.10182319]\n",
      "The last layer activations are: \n",
      "[0.10013499 0.11035757 0.0986704  0.10290186 0.10644715 0.08713657\n",
      " 0.10262715 0.11051304 0.09303412 0.1018498 ]\n",
      "The last layer activations are: \n",
      "[0.10015922 0.11032287 0.09869701 0.10292556 0.10649339 0.08716123\n",
      " 0.10264811 0.11054965 0.09305157 0.10187764]\n",
      "The last layer activations are: \n",
      "[0.10018459 0.11028654 0.09872484 0.10295038 0.10654175 0.08718705\n",
      " 0.10267006 0.11058794 0.09306983 0.10190677]\n",
      "The last layer activations are: \n",
      "[0.10021119 0.11024847 0.09875398 0.1029764  0.10659234 0.0872141\n",
      " 0.10269308 0.11062803 0.09308894 0.10193728]\n",
      "The last layer activations are: \n",
      "[0.1002391  0.11020853 0.0987845  0.10300368 0.10664534 0.08724247\n",
      " 0.10271723 0.11067006 0.09310898 0.10196927]\n",
      "The last layer activations are: \n",
      "[0.10026841 0.11016659 0.09881652 0.10303232 0.1067009  0.08727225\n",
      " 0.1027426  0.11071414 0.09312999 0.10200285]\n",
      "The last layer activations are: \n",
      "[0.10029922 0.11012251 0.09885012 0.10306242 0.10675921 0.08730353\n",
      " 0.10276927 0.11076044 0.09315205 0.10203812]\n",
      "The last layer activations are: \n",
      "[0.10033165 0.11007612 0.09888542 0.10309408 0.10682046 0.08733643\n",
      " 0.10279734 0.11080911 0.09317523 0.10207521]\n",
      "The last layer activations are: \n",
      "[0.10036581 0.11002724 0.09892254 0.10312742 0.10688487 0.08737106\n",
      " 0.10282691 0.11086033 0.09319962 0.10211426]\n",
      "The last layer activations are: \n",
      "[0.10040184 0.10997568 0.09896162 0.10316256 0.10695268 0.08740756\n",
      " 0.1028581  0.1109143  0.09322531 0.10215541]\n",
      "The last layer activations are: \n",
      "[0.10043988 0.10992123 0.0990028  0.10319964 0.10702414 0.08744606\n",
      " 0.10289103 0.11097121 0.09325239 0.10219883]\n",
      "The last layer activations are: \n",
      "[0.1004801  0.10986364 0.09904623 0.10323881 0.10709955 0.08748673\n",
      " 0.10292585 0.11103132 0.09328096 0.1022447 ]\n",
      "The last layer activations are: \n",
      "[0.10052266 0.10980268 0.09909211 0.10328024 0.1071792  0.08752972\n",
      " 0.1029627  0.11109486 0.09331116 0.10229321]\n",
      "The last layer activations are: \n",
      "[0.10056777 0.10973803 0.09914061 0.10332412 0.10726345 0.08757524\n",
      " 0.10300176 0.11116214 0.09334311 0.10234458]\n",
      "The last layer activations are: \n",
      "[0.10061564 0.10966941 0.09919195 0.10337064 0.10735268 0.08762348\n",
      " 0.10304321 0.11123345 0.09337695 0.10239906]\n",
      "The last layer activations are: \n",
      "[0.10066651 0.10959644 0.09924635 0.10342004 0.10744729 0.08767468\n",
      " 0.10308725 0.11130914 0.09341284 0.1024569 ]\n",
      "The last layer activations are: \n",
      "[0.10072065 0.10951876 0.09930409 0.10347255 0.10754775 0.08772909\n",
      " 0.10313412 0.11138959 0.09345096 0.1025184 ]\n",
      "The last layer activations are: \n",
      "[0.10077833 0.10943594 0.09936542 0.10352845 0.10765456 0.08778697\n",
      " 0.10318407 0.11147521 0.0934915  0.10258389]\n",
      "The last layer activations are: \n",
      "[0.1008399  0.1093475  0.09943067 0.10358805 0.10776828 0.08784865\n",
      " 0.10323737 0.11156648 0.09353468 0.10265373]\n",
      "The last layer activations are: \n",
      "[0.10090571 0.10925293 0.09950018 0.10365167 0.10788952 0.08791446\n",
      " 0.10329434 0.11166389 0.09358072 0.10272832]\n",
      "The last layer activations are: \n",
      "[0.10097617 0.10915164 0.09957431 0.10371969 0.10801896 0.08798477\n",
      " 0.10335532 0.11176802 0.0936299  0.10280809]\n",
      "The last layer activations are: \n",
      "[0.1010517  0.10904301 0.09965348 0.10379251 0.10815736 0.08805999\n",
      " 0.1034207  0.11187949 0.0936825  0.10289354]\n",
      "The last layer activations are: \n",
      "[0.10113282 0.10892633 0.09973813 0.10387059 0.10830553 0.08814057\n",
      " 0.10349089 0.11199899 0.09373885 0.10298521]\n",
      "The last layer activations are: \n",
      "[0.10122007 0.10880083 0.09982877 0.10395442 0.10846437 0.08822701\n",
      " 0.10356636 0.11212728 0.09379929 0.10308371]\n",
      "The last layer activations are: \n",
      "[0.10131405 0.10866566 0.09992592 0.10404454 0.10863488 0.08831986\n",
      " 0.10364763 0.11226519 0.09386422 0.1031897 ]\n",
      "The last layer activations are: \n",
      "[0.10141544 0.10851989 0.10003017 0.10414157 0.10881813 0.08841972\n",
      " 0.10373528 0.11241363 0.09393406 0.1033039 ]\n",
      "The last layer activations are: \n",
      "[0.10152496 0.10836252 0.10014216 0.10424614 0.10901531 0.08852722\n",
      " 0.10382992 0.11257361 0.09400929 0.10342712]\n",
      "The last layer activations are: \n",
      "[0.10164344 0.10819246 0.10026255 0.10435898 0.10922765 0.08864308\n",
      " 0.10393223 0.11274619 0.0940904  0.10356023]\n",
      "The last layer activations are: \n",
      "[0.10177175 0.10800855 0.10039205 0.10448083 0.10945653 0.08876803\n",
      " 0.10404296 0.11293254 0.09417795 0.10370418]\n",
      "The last layer activations are: \n",
      "[0.10191084 0.1078096  0.10053141 0.1046125  0.10970334 0.08890287\n",
      " 0.1041629  0.11313386 0.09427252 0.10385998]\n",
      "The last layer activations are: \n",
      "[0.10206171 0.10759436 0.10068139 0.10475483 0.10996956 0.08904841\n",
      " 0.10429287 0.11335143 0.09437473 0.1040287 ]\n",
      "The last layer activations are: \n",
      "[0.10222542 0.10736159 0.10084274 0.10490869 0.11025667 0.0892055\n",
      " 0.10443374 0.11358655 0.09448521 0.10421145]\n",
      "The last layer activations are: \n",
      "[0.10240307 0.10711011 0.10101617 0.10507492 0.11056612 0.08937495\n",
      " 0.10458639 0.1138405  0.0946046  0.10440936]\n",
      "The last layer activations are: \n",
      "[0.10259574 0.10683889 0.10120234 0.10525434 0.11089927 0.08955754\n",
      " 0.10475167 0.1141145  0.09473351 0.10462353]\n",
      "The last layer activations are: \n",
      "[0.10280445 0.10654709 0.10140177 0.10544765 0.11125729 0.08975396\n",
      " 0.10493037 0.11440961 0.09487251 0.10485499]\n",
      "The last layer activations are: \n",
      "[0.10303014 0.10623426 0.10161478 0.10565542 0.11164106 0.08996473\n",
      " 0.10512316 0.11472669 0.09502206 0.10510463]\n",
      "The last layer activations are: \n",
      "[0.10327356 0.1059004  0.10184147 0.10587801 0.11205109 0.09019017\n",
      " 0.10533054 0.11506626 0.09518249 0.10537311]\n",
      "The last layer activations are: \n",
      "[0.1035352  0.10554621 0.10208159 0.10611544 0.11248733 0.09043031\n",
      " 0.10555274 0.1154284  0.09535393 0.10566079]\n",
      "The last layer activations are: \n",
      "[0.10381521 0.10517317 0.1023345  0.1063674  0.11294912 0.09068482\n",
      " 0.10578966 0.11581266 0.09553623 0.10596762]\n",
      "The last layer activations are: \n",
      "[0.10411329 0.10478375 0.10259914 0.10663312 0.11343503 0.09095294\n",
      " 0.10604082 0.11621795 0.09572894 0.10629308]\n",
      "The last layer activations are: \n",
      "[0.10442868 0.10438147 0.10287396 0.10691134 0.11394291 0.09123345\n",
      " 0.10630526 0.11664252 0.09593127 0.1066361 ]\n",
      "The last layer activations are: \n",
      "[0.10476009 0.10397096 0.103157   0.10720033 0.11446983 0.09152469\n",
      " 0.1065816  0.11708398 0.09614205 0.10699504]\n",
      "The last layer activations are: \n",
      "[0.10510576 0.10355789 0.10344594 0.10749795 0.11501227 0.09182458\n",
      " 0.10686802 0.11753938 0.09635978 0.10736779]\n",
      "The last layer activations are: \n",
      "[0.10546352 0.1031488  0.10373822 0.10780173 0.11556629 0.09213073\n",
      " 0.10716237 0.11800542 0.09658268 0.10775183]\n",
      "The last layer activations are: \n",
      "[0.10583095 0.10275083 0.10403121 0.10810904 0.11612783 0.09244058\n",
      " 0.1074623  0.11847863 0.09680879 0.10814439]\n",
      "The last layer activations are: \n",
      "[0.10620556 0.10237141 0.10432239 0.10841727 0.11669301 0.09275157\n",
      " 0.10776544 0.11895569 0.09703608 0.10854272]\n",
      "The last layer activations are: \n",
      "[0.10658497 0.10201786 0.10460954 0.10872403 0.11725841 0.09306128\n",
      " 0.10806956 0.11943364 0.09726263 0.10894422]\n",
      "The last layer activations are: \n",
      "[0.10696712 0.10169714 0.10489082 0.10902725 0.11782134 0.09336764\n",
      " 0.10837272 0.11991014 0.09748668 0.10934671]\n",
      "The last layer activations are: \n",
      "[0.10735038 0.10141555 0.10516493 0.10932537 0.11837996 0.09366899\n",
      " 0.10867342 0.12038356 0.09770678 0.10974852]\n",
      "The last layer activations are: \n",
      "[0.10773371 0.10117862 0.10543106 0.10961732 0.11893338 0.09396415\n",
      " 0.10897062 0.12085306 0.09792181 0.1101486 ]\n",
      "The last layer activations are: \n",
      "[0.1081166  0.10099107 0.1056889  0.10990255 0.1194816  0.09425246\n",
      " 0.10926381 0.12131858 0.09813102 0.11054655]\n",
      "The last layer activations are: \n",
      "[0.10849915 0.10085688 0.10593859 0.11018102 0.12002547 0.09453371\n",
      " 0.10955293 0.12178078 0.098334   0.11094257]\n",
      "The last layer activations are: \n",
      "[0.10888201 0.10077942 0.10618062 0.11045306 0.12056655 0.09480813\n",
      " 0.10983838 0.12224091 0.09853065 0.11133742]\n",
      "The last layer activations are: \n",
      "[0.10926629 0.10076167 0.10641575 0.11071937 0.12110698 0.09507628\n",
      " 0.11012091 0.12270075 0.09872112 0.11173235]\n",
      "The last layer activations are: \n",
      "[0.10965352 0.10080646 0.10664492 0.11098089 0.12164944 0.095339\n",
      " 0.1104016  0.12316249 0.09890574 0.11212905]\n",
      "The last layer activations are: \n",
      "[0.11004564 0.10091668 0.10686919 0.11123874 0.12219697 0.09559736\n",
      " 0.11068177 0.12362867 0.09908501 0.11252952]\n",
      "The last layer activations are: \n",
      "[0.11044492 0.10109563 0.10708968 0.11149418 0.12275301 0.09585255\n",
      " 0.11096293 0.12410215 0.0992595  0.11293611]\n",
      "The last layer activations are: \n",
      "[0.11085394 0.10134722 0.10730751 0.11174856 0.12332132 0.09610587\n",
      " 0.11124682 0.12458605 0.0994298  0.11335148]\n",
      "The last layer activations are: \n",
      "[0.11127566 0.10167639 0.10752378 0.11200331 0.12390606 0.09635872\n",
      " 0.11153531 0.12508382 0.09959657 0.11377858]\n",
      "The last layer activations are: \n",
      "[0.11171345 0.10208941 0.10773955 0.11225988 0.1245118  0.09661253\n",
      " 0.1118305  0.12559925 0.09976039 0.11422071]\n",
      "The last layer activations are: \n",
      "[0.11217113 0.10259436 0.1079558  0.11251982 0.12514365 0.09686879\n",
      " 0.11213469 0.12613654 0.09992186 0.11468161]\n",
      "The last layer activations are: \n",
      "[0.11265317 0.10320176 0.10817342 0.11278471 0.12580739 0.09712903\n",
      " 0.11245049 0.12670049 0.10008149 0.11516553]\n",
      "The last layer activations are: \n",
      "[0.11316484 0.10392535 0.10839319 0.11305624 0.12650976 0.09739486\n",
      " 0.11278084 0.12729656 0.10023973 0.11567745]\n",
      "The last layer activations are: \n",
      "[0.11371251 0.10478327 0.10861568 0.1133362  0.1272587  0.09766793\n",
      " 0.11312923 0.12793121 0.1003969  0.11622326]\n",
      "The last layer activations are: \n",
      "[0.11430407 0.10579975 0.1088412  0.11362652 0.12806383 0.09795003\n",
      " 0.11349976 0.1286122  0.10055316 0.11681013]\n",
      "The last layer activations are: \n",
      "[0.11494952 0.10700772 0.1090696  0.11392928 0.12893715 0.09824302\n",
      " 0.11389749 0.12934906 0.10070839 0.11744697]\n",
      "The last layer activations are: \n",
      "[0.11566196 0.10845291 0.10929998 0.11424671 0.12989402 0.09854891\n",
      " 0.11432872 0.13015384 0.10086201 0.11814519]\n",
      "The last layer activations are: \n",
      "[0.11645907 0.11020049 0.10953011 0.11458107 0.13095479 0.09886976\n",
      " 0.11480153 0.1310422  0.10101254 0.11891981]\n",
      "The last layer activations are: \n",
      "[0.11736561 0.11234611 0.10975534 0.11493433 0.13214742 0.0992076\n",
      " 0.11532648 0.13203518 0.10115677 0.11979126]\n",
      "The last layer activations are: \n",
      "[0.1184178  0.11503448 0.1099664  0.11530726 0.13351223 0.09956409\n",
      " 0.11591765 0.13316218 0.10128792 0.12078843]\n",
      "The last layer activations are: \n",
      "[0.11967142 0.11848823 0.11014507 0.11569708 0.1351107  0.09993983\n",
      " 0.11659412 0.13446636 0.10139152 0.12195409]\n",
      "The last layer activations are: \n",
      "[0.1212183  0.12303368 0.11025585 0.11609192 0.13704431 0.10033364\n",
      " 0.11738219 0.13601592 0.1014358  0.12335568]\n",
      "The last layer activations are: \n",
      "[0.12322616 0.12896377 0.11023648 0.11646165 0.13950444 0.10074914\n",
      " 0.11832277 0.13793538 0.10135225 0.12511282]\n",
      "The last layer activations are: \n",
      "[0.12607056 0.13499428 0.11005717 0.11678935 0.1429726  0.10128425\n",
      " 0.11953099 0.14054613 0.1010302  0.12750846]\n",
      "The last layer activations are: \n",
      "[0.13079216 0.12975741 0.11031723 0.11749625 0.14907455 0.10275962\n",
      " 0.12158481 0.14500614 0.10059925 0.13146846]\n",
      "The last layer activations are: \n",
      "[0.13658664 0.102259   0.11149126 0.11891801 0.15636245 0.10565476\n",
      " 0.12437307 0.15011747 0.10041509 0.1362161 ]\n",
      "The last layer activations are: \n",
      "[0.13478616 0.09837661 0.1096308  0.11813467 0.15023777 0.10487719\n",
      " 0.12339942 0.1452369  0.09941433 0.13372537]\n",
      "The last layer activations are: \n",
      "[0.13960487 0.06194715 0.11170433 0.12026959 0.15685231 0.1083665\n",
      " 0.12617647 0.14977658 0.09988721 0.13763622]\n",
      "The last layer activations are: \n",
      "[0.12364326 0.10286893 0.1046686  0.11432406 0.12977154 0.09947418\n",
      " 0.1179927  0.12991868 0.09777174 0.12275383]\n",
      "The last layer activations are: \n",
      "[0.14677489 0.02521124 0.11392433 0.12074755 0.16323162 0.11160077\n",
      " 0.12945991 0.15433244 0.09951734 0.14177358]\n",
      "The last layer activations are: \n",
      "[0.11403017 0.03326851 0.10788459 0.11283468 0.11779208 0.10109227\n",
      " 0.11465754 0.12212808 0.0982353  0.11559117]\n",
      "The last layer activations are: \n",
      "[0.11075738 0.06323784 0.10548027 0.11121016 0.11503117 0.09675819\n",
      " 0.11216307 0.11942004 0.09846315 0.1120926 ]\n",
      "The last layer activations are: \n",
      "[0.11675105 0.05667134 0.10591514 0.1146664  0.12279239 0.09861391\n",
      " 0.11561532 0.12484519 0.09975925 0.11705964]\n",
      "The last layer activations are: \n",
      "[0.11901129 0.05493029 0.10593182 0.11606238 0.12560869 0.09919304\n",
      " 0.11691361 0.12648867 0.10051825 0.11884045]\n",
      "The last layer activations are: \n",
      "[0.12054994 0.05055452 0.10612667 0.11698927 0.12741892 0.09979607\n",
      " 0.11780863 0.12757275 0.10096352 0.12005742]\n",
      "The last layer activations are: \n",
      "[0.120651   0.04865898 0.10618486 0.11718628 0.12738406 0.09985244\n",
      " 0.11795758 0.12747648 0.1012323  0.12011513]\n",
      "The last layer activations are: \n",
      "[0.12090439 0.04586666 0.10634931 0.11740402 0.12759392 0.10000819\n",
      " 0.11816661 0.12761904 0.10139109 0.12030498]\n",
      "The last layer activations are: \n",
      "[0.12069214 0.04415811 0.10645633 0.11738313 0.12724309 0.09994825\n",
      " 0.11812948 0.12740199 0.1014955  0.12012117]\n",
      "The last layer activations are: \n",
      "[0.12064367 0.04224414 0.10660007 0.11741926 0.12712296 0.09994709\n",
      " 0.11816354 0.12735904 0.10155947 0.1200699 ]\n",
      "The last layer activations are: \n",
      "[0.12045465 0.0407883  0.10671539 0.11738047 0.12683808 0.09987224\n",
      " 0.11811867 0.12720651 0.10160496 0.11990627]\n",
      "The last layer activations are: \n",
      "[0.12034676 0.03935782 0.10683567 0.11737146 0.12666102 0.09982412\n",
      " 0.11810758 0.1271306  0.10163432 0.11980762]\n",
      "The last layer activations are: \n",
      "[0.12020628 0.03813921 0.10693991 0.11734204 0.12644689 0.09975693\n",
      " 0.11807416 0.12702826 0.10165611 0.11968315]\n",
      "The last layer activations are: \n",
      "[0.12010125 0.03699927 0.10703902 0.11732471 0.1262781  0.09970217\n",
      " 0.11805354 0.1269564  0.10167091 0.11958695]\n",
      "The last layer activations are: \n",
      "[0.11999583 0.03597729 0.1071278  0.11730364 0.12610986 0.09964627\n",
      " 0.11802825 0.12688322 0.101682   0.11949055]\n",
      "The last layer activations are: \n",
      "[0.11990746 0.03503206 0.10720987 0.11728743 0.12596305 0.09959755\n",
      " 0.11800768 0.12682327 0.10168985 0.11940771]\n",
      "The last layer activations are: \n",
      "[0.11982578 0.03416547 0.10728469 0.11727164 0.12582496 0.09955228\n",
      " 0.11798704 0.12676761 0.10169577 0.11933023]\n",
      "The last layer activations are: \n",
      "[0.11975402 0.03336228 0.10735376 0.11725806 0.12569944 0.09951212\n",
      " 0.11796837 0.12671899 0.10170022 0.11926064]\n",
      "The last layer activations are: \n",
      "[0.11968896 0.03261751 0.10741757 0.11724543 0.12558266 0.09947589\n",
      " 0.11795037 0.12667477 0.10170381 0.11919638]\n",
      "The last layer activations are: \n",
      "[0.11963053 0.0319235  0.10747697 0.11723388 0.12547455 0.09944361\n",
      " 0.1179333  0.126635   0.10170692 0.11913741]\n",
      "The last layer activations are: \n",
      "[0.11957739 0.03127525 0.10753253 0.11722295 0.12537361 0.09941474\n",
      " 0.11791671 0.12659867 0.1017099  0.11908268]\n",
      "The last layer activations are: \n",
      "[0.11952896 0.03066785 0.10758483 0.11721247 0.12527918 0.099389\n",
      " 0.11790052 0.12656541 0.10171304 0.11903172]\n",
      "The last layer activations are: \n",
      "[0.11948453 0.03009734 0.10763431 0.1172022  0.12519045 0.09936605\n",
      " 0.1178845  0.12653471 0.10171657 0.11898398]\n",
      "The last layer activations are: \n",
      "[0.11944361 0.02956016 0.10768137 0.11719198 0.12510686 0.09934561\n",
      " 0.11786856 0.12650626 0.10172065 0.11893906]\n",
      "The last layer activations are: \n",
      "[0.11940574 0.02905328 0.10772633 0.11718167 0.12502793 0.09932745\n",
      " 0.11785258 0.12647975 0.10172544 0.11889663]\n",
      "The last layer activations are: \n",
      "[0.11937063 0.02857401 0.10776947 0.11717118 0.12495326 0.09931138\n",
      " 0.11783652 0.126455   0.10173101 0.11885642]\n",
      "The last layer activations are: \n",
      "[0.119338   0.02811996 0.10781101 0.11716045 0.12488256 0.09929722\n",
      " 0.11782033 0.12643185 0.10173741 0.11881824]\n",
      "The last layer activations are: \n",
      "[0.11930768 0.02768904 0.10785113 0.11714943 0.12481559 0.09928484\n",
      " 0.11780401 0.12641019 0.10174466 0.11878194]\n",
      "The last layer activations are: \n",
      "[0.11927952 0.02727936 0.10788998 0.11713812 0.12475214 0.09927414\n",
      " 0.11778757 0.12638996 0.10175275 0.11874741]\n",
      "The last layer activations are: \n",
      "[0.11925343 0.02688923 0.10792771 0.11712652 0.12469208 0.09926502\n",
      " 0.11777104 0.12637111 0.10176163 0.11871458]\n",
      "The last layer activations are: \n",
      "[0.11922933 0.02651713 0.1079644  0.11711464 0.12463527 0.0992574\n",
      " 0.11775446 0.12635362 0.10177126 0.11868339]\n",
      "The last layer activations are: \n",
      "[0.11920716 0.02616168 0.10800016 0.1171025  0.1245816  0.0992512\n",
      " 0.11773787 0.1263375  0.10178156 0.11865379]\n",
      "The last layer activations are: \n",
      "[0.11918687 0.02582163 0.10803507 0.11709013 0.124531   0.09924638\n",
      " 0.11772133 0.12632275 0.10179246 0.11862576]\n",
      "The last layer activations are: \n",
      "[0.11916843 0.02549585 0.10806919 0.11707756 0.12448338 0.09924287\n",
      " 0.11770488 0.12630937 0.10180386 0.11859927]\n",
      "The last layer activations are: \n",
      "[0.11915178 0.02518329 0.1081026  0.11706481 0.12443867 0.09924062\n",
      " 0.11768857 0.12629739 0.10181567 0.11857429]\n",
      "The last layer activations are: \n",
      "[0.1191369  0.02488302 0.10813534 0.1170519  0.12439679 0.09923957\n",
      " 0.11767244 0.12628681 0.10182782 0.11855081]\n",
      "The last layer activations are: \n",
      "[0.11912372 0.02459417 0.10816747 0.11703886 0.12435768 0.09923966\n",
      " 0.11765654 0.12627765 0.10184021 0.1185288 ]\n",
      "The last layer activations are: \n",
      "[0.11911222 0.02431595 0.10819904 0.1170257  0.12432128 0.09924084\n",
      " 0.1176409  0.12626992 0.10185275 0.11850824]\n",
      "The last layer activations are: \n",
      "[0.11910232 0.02404762 0.10823009 0.11701244 0.12428752 0.09924305\n",
      " 0.11762555 0.12626362 0.10186536 0.11848909]\n",
      "The last layer activations are: \n",
      "[0.11909399 0.02378854 0.10826066 0.11699906 0.12425634 0.09924623\n",
      " 0.11761052 0.12625875 0.10187797 0.11847133]\n",
      "The last layer activations are: \n",
      "[0.11908715 0.02353809 0.10829077 0.11698559 0.12422767 0.09925032\n",
      " 0.11759582 0.12625532 0.1018905  0.11845493]\n",
      "The last layer activations are: \n",
      "[0.11908174 0.02329571 0.10832047 0.11697202 0.12420147 0.09925526\n",
      " 0.11758147 0.1262533  0.10190289 0.11843984]\n",
      "The last layer activations are: \n",
      "[0.11907769 0.02306091 0.10834978 0.11695835 0.12417765 0.09926098\n",
      " 0.11756749 0.12625269 0.10191507 0.11842603]\n",
      "The last layer activations are: \n",
      "[0.11907495 0.02283321 0.10837873 0.11694456 0.12415618 0.09926743\n",
      " 0.11755387 0.12625347 0.10192699 0.11841346]\n",
      "The last layer activations are: \n",
      "[0.11907343 0.02261221 0.10840733 0.11693066 0.12413699 0.09927455\n",
      " 0.11754063 0.12625564 0.1019386  0.11840209]\n",
      "The last layer activations are: \n",
      "[0.11907309 0.02239751 0.1084356  0.11691664 0.12412003 0.09928227\n",
      " 0.11752778 0.12625916 0.10194986 0.11839189]\n",
      "The last layer activations are: \n",
      "[0.11907386 0.02218878 0.10846355 0.1169025  0.12410526 0.09929054\n",
      " 0.11751531 0.12626403 0.10196072 0.11838283]\n",
      "The last layer activations are: \n",
      "[0.1190757  0.02198569 0.1084912  0.11688822 0.12409264 0.09929931\n",
      " 0.11750323 0.12627023 0.10197116 0.11837488]\n",
      "The last layer activations are: \n",
      "[0.11907855 0.02178796 0.10851856 0.1168738  0.12408214 0.09930853\n",
      " 0.11749155 0.12627775 0.10198115 0.118368  ]\n",
      "The last layer activations are: \n",
      "[0.11908239 0.02159534 0.10854564 0.11685925 0.12407372 0.09931818\n",
      " 0.11748028 0.12628657 0.10199066 0.11836219]\n",
      "The last layer activations are: \n",
      "[0.1190872  0.02140757 0.10857244 0.11684456 0.12406738 0.09932821\n",
      " 0.11746942 0.12629671 0.10199968 0.11835743]\n",
      "The last layer activations are: \n",
      "[0.11909296 0.02122444 0.10859898 0.11682973 0.12406309 0.09933859\n",
      " 0.11745898 0.12630815 0.10200819 0.11835372]\n",
      "The last layer activations are: \n",
      "[0.11909968 0.02104575 0.10862526 0.11681478 0.12406086 0.09934932\n",
      " 0.11744898 0.1263209  0.1020162  0.11835106]\n",
      "The last layer activations are: \n",
      "[0.11910735 0.0208713  0.10865128 0.1167997  0.12406069 0.09936037\n",
      " 0.11743943 0.12633497 0.1020237  0.11834946]\n",
      "The last layer activations are: \n",
      "[0.11911601 0.02070093 0.10867705 0.11678451 0.1240626  0.09937175\n",
      " 0.11743035 0.12635039 0.10203068 0.11834893]\n",
      "The last layer activations are: \n",
      "[0.11912567 0.02053445 0.10870259 0.11676921 0.12406659 0.09938346\n",
      " 0.11742176 0.12636717 0.10203715 0.11834948]\n",
      "The last layer activations are: \n",
      "[0.11913637 0.02037173 0.1087279  0.11675382 0.12407269 0.0993955\n",
      " 0.11741367 0.12638535 0.10204312 0.11835116]\n",
      "The last layer activations are: \n",
      "[0.11914817 0.02021259 0.108753   0.11673834 0.12408094 0.09940789\n",
      " 0.11740611 0.12640495 0.1020486  0.11835399]\n",
      "The last layer activations are: \n",
      "[0.11916111 0.0200569  0.10877788 0.11672279 0.12409137 0.09942064\n",
      " 0.1173991  0.12642601 0.10205359 0.118358  ]\n",
      "The last layer activations are: \n",
      "[0.11917524 0.01990453 0.10880257 0.11670718 0.12410403 0.09943378\n",
      " 0.11739266 0.12644859 0.10205812 0.11836324]\n",
      "The last layer activations are: \n",
      "[0.11919064 0.01975533 0.10882708 0.11669152 0.12411895 0.09944733\n",
      " 0.11738682 0.12647272 0.10206219 0.11836976]\n",
      "The last layer activations are: \n",
      "[0.11920738 0.01960918 0.10885142 0.11667582 0.1241362  0.09946131\n",
      " 0.11738161 0.12649847 0.10206582 0.11837759]\n",
      "The last layer activations are: \n",
      "[0.11922552 0.01946595 0.10887561 0.11666009 0.12415584 0.09947577\n",
      " 0.11737704 0.12652588 0.10206902 0.1183868 ]\n",
      "The last layer activations are: \n",
      "[0.11924515 0.01932553 0.10889967 0.11664434 0.12417792 0.09949073\n",
      " 0.11737314 0.12655503 0.10207182 0.11839744]\n",
      "The last layer activations are: \n",
      "[0.11926635 0.01918779 0.10892361 0.11662857 0.12420252 0.09950622\n",
      " 0.11736995 0.12658597 0.10207423 0.11840958]\n",
      "The last layer activations are: \n",
      "[0.1192892  0.01905261 0.10894746 0.1166128  0.12422972 0.09952228\n",
      " 0.11736749 0.1266188  0.10207627 0.11842326]\n",
      "The last layer activations are: \n",
      "[0.11931382 0.0189199  0.10897124 0.11659703 0.1242596  0.09953895\n",
      " 0.11736578 0.12665358 0.10207794 0.11843857]\n",
      "The last layer activations are: \n",
      "[0.11934028 0.01878953 0.10899498 0.11658126 0.12429225 0.09955627\n",
      " 0.11736486 0.1266904  0.10207928 0.11845558]\n",
      "The last layer activations are: \n",
      "[0.1193687  0.01866141 0.1090187  0.1165655  0.12432778 0.09957427\n",
      " 0.11736476 0.12672936 0.1020803  0.11847435]\n",
      "The last layer activations are: \n",
      "[0.11939918 0.01853543 0.10904242 0.11654975 0.12436629 0.09959299\n",
      " 0.11736551 0.12677055 0.10208101 0.11849498]\n",
      "The last layer activations are: \n",
      "[0.11943184 0.01841149 0.10906618 0.11653401 0.1244079  0.09961248\n",
      " 0.11736714 0.12681409 0.10208142 0.11851755]\n",
      "The last layer activations are: \n",
      "[0.11946681 0.0182895  0.10909001 0.11651828 0.12445275 0.09963277\n",
      " 0.1173697  0.12686009 0.10208157 0.11854216]\n",
      "The last layer activations are: \n",
      "[0.11950421 0.01816935 0.10911395 0.11650256 0.12450097 0.09965391\n",
      " 0.11737321 0.12690867 0.10208146 0.1185689 ]\n",
      "The last layer activations are: \n",
      "[0.11954419 0.01805096 0.10913802 0.11648685 0.12455273 0.09967595\n",
      " 0.11737773 0.12695998 0.1020811  0.11859788]\n",
      "The last layer activations are: \n",
      "[0.11958689 0.01793425 0.10916227 0.11647114 0.12460819 0.09969893\n",
      " 0.11738328 0.12701416 0.10208052 0.11862922]\n",
      "The last layer activations are: \n",
      "[0.11963249 0.01781911 0.10918674 0.11645541 0.12466754 0.09972291\n",
      " 0.11738993 0.12707137 0.10207973 0.11866306]\n",
      "The last layer activations are: \n",
      "[0.11968114 0.01770548 0.10921148 0.11643968 0.12473099 0.09974793\n",
      " 0.11739772 0.12713181 0.10207874 0.11869951]\n",
      "The last layer activations are: \n",
      "[0.11973306 0.01759326 0.10923653 0.11642391 0.12479877 0.09977405\n",
      " 0.1174067  0.12719566 0.10207758 0.11873874]\n",
      "The last layer activations are: \n",
      "[0.11978843 0.01748239 0.10926194 0.11640811 0.12487113 0.09980132\n",
      " 0.11741694 0.12726313 0.10207625 0.11878092]\n",
      "The last layer activations are: \n",
      "[0.1198475  0.01737277 0.10928776 0.11639225 0.12494836 0.09982982\n",
      " 0.1174285  0.12733447 0.10207477 0.11882622]\n",
      "The last layer activations are: \n",
      "[0.11991051 0.01726435 0.10931406 0.11637633 0.12503077 0.09985961\n",
      " 0.11744146 0.12740993 0.10207317 0.11887484]\n",
      "The last layer activations are: \n",
      "[0.11997774 0.01715705 0.10934091 0.1163603  0.1251187  0.09989075\n",
      " 0.11745588 0.12748982 0.10207145 0.11892701]\n",
      "The last layer activations are: \n",
      "[0.1200495  0.01705079 0.10936836 0.11634417 0.12521254 0.09992333\n",
      " 0.11747187 0.12757445 0.10206965 0.11898297]\n",
      "The last layer activations are: \n",
      "[0.12012613 0.01694553 0.10939649 0.11632789 0.12531274 0.09995743\n",
      " 0.11748952 0.12766419 0.10206778 0.119043  ]\n",
      "The last layer activations are: \n",
      "[0.120208   0.01684118 0.10942539 0.11631143 0.12541977 0.09999315\n",
      " 0.11750893 0.12775944 0.10206586 0.11910741]\n",
      "The last layer activations are: \n",
      "[0.12029555 0.01673771 0.10945515 0.11629477 0.12553419 0.10003058\n",
      " 0.11753022 0.12786066 0.10206393 0.11917654]\n",
      "The last layer activations are: \n",
      "[0.12038924 0.01663504 0.10948585 0.11627787 0.12565663 0.10006983\n",
      " 0.11755353 0.12796836 0.102062   0.11925078]\n",
      "The last layer activations are: \n",
      "[0.12048963 0.01653313 0.1095176  0.11626067 0.12578778 0.10011102\n",
      " 0.11757901 0.12808312 0.10206012 0.11933057]\n",
      "The last layer activations are: \n",
      "[0.12059731 0.01643194 0.10955052 0.11624314 0.12592844 0.10015429\n",
      " 0.11760682 0.12820562 0.10205832 0.11941641]\n",
      "The last layer activations are: \n",
      "[0.12071299 0.01633142 0.10958473 0.11622522 0.12607953 0.10019976\n",
      " 0.11763714 0.12833658 0.10205665 0.11950885]\n",
      "The last layer activations are: \n",
      "[0.12083744 0.01623154 0.10962037 0.11620684 0.12624209 0.10024761\n",
      " 0.11767018 0.12847689 0.10205514 0.11960856]\n",
      "The last layer activations are: \n",
      "[0.12097158 0.01613227 0.10965758 0.11618793 0.12641731 0.100298\n",
      " 0.11770617 0.12862753 0.10205387 0.11971627]\n",
      "The last layer activations are: \n",
      "[0.12111643 0.01603359 0.10969652 0.11616841 0.12660658 0.10035111\n",
      " 0.11774539 0.12878963 0.1020529  0.11983282]\n",
      "The last layer activations are: \n",
      "[0.1212732  0.01593551 0.10973738 0.11614819 0.1268115  0.10040716\n",
      " 0.11778812 0.12896454 0.10205231 0.11995922]\n",
      "The last layer activations are: \n",
      "[0.12144328 0.01583803 0.10978034 0.11612717 0.12703393 0.10046636\n",
      " 0.11783469 0.12915379 0.10205219 0.1200966 ]\n",
      "The last layer activations are: \n",
      "[0.12162827 0.0157412  0.10982562 0.11610522 0.12727606 0.10052894\n",
      " 0.1178855  0.12935921 0.10205266 0.12024631]\n",
      "The last layer activations are: \n",
      "[0.12183007 0.01564508 0.10987344 0.1160822  0.12754047 0.10059516\n",
      " 0.11794096 0.12958294 0.10205385 0.12040993]\n",
      "The last layer activations are: \n",
      "[0.12205093 0.01554976 0.10992403 0.11605795 0.12783021 0.10066529\n",
      " 0.11800157 0.12982755 0.10205595 0.12058932]\n",
      "The last layer activations are: \n",
      "[0.12229349 0.01545541 0.10997766 0.11603225 0.12814895 0.1007396\n",
      " 0.11806789 0.13009611 0.10205914 0.12078672]\n",
      "The last layer activations are: \n",
      "[0.12256094 0.01536226 0.1100346  0.11600489 0.1285011  0.10081837\n",
      " 0.11814055 0.13039233 0.10206371 0.12100482]\n",
      "The last layer activations are: \n",
      "[0.12285709 0.01527063 0.11009512 0.11597556 0.12889204 0.10090184\n",
      " 0.1182203  0.13072079 0.10206998 0.12124687]\n",
      "The last layer activations are: \n",
      "[0.12318662 0.01518099 0.11015947 0.11594392 0.12932839 0.10099025\n",
      " 0.11830798 0.1310871  0.10207839 0.12151688]\n",
      "The last layer activations are: \n",
      "[0.12355522 0.01509399 0.11022792 0.11590953 0.12981839 0.10108371\n",
      " 0.11840457 0.13149832 0.1020895  0.12181982]\n",
      "The last layer activations are: \n",
      "[0.12397    0.01501057 0.11030066 0.11587185 0.13037241 0.10118224\n",
      " 0.11851118 0.13196341 0.10210408 0.12216192]\n",
      "The last layer activations are: \n",
      "[0.12443982 0.014932   0.11037783 0.11583027 0.1310037  0.10128562\n",
      " 0.11862914 0.13249388 0.10212318 0.12255111]\n",
      "The last layer activations are: \n",
      "[0.12497595 0.0148601  0.11045947 0.11578404 0.1317295  0.10139331\n",
      " 0.11875999 0.13310482 0.10214826 0.12299768]\n",
      "The last layer activations are: \n",
      "[0.12559293 0.01479734 0.11054554 0.11573249 0.13257262 0.10150442\n",
      " 0.11890563 0.13381642 0.10218142 0.12351524]\n",
      "The last layer activations are: \n",
      "[0.1263099  0.01474697 0.11063615 0.11567523 0.13356404 0.10161771\n",
      " 0.11906857 0.13465626 0.10222568 0.12412223]\n",
      "The last layer activations are: \n",
      "[0.12715289 0.01471294 0.11073208 0.11561305 0.13474717 0.10173233\n",
      " 0.11925254 0.13566319 0.10228539 0.1248446 ]\n",
      "The last layer activations are: \n",
      "[0.12815892 0.01469915 0.11083651 0.11554965 0.13618516 0.10184982\n",
      " 0.11946401 0.13689375 0.10236652 0.12572044]\n",
      "The last layer activations are: \n",
      "[0.12938336 0.01470703 0.11095857 0.11549517 0.13797341 0.10197947\n",
      " 0.11971536 0.13843265 0.10247645 0.1268081 ]\n",
      "The last layer activations are: \n",
      "[0.13091082 0.01473027 0.11111946 0.11547118 0.14025681 0.10214819\n",
      " 0.12002993 0.14040682 0.10262155 0.12819799]\n",
      "The last layer activations are: \n",
      "[0.13286067 0.01474736 0.11135651 0.11551287 0.14323782 0.10241054\n",
      " 0.12044461 0.14299138 0.10280083 0.13001967]\n",
      "The last layer activations are: \n",
      "[0.13535673 0.01472006 0.11171072 0.11565749 0.14713072 0.10283815\n",
      " 0.12099596 0.14637278 0.10299689 0.13241642]\n",
      "The last layer activations are: \n",
      "[0.13844702 0.01460794 0.11219155 0.11591791 0.15204195 0.10347322\n",
      " 0.12168481 0.15065653 0.10317401 0.13547372]\n",
      "The last layer activations are: \n",
      "[0.14208367 0.01438274 0.11276521 0.11627561 0.15793811 0.10430409\n",
      " 0.12246745 0.15585148 0.10328328 0.13919951]\n",
      "The last layer activations are: \n",
      "[0.14621155 0.01402462 0.11338733 0.1166982  0.16477796 0.10530313\n",
      " 0.12329117 0.16198358 0.10325421 0.14360388]\n",
      "The last layer activations are: \n",
      "[0.15073746 0.01353056 0.11400127 0.11713302 0.17246691 0.10641982\n",
      " 0.12408843 0.16905426 0.10299078 0.14867349]\n",
      "The last layer activations are: \n",
      "[0.15562426 0.01289836 0.11456775 0.11750219 0.1809949  0.10762905\n",
      " 0.12481019 0.17714516 0.1023416  0.15445416]\n",
      "The last layer activations are: \n",
      "[0.1605466  0.01217867 0.1149913  0.11767712 0.18988812 0.10879198\n",
      " 0.12533487 0.18594342 0.10117921 0.16072064]\n",
      "The last layer activations are: \n",
      "[0.16619615 0.01129175 0.11536249 0.11737558 0.20029694 0.11017727\n",
      " 0.12578899 0.19649741 0.0990292  0.16821945]\n",
      "The last layer activations are: \n",
      "[0.16837884 0.01076215 0.11511009 0.11689837 0.20512401 0.11025387\n",
      " 0.12533478 0.20261327 0.09715601 0.17272167]\n",
      "The last layer activations are: \n",
      "[0.18569446 0.00867266 0.11580638 0.11168899 0.23721275 0.115368\n",
      " 0.12687226 0.23363737 0.08804393 0.19406007]\n",
      "The last layer activations are: \n",
      "[0.05291229 0.0652979  0.11129022 0.18827545 0.03890527 0.06307333\n",
      " 0.10622211 0.04678662 0.20460999 0.05416867]\n",
      "The last layer activations are: \n",
      "[0.20890386 0.00242136 0.11496427 0.0539469  0.26466242 0.12845985\n",
      " 0.13268979 0.2794386  0.02197424 0.2275347 ]\n",
      "The last layer activations are: \n",
      "[0.10249245 0.00469152 0.11019206 0.06210751 0.0756396  0.10797226\n",
      " 0.11930327 0.06958585 0.02390316 0.09330416]\n",
      "The last layer activations are: \n",
      "[0.12457851 0.0028155  0.10732681 0.06733627 0.09228516 0.11030587\n",
      " 0.12393551 0.08429907 0.0251239  0.10839305]\n",
      "The last layer activations are: \n",
      "[0.11710074 0.00380256 0.10743376 0.07753472 0.10199198 0.10345122\n",
      " 0.11775204 0.09680743 0.02767927 0.11082953]\n",
      "The last layer activations are: \n",
      "[0.11890224 0.00367146 0.10729885 0.08797975 0.11204562 0.10214026\n",
      " 0.11716224 0.10889858 0.03049719 0.11522928]\n",
      "The last layer activations are: \n",
      "[0.1188806  0.00367623 0.10751911 0.0973697  0.11829116 0.10106624\n",
      " 0.11661689 0.117542   0.03385169 0.11708339]\n",
      "The last layer activations are: \n",
      "[0.11895606 0.00370305 0.10772343 0.10464945 0.12189509 0.10043547\n",
      " 0.11635567 0.12279509 0.0377796  0.11810937]\n",
      "The last layer activations are: \n",
      "[0.11934694 0.00373335 0.10789931 0.10949941 0.12412413 0.10012097\n",
      " 0.11627136 0.12583744 0.04235124 0.11894295]\n",
      "The last layer activations are: \n",
      "[0.12015032 0.00376403 0.10806643 0.11235422 0.12598836 0.10003199\n",
      " 0.11628548 0.12800202 0.04761719 0.11994133]\n",
      "The last layer activations are: \n",
      "[0.12161935 0.0037983  0.10825249 0.11396774 0.12832388 0.10012524\n",
      " 0.11635891 0.13039439 0.05356015 0.12149121]\n",
      "The last layer activations are: \n",
      "[0.12423531 0.00385659 0.10854741 0.11520478 0.13196149 0.10031915\n",
      " 0.11648129 0.13406015 0.06000755 0.12415553]\n",
      "The last layer activations are: \n",
      "[0.12830083 0.00401816 0.10933871 0.11775968 0.13723601 0.10021631\n",
      " 0.11670033 0.13982768 0.06670052 0.12842648]\n",
      "The last layer activations are: \n",
      "[0.14549919 0.00401008 0.10796429 0.11655323 0.16349147 0.10270761\n",
      " 0.11695076 0.16586273 0.07429211 0.14743374]\n",
      "The last layer activations are: \n",
      "[0.15312915 0.00415435 0.10764358 0.11509659 0.17655279 0.10329547\n",
      " 0.11676349 0.17871018 0.08270707 0.15728524]\n",
      "The last layer activations are: \n",
      "[0.17388961 0.00397874 0.1007669  0.09767768 0.21623887 0.10604608\n",
      " 0.11453006 0.21749017 0.08838323 0.18491691]\n",
      "The last layer activations are: \n",
      "[0.11380086 0.00578131 0.12745092 0.14748436 0.11330128 0.0979699\n",
      " 0.12374008 0.1180762  0.1127437  0.11524177]\n",
      "The last layer activations are: \n",
      "[0.19062036 0.00393412 0.07374692 0.04566823 0.25162416 0.10859004\n",
      " 0.10337735 0.26413754 0.0782383  0.21667767]\n",
      "The last layer activations are: \n",
      "[0.11932688 0.00405841 0.08219264 0.05276941 0.09470818 0.10402368\n",
      " 0.10882788 0.08786266 0.08510144 0.10796564]\n",
      "The last layer activations are: \n",
      "[0.12382589 0.00394798 0.09016762 0.06152793 0.10936697 0.10298073\n",
      " 0.11268246 0.10367075 0.09096759 0.11689214]\n",
      "The last layer activations are: \n",
      "[0.12812234 0.00399056 0.09702494 0.07223066 0.12371794 0.10226413\n",
      " 0.1141361  0.12014625 0.09588551 0.12478491]\n",
      "The last layer activations are: \n",
      "[0.13923189 0.00403607 0.10184815 0.08666858 0.14293027 0.10238336\n",
      " 0.11349433 0.14138814 0.09942674 0.13847365]\n",
      "The last layer activations are: \n",
      "[0.15857452 0.00403897 0.0957371  0.09390054 0.1780782  0.10316781\n",
      " 0.109771   0.17889491 0.08990568 0.16577364]\n",
      "The last layer activations are: \n",
      "[0.16817488 0.00412342 0.09394643 0.09421755 0.19407502 0.10285148\n",
      " 0.10711979 0.19688489 0.0883959  0.17794773]\n",
      "The last layer activations are: \n",
      "[0.18085295 0.00408351 0.08309746 0.07405137 0.22224107 0.10250481\n",
      " 0.10194527 0.22766945 0.07660402 0.19955081]\n",
      "The last layer activations are: \n",
      "[0.16148099 0.00447093 0.09387818 0.09369324 0.17752395 0.10143937\n",
      " 0.10554402 0.18067887 0.08728653 0.16852287]\n",
      "The last layer activations are: \n",
      "[0.18360262 0.00407812 0.05831899 0.04304023 0.24655016 0.10061892\n",
      " 0.08673723 0.26880742 0.05480131 0.22165461]\n",
      "The last layer activations are: \n",
      "[0.1316368  0.00419794 0.06806813 0.0510749  0.11440468 0.09969663\n",
      " 0.0959208  0.10295946 0.06310471 0.11971899]\n",
      "The last layer activations are: \n",
      "[0.15502424 0.00404903 0.0767979  0.06477456 0.1467829  0.09946979\n",
      " 0.09591069 0.13472073 0.07006128 0.14864914]\n",
      "The last layer activations are: \n",
      "[0.17241532 0.00414082 0.07580985 0.07247387 0.1801328  0.09816526\n",
      " 0.09177253 0.17188187 0.0701796  0.17545916]\n",
      "The last layer activations are: \n",
      "[0.18209415 0.00413857 0.07268143 0.06810383 0.20347661 0.09705578\n",
      " 0.08927621 0.20268548 0.06764268 0.19259452]\n",
      "The last layer activations are: \n",
      "[0.18736544 0.00415202 0.06886823 0.06421217 0.21567403 0.09604495\n",
      " 0.0860458  0.22039096 0.06448761 0.20192487]\n",
      "The last layer activations are: \n",
      "[0.19136878 0.00415054 0.06515548 0.06032746 0.22349552 0.09511904\n",
      " 0.08263293 0.23061161 0.06134067 0.20882333]\n",
      "The last layer activations are: \n",
      "[0.19304188 0.00416026 0.06311594 0.05867897 0.22524641 0.09418057\n",
      " 0.07991439 0.2328874  0.05961886 0.21124925]\n",
      "The last layer activations are: \n",
      "[0.1958861  0.0041544  0.05924574 0.05458324 0.23113091 0.09325679\n",
      " 0.07612006 0.24030515 0.05664761 0.21697073]\n",
      "The last layer activations are: \n",
      "[0.19411429 0.00418313 0.05990881 0.05609012 0.22438721 0.0924013\n",
      " 0.07529568 0.2321634  0.05691931 0.21231182]\n",
      "The last layer activations are: \n",
      "[0.19921154 0.00414852 0.05186863 0.04735509 0.24125813 0.09153968\n",
      " 0.06837775 0.25460465 0.05136531 0.22731731]\n",
      "The last layer activations are: \n",
      "[0.17112902 0.00452011 0.06712885 0.06377491 0.18327286 0.09203072\n",
      " 0.08000951 0.18714906 0.06214169 0.17811865]\n",
      "The last layer activations are: \n",
      "[0.19115694 0.004137   0.03204025 0.02841638 0.24490365 0.08922288\n",
      " 0.04837089 0.29011145 0.03801273 0.23672146]\n",
      "The last layer activations are: \n",
      "[0.13153807 0.00426366 0.03742726 0.03276816 0.11706547 0.09171125\n",
      " 0.05693915 0.09260852 0.04351749 0.11485926]\n",
      "The last layer activations are: \n",
      "[0.15729069 0.0040734  0.04724307 0.04289718 0.15022102 0.09157801\n",
      " 0.06803908 0.12153023 0.050544   0.14502687]\n",
      "The last layer activations are: \n",
      "[0.17795673 0.00419333 0.05298287 0.05056312 0.1861352  0.0904872\n",
      " 0.06704816 0.15996496 0.05185778 0.17732503]\n",
      "The last layer activations are: \n",
      "[0.18911846 0.00417868 0.05249474 0.04999416 0.20935842 0.08964626\n",
      " 0.06615741 0.1970987  0.05152994 0.19895747]\n",
      "The last layer activations are: \n",
      "[0.19496652 0.00419195 0.04988461 0.04747994 0.22151206 0.08905889\n",
      " 0.0631837  0.22325577 0.04994808 0.21108354]\n",
      "The last layer activations are: \n",
      "[0.19729413 0.00418921 0.04924027 0.04716979 0.22498294 0.08861116\n",
      " 0.06197658 0.23275717 0.04919012 0.21520382]\n",
      "The last layer activations are: \n",
      "[0.19995635 0.00419067 0.04649564 0.04443533 0.2307432  0.08821486\n",
      " 0.05886664 0.24099134 0.04743538 0.22095579]\n",
      "The last layer activations are: \n",
      "[0.19955075 0.00419537 0.04715996 0.04546859 0.22788944 0.08769873\n",
      " 0.0589374  0.23733894 0.04750465 0.21887527]\n",
      "The last layer activations are: \n",
      "[0.20208967 0.00419159 0.0432076  0.04150739 0.23616587 0.08731721\n",
      " 0.05468649 0.2483788  0.04516208 0.22672849]\n",
      "The last layer activations are: \n",
      "[0.19866891 0.00421165 0.04642146 0.04503884 0.22502931 0.08673689\n",
      " 0.05731402 0.23381153 0.04667087 0.21689192]\n",
      "The last layer activations are: \n",
      "[0.20232141 0.00418795 0.03800021 0.03685654 0.24555745 0.08645773\n",
      " 0.04867124 0.26246695 0.04197081 0.23605292]\n",
      "The last layer activations are: \n",
      "[0.17944563 0.00438504 0.05035201 0.04870259 0.19182438 0.08663029\n",
      " 0.06038515 0.19577992 0.04935669 0.1869691 ]\n",
      "The last layer activations are: \n",
      "[0.19071842 0.00418204 0.02621298 0.02632037 0.24321065 0.08493013\n",
      " 0.03548961 0.28296241 0.03449141 0.23851373]\n",
      "The last layer activations are: \n",
      "[0.15285261 0.00426502 0.03302268 0.03268381 0.14907585 0.08691111\n",
      " 0.04405016 0.1344432  0.04041216 0.1449456 ]\n",
      "The last layer activations are: \n",
      "[0.17216848 0.00411626 0.03784525 0.03837595 0.19297142 0.08691538\n",
      " 0.04929017 0.18529236 0.04134322 0.18531573]\n",
      "The last layer activations are: \n",
      "[0.18553885 0.0042116  0.0431895  0.04332802 0.20904195 0.08546353\n",
      " 0.05296236 0.21152966 0.04457013 0.20094787]\n",
      "The last layer activations are: \n",
      "[0.19726388 0.00419047 0.03729014 0.03677315 0.22889234 0.08548887\n",
      " 0.04714781 0.23880387 0.041793   0.21984958]\n",
      "The last layer activations are: \n",
      "[0.19770459 0.0041886  0.04136645 0.04161434 0.22306799 0.08484465\n",
      " 0.0509738  0.23208141 0.04337364 0.21554824]\n",
      "The last layer activations are: \n",
      "[0.20252805 0.00420185 0.0354651  0.03516201 0.23751932 0.08474033\n",
      " 0.04446839 0.25121553 0.04030212 0.22896239]\n",
      "The last layer activations are: \n",
      "[0.19972075 0.00418445 0.04012066 0.04048822 0.22485535 0.08405484\n",
      " 0.04932886 0.2336883  0.04234884 0.21771976]\n",
      "The last layer activations are: \n",
      "[0.20334215 0.00420504 0.03359597 0.03352647 0.24180641 0.08391926\n",
      " 0.041919   0.25700653 0.03893389 0.23331079]\n",
      "The last layer activations are: \n",
      "[0.19838677 0.00419155 0.03944608 0.03986015 0.22195619 0.08314591\n",
      " 0.04818826 0.23005318 0.04170959 0.21530072]\n",
      "The last layer activations are: \n",
      "[0.20178588 0.00420459 0.0309215  0.0311949  0.24668413 0.08311073\n",
      " 0.03866903 0.26499203 0.0371728  0.23833737]\n",
      "The last layer activations are: \n",
      "[0.19081995 0.00421701 0.03918828 0.03955291 0.20862855 0.08240865\n",
      " 0.04734997 0.21431914 0.04157568 0.2030801 ]\n",
      "The last layer activations are: \n",
      "[0.1952801  0.0042013  0.02754698 0.02820025 0.24623323 0.08235418\n",
      " 0.0349205  0.2699998  0.034818   0.2391451 ]\n",
      "The last layer activations are: \n",
      "[0.17877064 0.00422454 0.03661632 0.03717307 0.18937115 0.08221953\n",
      " 0.04453121 0.19092556 0.04051114 0.18501819]\n",
      "The last layer activations are: \n",
      "[0.19030037 0.00419362 0.02876925 0.02969963 0.231648   0.08248974\n",
      " 0.03621363 0.25145783 0.03486833 0.22499214]\n",
      "The last layer activations are: \n",
      "[0.18561942 0.00418747 0.03680438 0.03785074 0.20411765 0.0819539\n",
      " 0.04448823 0.21053274 0.03970428 0.19849253]\n",
      "The last layer activations are: \n",
      "[0.19805968 0.00420624 0.02909289 0.02979075 0.23596674 0.08272751\n",
      " 0.03680409 0.2533679  0.03599052 0.22786787]\n",
      "The last layer activations are: \n",
      "[0.19602704 0.00416882 0.03566357 0.03680667 0.21900308 0.08203253\n",
      " 0.04376972 0.22738726 0.03921135 0.21268337]\n",
      "The last layer activations are: \n",
      "[0.20329038 0.00421236 0.02932264 0.03005341 0.24150548 0.08237393\n",
      " 0.03618015 0.2581236  0.03575625 0.23340588]\n",
      "The last layer activations are: \n",
      "[0.20132988 0.00416333 0.03451725 0.035784   0.22707921 0.08167425\n",
      " 0.04225059 0.23628827 0.03817459 0.22041962]\n",
      "The last layer activations are: \n",
      "[0.20513278 0.00421413 0.0293134  0.03002873 0.24393559 0.08158921\n",
      " 0.03591744 0.25994902 0.03552921 0.23614477]\n",
      "The last layer activations are: \n",
      "[0.20347488 0.00416952 0.03348141 0.03476568 0.23228226 0.08088633\n",
      " 0.04088068 0.24252359 0.03727608 0.22539864]\n",
      "The last layer activations are: \n",
      "[0.20511975 0.00421353 0.02893225 0.0297609  0.24847874 0.0805652\n",
      " 0.03523353 0.26518499 0.03505134 0.24063541]\n",
      "The last layer activations are: \n",
      "[0.20282474 0.00419105 0.03294563 0.03422758 0.23480186 0.07988817\n",
      " 0.04002739 0.24552551 0.03676367 0.22785847]\n",
      "The last layer activations are: \n",
      "[0.20228527 0.00420924 0.02771983 0.0287242  0.25971734 0.07945148\n",
      " 0.03365737 0.27983021 0.0341594  0.2513759 ]\n",
      "The last layer activations are: \n",
      "[0.19498971 0.00426774 0.03424867 0.03539286 0.22099731 0.07898504\n",
      " 0.04101322 0.22839052 0.03760573 0.21517943]\n",
      "The last layer activations are: \n",
      "[0.19378301 0.00420746 0.02399757 0.02534662 0.27010733 0.07827385\n",
      " 0.02945841 0.2983128  0.03198444 0.26223122]\n",
      "The last layer activations are: \n",
      "[0.17922019 0.00424302 0.0322388  0.03341612 0.18733526 0.07786131\n",
      " 0.03833735 0.18526538 0.03708783 0.18368982]\n",
      "The last layer activations are: \n",
      "[0.18419423 0.00419983 0.02253341 0.024044   0.23416076 0.07772431\n",
      " 0.02819391 0.25545657 0.02997949 0.22889741]\n",
      "The last layer activations are: \n",
      "[0.18219265 0.00417062 0.0296587  0.03144374 0.20398902 0.07753018\n",
      " 0.03614247 0.21007396 0.03482524 0.1983387 ]\n",
      "The last layer activations are: \n",
      "[0.1947444  0.00421095 0.02671859 0.02815283 0.23857115 0.07885723\n",
      " 0.03270731 0.25547586 0.03316397 0.23031979]\n",
      "The last layer activations are: \n",
      "[0.19894866 0.0041719  0.03030451 0.03210928 0.2318536  0.07832662\n",
      " 0.03677464 0.24256834 0.03497252 0.22487457]\n",
      "The last layer activations are: \n",
      "[0.20300377 0.00421958 0.02655791 0.02781485 0.24579837 0.07833962\n",
      " 0.0321403  0.26126323 0.03322054 0.23817732]\n",
      "The last layer activations are: \n",
      "[0.20321123 0.00418315 0.02913718 0.03093998 0.24364467 0.07792723\n",
      " 0.03543694 0.25690051 0.03404817 0.23619091]\n",
      "The last layer activations are: \n",
      "[0.20213639 0.00421428 0.02634477 0.02773378 0.26087007 0.07728015\n",
      " 0.03166761 0.27985582 0.03274761 0.25242301]\n",
      "The last layer activations are: \n",
      "[0.19834028 0.00425529 0.02978623 0.03156857 0.2437611  0.0769635\n",
      " 0.03591896 0.25661341 0.03455143 0.23639413]\n",
      "The last layer activations are: \n",
      "[0.19441467 0.00419634 0.02426463 0.02578614 0.28601106 0.07558492\n",
      " 0.02906279 0.31418806 0.03135176 0.27608738]\n",
      "The last layer activations are: \n",
      "[0.17820271 0.00441732 0.03398016 0.03516198 0.18852008 0.07572983\n",
      " 0.03969007 0.18722636 0.03725995 0.18503758]\n",
      "The last layer activations are: \n",
      "[0.18403213 0.004203   0.01882669 0.02104103 0.2614767  0.07409485\n",
      " 0.02339868 0.28968858 0.02849284 0.25479665]\n",
      "The last layer activations are: \n",
      "[0.17478257 0.0041622  0.02463723 0.02746232 0.18952349 0.0742004\n",
      " 0.03015916 0.18788905 0.03324786 0.18498202]\n",
      "The last layer activations are: \n",
      "[0.1858682  0.00417385 0.02501409 0.02661406 0.23214471 0.07550169\n",
      " 0.03047135 0.24416457 0.03047194 0.22458577]\n",
      "The last layer activations are: \n",
      "[0.19386741 0.00415823 0.02771333 0.0298442  0.23138008 0.07471096\n",
      " 0.03328346 0.24225639 0.03294185 0.2240956 ]\n",
      "The last layer activations are: \n",
      "[0.19999055 0.00419777 0.02460558 0.02617159 0.24330737 0.0749731\n",
      " 0.02963475 0.2575462  0.03157511 0.23568704]\n",
      "The last layer activations are: \n",
      "[0.20223238 0.00416062 0.02639253 0.02847407 0.24617121 0.07487259\n",
      " 0.03194786 0.25996626 0.03191292 0.23837544]\n",
      "The last layer activations are: \n",
      "[0.20195352 0.00420088 0.02456614 0.02624264 0.25794492 0.07442318\n",
      " 0.02936838 0.2755936  0.03115775 0.24973191]\n",
      "The last layer activations are: \n",
      "[0.1998294  0.00419671 0.02615966 0.02823323 0.25818006 0.07414073\n",
      " 0.03154316 0.27494972 0.03177516 0.24981401]\n",
      "The last layer activations are: \n",
      "[0.19684453 0.00417638 0.02381143 0.02557228 0.28293841 0.07295908\n",
      " 0.02832572 0.30770383 0.03041723 0.27321258]\n",
      "The last layer activations are: \n",
      "[0.1771825  0.00459125 0.03375308 0.03602615 0.20445365 0.07684193\n",
      " 0.0397638  0.20890553 0.03819426 0.1986658 ]\n",
      "The last layer activations are: \n",
      "[0.18411442 0.00416201 0.01916876 0.02085958 0.30778637 0.07015663\n",
      " 0.02264145 0.35081186 0.02749891 0.2966152 ]\n",
      "The last layer activations are: \n",
      "[0.16762416 0.00423547 0.02542128 0.0271195  0.15963691 0.06996955\n",
      " 0.02927988 0.14371593 0.03212064 0.15814635]\n",
      "The last layer activations are: \n",
      "[0.17252992 0.00411225 0.01956426 0.02154238 0.20688419 0.07032525\n",
      " 0.02455017 0.20451676 0.02615236 0.20305786]\n",
      "The last layer activations are: \n",
      "[0.18533626 0.00405165 0.02492788 0.02750224 0.22219591 0.07038204\n",
      " 0.03076666 0.22873903 0.03027345 0.21489772]\n",
      "Average cost is:  0.07183512180118339\n",
      "Percentage of correct is:  0.3188\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.060237   0.01264053 0.21273463 0.21387904 0.01563199 0.13202491\n",
      " 0.21119695 0.01136387 0.18886479 0.01614766]\n",
      "The cost is: 0.07653693481050593\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[1.35060507e-04 9.96827117e-01 4.98625220e-04 1.26290854e-03\n",
      " 1.88266201e-06 5.91698458e-05 1.38439445e-03 1.53253205e-05\n",
      " 2.46766123e-02 7.10772427e-05]\n",
      "The cost is: 6.227895242318465e-05\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.06028497 0.01262102 0.2128398  0.21397026 0.01564885 0.13212701\n",
      " 0.21129915 0.01137272 0.18885989 0.01615833]\n",
      "The cost is: 0.07653453498052595\n",
      "Label for the data is: 9\n",
      "The last layer activations are: \n",
      "[0.17463007 0.00464837 0.02930375 0.03105888 0.19051717 0.0753222\n",
      " 0.0335707  0.19534257 0.03632673 0.18514771]\n",
      "The cost is: 0.07789004523886597\n",
      "Label for the data is: 1\n",
      "The last layer activations are: \n",
      "[1.38215551e-04 9.96707147e-01 5.14018948e-04 1.29647180e-03\n",
      " 1.94749503e-06 6.12216814e-05 1.41938239e-03 1.56889176e-05\n",
      " 2.50246612e-02 7.24300077e-05]\n",
      "The cost is: 6.410645955989083e-05\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[0.18545182 0.00438878 0.02545143 0.02708275 0.21805631 0.07227296\n",
      " 0.02944865 0.2276526  0.03242744 0.21170778]\n",
      "The cost is: 0.08162258334752608\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.04138817 0.02603292 0.15377912 0.16131514 0.00946522 0.08558264\n",
      " 0.15652765 0.00809147 0.17792118 0.01215286]\n",
      "The cost is: 0.09446799965829696\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "[0.19441104 0.00413794 0.02347738 0.02501709 0.23954611 0.07121984\n",
      " 0.027339   0.25185426 0.03014483 0.23143745]\n",
      "The cost is: 0.0741002200140046\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.06146659 0.01223008 0.21229827 0.21327432 0.01615842 0.1333006\n",
      " 0.21096477 0.01171091 0.18697759 0.01656925]\n",
      "The cost is: 0.09257915836897246\n",
      "Label for the data is: 9\n",
      "The last layer activations are: \n",
      "[0.19355299 0.00417845 0.02340779 0.024955   0.23819354 0.07088077\n",
      " 0.02725871 0.2507814  0.03016961 0.23057127]\n",
      "The cost is: 0.0756976258118572\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "[0.19436479 0.00414021 0.02347158 0.02501172 0.23947701 0.0711978\n",
      " 0.0273326  0.25180438 0.03014438 0.23139748]\n",
      "The cost is: 0.0741041655512542\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.06242485 0.012019   0.20858832 0.20961359 0.0167001  0.13284993\n",
      " 0.20759974 0.0121383  0.18378863 0.01708698]\n",
      "The cost is: 0.0921031613266572\n",
      "Label for the data is: 3\n",
      "The last layer activations are: \n",
      "[0.05982983 0.01280579 0.21193713 0.21319359 0.01548682 0.13119962\n",
      " 0.21041316 0.01128558 0.18896578 0.01605302]\n",
      "The cost is: 0.0765545280574489\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[0.06011373 0.01270671 0.21176925 0.21299444 0.01560603 0.13146317\n",
      " 0.2103165  0.0113651  0.18846055 0.01615025]\n",
      "The cost is: 0.08147333791753814\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[0.05918587 0.01313386 0.2078952  0.20951494 0.01531935 0.12869379\n",
      " 0.20673979 0.01125105 0.18739194 0.01601619]\n",
      "The cost is: 0.08110454641498432\n",
      "Label for the data is: 8\n",
      "The last layer activations are: \n",
      "[0.05914673 0.01309203 0.21059725 0.21203829 0.01524673 0.1298186\n",
      " 0.20908108 0.01115636 0.18916084 0.01589639]\n",
      "The cost is: 0.08116187000469592\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "[0.18370648 0.00442928 0.02601786 0.02766879 0.21353115 0.07273371\n",
      " 0.03005785 0.22233003 0.03300918 0.20735247]\n",
      "The cost is: 0.07534523890199284\n",
      "Label for the data is: 5\n",
      "The last layer activations are: \n",
      "[0.15819226 0.00543344 0.03138583 0.03336052 0.15977066 0.07340511\n",
      " 0.03572109 0.16430012 0.03983302 0.1593529 ]\n",
      "The cost is: 0.09665076061523142\n",
      "Label for the data is: 0\n",
      "The last layer activations are: \n",
      "[0.18645507 0.0043603  0.02520851 0.02682909 0.22046129 0.07213545\n",
      " 0.02918921 0.23037405 0.03215246 0.21393181]\n",
      "The cost is: 0.08177611459315373\n",
      "Label for the data is: 4\n",
      "The last layer activations are: \n",
      "[0.17561504 0.00514053 0.02169109 0.02341215 0.20921748 0.06332921\n",
      " 0.02541053 0.227561   0.03029557 0.21193709]\n",
      "The cost is: 0.07594981459794847\n",
      "Layer:  0\n",
      "Activations: \n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.03137255 0.54901961\n",
      " 0.72941176 0.43921569 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.10588235\n",
      " 0.02745098 0.         0.         0.         0.         0.\n",
      " 0.         0.22745098 0.78039216 0.99607843 0.99607843 0.9254902\n",
      " 0.23529412 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.14509804 0.91764706 0.57254902 0.1254902\n",
      " 0.         0.         0.         0.         0.         0.4745098\n",
      " 0.99607843 0.99607843 0.99607843 0.95686275 0.36862745 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21176471 0.99607843 0.99607843 0.89019608 0.16470588 0.\n",
      " 0.         0.         0.22745098 0.95686275 0.99607843 0.99607843\n",
      " 0.99607843 0.67843137 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.03921569 0.79215686\n",
      " 0.99607843 0.99607843 0.89803922 0.15686275 0.         0.\n",
      " 0.47058824 0.99607843 0.99607843 0.99607843 0.95686275 0.22745098\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54901961 0.99607843 0.99607843\n",
      " 0.99607843 0.44313725 0.         0.         0.76862745 0.99607843\n",
      " 0.99607843 0.99607843 0.68627451 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01568627 0.59607843 0.99607843 0.99607843 0.92941176\n",
      " 0.19215686 0.00784314 0.77254902 0.99607843 0.99607843 0.99607843\n",
      " 0.41960784 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.15686275 0.89019608 0.99607843 0.99607843 0.85098039 0.30588235\n",
      " 0.99607843 0.99607843 0.99607843 0.99607843 0.41960784 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.35294118\n",
      " 0.99215686 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.99607843 0.41960784 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.51764706 0.99607843\n",
      " 0.99607843 1.         0.99607843 1.         0.99607843 0.98823529\n",
      " 0.32156863 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00784314 0.49411765 0.88627451 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.80784314 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.23921569 1.         0.99607843 1.\n",
      " 0.99607843 0.48627451 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.16470588 0.99607843 0.99607843 0.99607843 0.99607843 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.16470588 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.01960784 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.16470588 1.         0.99607843 0.99607843\n",
      " 0.99607843 0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.16470588 0.99607843 0.99607843 0.99607843 0.99607843 0.01960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.16470588 0.99607843\n",
      " 0.99607843 0.99607843 0.88235294 0.01568627 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.16470588 0.99607843 0.99607843 0.99607843\n",
      " 0.69803922 0.00784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02352941 0.55294118 0.99607843 0.99607843 0.99607843 0.01960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01568627\n",
      " 0.42745098 0.9254902  0.79215686 0.01176471 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Biases: \n",
      "[-0.81059092 -0.52333449 -0.0098939  -0.59209522 -0.98937187 -0.04193782\n",
      " -0.44071614 -0.10402282 -0.88675393 -0.92264784 -0.43435632 -0.79819657\n",
      " -0.77022766 -0.02587886 -0.27334204 -0.98630154 -0.76140321 -0.11718551\n",
      " -0.57295887 -0.0580304  -0.80190813 -0.90148061 -0.91202066 -0.2613117\n",
      " -0.78158364 -0.59222242 -0.5602912  -0.20247911 -0.49726514 -0.59694431\n",
      " -0.69259266 -0.31878682 -0.34717616 -0.66965407 -0.86854141 -0.24953775\n",
      " -0.44770138 -0.8618247  -0.95260363 -0.02102477 -0.29322279 -0.54675476\n",
      " -0.73690952 -0.9203162  -0.96853203 -0.49162459 -0.88876769 -0.70278179\n",
      " -0.37130856 -0.14550136 -0.20679881 -0.41943501 -0.97681844 -0.86401772\n",
      " -0.93551936 -0.13816336 -0.90412136 -0.56423722 -0.03703341 -0.33890746\n",
      " -0.58167633 -0.86058591 -0.11891009 -0.98026988 -0.38485371 -0.38991067\n",
      " -0.84643825 -0.60737475 -0.02922452 -0.74578903 -0.27216902 -0.98246019\n",
      " -0.26614301 -0.67322132 -0.31524751 -0.40901873 -0.09850077 -0.14097146\n",
      " -0.54102994 -0.491002   -0.23215556 -0.3391536  -0.88411796 -0.76956894\n",
      " -0.61865505 -0.08774118 -0.33570896 -0.15850754 -0.88312109 -0.83802972\n",
      " -0.32630283 -0.59382916 -0.75594784 -0.41045355 -0.37646516 -0.70314286\n",
      " -0.90193337 -0.40730011 -0.95526618 -0.036623   -0.37051135 -0.29749432\n",
      " -0.7113949  -0.29894835 -0.07346482 -0.16200147 -0.85509452 -0.90899194\n",
      " -0.04264623 -0.99870497 -0.54779074 -0.44501083 -0.24135562 -0.58092006\n",
      " -0.40305316 -0.19944736 -0.85777154 -0.66341571 -0.58602922 -0.93649506\n",
      " -0.83215207 -0.56215643 -0.30291879 -0.80293816 -0.48402472 -0.15969539\n",
      " -0.53575952 -0.52939073 -0.65954135 -0.99658333 -0.68962982 -0.03877699\n",
      " -0.42696438 -0.07581958 -0.69613317 -0.91930322 -0.48872203 -0.33592903\n",
      " -0.26540314 -0.69878503 -0.01979079 -0.51378904 -0.89206135 -0.18589399\n",
      " -0.16866514 -0.13425673 -0.27988608 -0.55539314 -0.2621873  -0.96730577\n",
      " -0.20515344 -0.98540531 -0.16423671 -0.16563092 -0.51246042 -0.41780538\n",
      " -0.81076112 -0.49489924 -0.81207145 -0.31205786 -0.90869782 -0.61833222\n",
      " -0.71981171 -0.78256066 -0.14505589 -0.05641234 -0.46006384 -0.47697687\n",
      " -0.13124661 -0.91564264 -0.26594746 -0.39427527 -0.38444651 -0.44637376\n",
      " -0.03757287 -0.07340924 -0.72102357 -0.81544591 -0.11096584 -0.34719164\n",
      " -0.38228995 -0.93845805 -0.57293351 -0.20021635 -0.50552255 -0.56982017\n",
      " -0.29222329 -0.5588375  -0.00118468 -0.23408922 -0.56772243 -0.90348996\n",
      " -0.59703125 -0.88047551 -0.77679279 -0.8144709  -0.11529048 -0.17019973\n",
      " -0.67077146 -0.01300325 -0.57500783 -0.09408466 -0.29140817 -0.420027\n",
      " -0.46709144 -0.14819374 -0.50788523 -0.78473656 -0.9938394  -0.01128273\n",
      " -0.74817028 -0.41827609 -0.04748742 -0.4099681  -0.67994532 -0.62412199\n",
      " -0.22173484 -0.2885719  -0.43872755 -0.6704019  -0.07881317 -0.82336535\n",
      " -0.12779079 -0.59956432 -0.95666946 -0.65327381 -0.80907263 -0.44036545\n",
      " -0.90482191 -0.58835343 -0.10482469 -0.03685465 -0.47649333 -0.84576334\n",
      " -0.88164051 -0.54660639 -0.52503137 -0.93122414 -0.56379279 -0.64310107\n",
      " -0.02877748 -0.40522243 -0.41655001 -0.04210089 -0.83425181 -0.73317656\n",
      " -0.77298165 -0.34274002 -0.90376232 -0.70653985 -0.97250541 -0.44888164\n",
      " -0.78294973 -0.64377561 -0.05616358 -0.41931248 -0.76336113 -0.06670209\n",
      " -0.99025115 -0.46512503 -0.91590139 -0.57539116 -0.6325219  -0.79801357\n",
      " -0.65185392 -0.24739083 -0.89029113 -0.68448281 -0.47042769 -0.77287477\n",
      " -0.68504659 -0.64906145 -0.39106146 -0.94532519 -0.00209353 -0.57187332\n",
      " -0.95878295 -0.13476811 -0.05936606 -0.40674903 -0.08300028 -0.03949664\n",
      " -0.85152732 -0.8612381  -0.9610333  -0.75234387 -0.10502586 -0.53260048\n",
      " -0.48440626 -0.47000023 -0.10768416 -0.36678457 -0.28627653 -0.05244297\n",
      " -0.31600246 -0.9505964  -0.07771184 -0.73552139 -0.92824596 -0.68667621\n",
      " -0.45005614 -0.86310804 -0.74130078 -0.22780922 -0.86643461 -0.77814029\n",
      " -0.48551069 -0.5755976  -0.43847134 -0.79824274 -0.24338803 -0.2961845\n",
      " -0.22244682 -0.99751597 -0.31792587 -0.01072502 -0.48437783 -0.59781079\n",
      " -0.66163761 -0.04581308 -0.13877891 -0.56357841 -0.00100415 -0.85177208\n",
      " -0.64825742 -0.90692711 -0.2230838  -0.27244905 -0.39813611 -0.10062194\n",
      " -0.45269194 -0.01184623 -0.55904603 -0.23484968 -0.6428136  -0.88772925\n",
      " -0.07672655 -0.81940806 -0.00205401 -0.74540175 -0.6987435  -0.51802508\n",
      " -0.75116244 -0.60851464 -0.96593423 -0.41101805 -0.94677814 -0.44405099\n",
      " -0.29634192 -0.39610921 -0.18480023 -0.93356602 -0.12612585 -0.44512284\n",
      " -0.23129059 -0.69270398 -0.31473941 -0.69724251 -0.97426231 -0.70176986\n",
      " -0.13423375 -0.92794775 -0.30290608 -0.27263487 -0.31587352 -0.70852583\n",
      " -0.0224428  -0.89919809 -0.42098785 -0.326354   -0.07910537 -0.49021553\n",
      " -0.48849187 -0.87899248 -0.684092   -0.93421412 -0.39596463 -0.18813623\n",
      " -0.165716   -0.07977847 -0.26839784 -0.95705139 -0.6452446  -0.04309552\n",
      " -0.32116785 -0.7299405  -0.58499715 -0.82478451 -0.49865836 -0.78770218\n",
      " -0.62375419 -0.07933045 -0.89984808 -0.21838444 -0.04812459 -0.15827796\n",
      " -0.05649644 -0.16625395 -0.70647865 -0.86136094 -0.20096227 -0.41292714\n",
      " -0.87093481 -0.36844944 -0.36312996 -0.61769717 -0.35732787 -0.13719634\n",
      " -0.69891818 -0.62589995 -0.9456339  -0.88042827 -0.80873264 -0.57476157\n",
      " -0.78534    -0.66281998 -0.47442408 -0.49894322 -0.33000948 -0.00900298\n",
      " -0.46200633 -0.81523733 -0.18427838 -0.15860961 -0.43400231 -0.44329926\n",
      " -0.1238957  -0.7832686  -0.91068837 -0.08998113 -0.96256112 -0.96406208\n",
      " -0.37922404 -0.62003933 -0.9620124  -0.40674562 -0.70029894 -0.06779755\n",
      " -0.58172298 -0.79871682 -0.00256259 -0.5509543  -0.67158224 -0.79152249\n",
      " -0.9963528  -0.52432373 -0.57432434 -0.65288177 -0.42447261 -0.7470635\n",
      " -0.3097059  -0.56314104 -0.41828319 -0.76650059 -0.9426322  -0.62885523\n",
      " -0.97733729 -0.58052168 -0.83749487 -0.80097983 -0.13944431 -0.21853072\n",
      " -0.61107078 -0.80164644 -0.7319158  -0.8720573  -0.23818864 -0.63058633\n",
      " -0.72961956 -0.37213366 -0.20965635 -0.16603027 -0.57919701 -0.71883523\n",
      " -0.51781497 -0.91682889 -0.68586122 -0.8242185  -0.7179241  -0.07429162\n",
      " -0.30564411 -0.10675254 -0.21618736 -0.89482515 -0.71391876 -0.64957337\n",
      " -0.73486044 -0.68166972 -0.48602132 -0.33524306 -0.69767541 -0.10404421\n",
      " -0.17226242 -0.55435669 -0.54295391 -0.41711093 -0.57905669 -0.91361664\n",
      " -0.71747546 -0.24355795 -0.12193792 -0.48628952 -0.00562037 -0.00857\n",
      " -0.71914906 -0.92765934 -0.41477654 -0.9194745  -0.17671596 -0.49808492\n",
      " -0.14622244 -0.81990964 -0.90137498 -0.08266541 -0.67462059 -0.95128237\n",
      " -0.80041193 -0.26817772 -0.19895361 -0.55397746 -0.70143912 -0.3060856\n",
      " -0.43986024 -0.16411444 -0.16848069 -0.48225824 -0.33550366 -0.287146\n",
      " -0.05296372 -0.44265434 -0.34326957 -0.07585525 -0.24816778 -0.19007032\n",
      " -0.89073034 -0.82487976 -0.42221694 -0.45948807 -0.0754388  -0.08961254\n",
      " -0.7155048  -0.8552898  -0.93809533 -0.65974419 -0.57231314 -0.23283127\n",
      " -0.97465229 -0.14154346 -0.02516157 -0.43809113 -0.58693539 -0.64323031\n",
      " -0.20472906 -0.37665823 -0.95520234 -0.73998229 -0.45110202 -0.36115512\n",
      " -0.59356716 -0.87953334 -0.53018059 -0.74902993 -0.83770682 -0.18659961\n",
      " -0.6218055  -0.56621227 -0.34138307 -0.6912311  -0.57620518 -0.1721962\n",
      " -0.81764965 -0.51135511 -0.56364422 -0.31723338 -0.50716339 -0.75911021\n",
      " -0.90008539 -0.26287379 -0.87892302 -0.53241253 -0.52178918 -0.03040794\n",
      " -0.76166193 -0.84532259 -0.82628378 -0.60319699 -0.49320308 -0.71578007\n",
      " -0.28890034 -0.82844175 -0.62221977 -0.5451606  -0.97616475 -0.12226336\n",
      " -0.63831233 -0.44929862 -0.46456453 -0.86204301 -0.9253302  -0.82365088\n",
      " -0.50772526 -0.56731381 -0.86365596 -0.69356931 -0.79671152 -0.44085587\n",
      " -0.61191632 -0.57615032 -0.39930652 -0.08286508 -0.03684755 -0.48463293\n",
      " -0.95499076 -0.7159346  -0.39138654 -0.4210621  -0.11212897 -0.92265304\n",
      " -0.0198319  -0.96968825 -0.95685251 -0.78210286 -0.33176915 -0.84440976\n",
      " -0.69649671 -0.00932075 -0.64903992 -0.22045105 -0.98700486 -0.37605902\n",
      " -0.63212836 -0.73681043 -0.96821843 -0.50996656 -0.80660147 -0.83476345\n",
      " -0.39404782 -0.33898062 -0.04860548 -0.12008465 -0.59126791 -0.46744211\n",
      " -0.33458335 -0.95605072 -0.89456034 -0.67539316 -0.37980182 -0.35509591\n",
      " -0.19491643 -0.12058519 -0.56089464 -0.28894477 -0.3800757  -0.79979441\n",
      " -0.7957978  -0.53167512 -0.4989921  -0.25758504 -0.08549318 -0.95359706\n",
      " -0.35033141 -0.80753299 -0.26879835 -0.20361151 -0.52748842 -0.6829247\n",
      " -0.93771665 -0.43602965 -0.43086683 -0.19526764 -0.54361179 -0.59572709\n",
      " -0.01737942 -0.2933928  -0.05595869 -0.1291752  -0.94673401 -0.96372209\n",
      " -0.87152205 -0.64366215 -0.42866615 -0.33303855 -0.7855627  -0.73739749\n",
      " -0.32278122 -0.76207542 -0.59922881 -0.3526425  -0.8072042  -0.13831227\n",
      " -0.53059325 -0.15406985 -0.02848803 -0.53432991 -0.31794688 -0.92626299\n",
      " -0.93273906 -0.12270305 -0.19528654 -0.05376204 -0.93262523 -0.01592539\n",
      " -0.68866652 -0.88477077 -0.33514955 -0.45801456 -0.52776873 -0.51552056\n",
      " -0.91846791 -0.87652713 -0.54949324 -0.63149003 -0.29888757 -0.10649095\n",
      " -0.84745425 -0.36062583 -0.23020568 -0.88404289 -0.56374189 -0.94098214\n",
      " -0.1541997  -0.17129316 -0.94245281 -0.53753351 -0.80934946 -0.3636135\n",
      " -0.97481817 -0.99433748 -0.35585291 -0.36406978 -0.68975577 -0.14434107\n",
      " -0.34655768 -0.91339665 -0.94769393 -0.44255777 -0.88480355 -0.82534864\n",
      " -0.35836755 -0.31884226 -0.16208451 -0.83780316 -0.69200982 -0.86463226\n",
      " -0.79494224 -0.92775123 -0.11239189 -0.65601955 -0.50890385 -0.40536787\n",
      " -0.61757634 -0.34489936 -0.04476859 -0.17223792 -0.87190842 -0.79739317\n",
      " -0.47882929 -0.84459982 -0.82732292 -0.97367517 -0.03017169 -0.93932517\n",
      " -0.85770888 -0.83337982 -0.78957669 -0.25454464 -0.2906462  -0.21094734\n",
      " -0.54422038 -0.97188688 -0.31647305 -0.38075067 -0.25788309 -0.82078183\n",
      " -0.9824616  -0.0236971  -0.13595069 -0.42780478 -0.19679944 -0.96715999\n",
      " -0.356853   -0.72217018 -0.39419502 -0.45793598]\n",
      "Z Vector: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Error: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "[[8.99081822e-04 2.69197334e-04 4.68394249e-04 ... 3.40266570e-05\n",
      "  1.23844324e-04 7.24484876e-04]\n",
      " [1.88024490e-04 1.72296246e-04 3.55469117e-04 ... 3.62994427e-04\n",
      "  9.26089950e-04 5.10219762e-04]\n",
      " [5.70203155e-04 8.53110611e-05 6.04223019e-04 ... 4.41054054e-04\n",
      "  8.77344943e-05 5.41732052e-04]\n",
      " ...\n",
      " [2.24212796e-05 7.38126541e-04 4.38343371e-04 ... 2.64761273e-04\n",
      "  2.84389790e-04 4.26051268e-04]\n",
      " [7.86060276e-05 1.23291018e-04 9.65467220e-04 ... 2.55707429e-04\n",
      "  4.40968058e-04 2.91082491e-04]\n",
      " [2.54280319e-04 4.00585801e-04 2.89419163e-04 ... 7.48146586e-04\n",
      "  6.40608970e-04 2.31857666e-05]]\n",
      "\n",
      "Layer:  1\n",
      "Activations: \n",
      "[0.88924032 0.1694081  0.83202329 0.83070289 0.03419147 0.9955309\n",
      " 0.97379657 0.88561657 0.40623515 0.75574031 0.48577186 0.64909213\n",
      " 0.29219698 0.97108071 0.65293032 0.77725164]\n",
      "Biases: \n",
      "[-0.19005303 -0.463821   -0.02661986 -0.00326139 -0.54477012 -0.5609287\n",
      " -0.38536834 -0.14173806  0.11397395 -0.29229883 -0.36681935 -0.31610188\n",
      "  0.12297663 -0.38551627  0.09124606 -0.27788961]\n",
      "Z Vector: \n",
      "[ 2.08300474 -1.58982799  1.60003507  1.59061698 -3.34098951  5.40608859\n",
      "  3.61531205  2.04672785 -0.37955119  1.12946586 -0.05692792  0.61505097\n",
      " -0.88473766  3.5139006   0.63194483  1.24972145]\n",
      "Error: \n",
      "[ 3.53578436e-09 -5.17063429e-07  3.95249121e-09  3.90206344e-09\n",
      "  4.20404734e-06 -1.84240651e-07 -1.81438006e-08  2.93999376e-09\n",
      "  6.06722052e-08  3.50708752e-08  3.62349045e-07  9.80186642e-08\n",
      "  1.28841302e-07 -1.69527350e-08  1.43155893e-08  2.83023516e-08]\n",
      "\n",
      "[[-0.56381989 -0.4087592  -0.60134832 -0.60546096 -0.43528273 -0.3936834\n",
      "  -0.40668996 -0.5757344  -0.6229759  -0.53500187 -0.51597642 -0.52839547\n",
      "  -0.62279035 -0.40737272 -0.6208601  -0.53781157]\n",
      " [-0.73136556 -0.89233737 -0.70754804 -0.70781833 -0.83280936 -1.05967061\n",
      "  -0.95367226 -0.72664569 -0.66962073 -0.74034434 -0.74615174 -0.74273222\n",
      "  -0.66463941 -0.95050287 -0.68338434 -0.74119   ]\n",
      " [-0.54765944 -0.39747059 -0.57884987 -0.58291484 -0.42105176 -0.38824605\n",
      "  -0.39667878 -0.55763424 -0.59390558 -0.52022652 -0.50255322 -0.51407888\n",
      "  -0.59202798 -0.39630076 -0.5936075  -0.52380049]\n",
      " [-0.39899575 -0.3960933  -0.39377066 -0.3953096  -0.4061794  -0.47195215\n",
      "  -0.40916845 -0.39851229 -0.37207325 -0.40652034 -0.42499865 -0.41470081\n",
      "  -0.36960923 -0.40928953 -0.38097134 -0.40714848]\n",
      " [-0.54236618 -0.39439783 -0.57237628 -0.57668077 -0.41742116 -0.38821905\n",
      "  -0.39392168 -0.55276368 -0.58505674 -0.51704566 -0.49959715 -0.51064489\n",
      "  -0.58295818 -0.39439138 -0.58562285 -0.51888157]\n",
      " [-0.46204446 -0.38489597 -0.46784436 -0.46989566 -0.39798143 -0.42963993\n",
      "  -0.393409   -0.46618954 -0.45204107 -0.45255742 -0.45110725 -0.45278161\n",
      "  -0.44852212 -0.39394185 -0.46121154 -0.45460898]\n",
      " [-0.55706153 -0.40359768 -0.5915869  -0.59583249 -0.42905454 -0.39149013\n",
      "  -0.40195324 -0.56818291 -0.61132559 -0.52946183 -0.51010697 -0.52238715\n",
      "  -0.60992617 -0.40268461 -0.60995698 -0.53217523]\n",
      " [ 0.44400395 -2.64604784  0.53961115  0.54203353 -1.18365685 -4.07106332\n",
      "  -2.98391187  0.46983263  0.66519849  0.36591864  0.28801734  0.33743005\n",
      "   0.68172398 -2.9410522   0.61436326  0.37031064]\n",
      " [-0.56596396 -0.41076832 -0.60307178 -0.60715842 -0.43629809 -0.3937369\n",
      "  -0.40782845 -0.57755456 -0.62689864 -0.53624091 -0.51711578 -0.53049336\n",
      "  -0.62605942 -0.40851008 -0.62347897 -0.53955706]\n",
      " [-0.50551396 -0.38208615 -0.52364122 -0.526098   -0.40039562 -0.39758751\n",
      "  -0.38499712 -0.51281361 -0.51909859 -0.48683936 -0.47479351 -0.48216474\n",
      "  -0.51649626 -0.38526201 -0.52498639 -0.48841454]\n",
      " [-0.49602431 -0.3817316  -0.50974817 -0.51238471 -0.39879045 -0.40426577\n",
      "  -0.3855662  -0.50223234 -0.50279983 -0.47806508 -0.46802833 -0.47489136\n",
      "  -0.49882046 -0.38667881 -0.50898047 -0.48093632]\n",
      " [-0.33319538 -0.40702956 -0.31997924 -0.32146105 -0.42354118 -0.50697382\n",
      "  -0.42279992 -0.33059693 -0.30012817 -0.36176966 -0.40694987 -0.38040191\n",
      "  -0.29928984 -0.42301855 -0.30628911 -0.36050836]\n",
      " [-0.49363115 -0.38162491 -0.50673972 -0.50985145 -0.39825605 -0.40637742\n",
      "  -0.38682216 -0.49908917 -0.49934772 -0.47680281 -0.46754649 -0.47382945\n",
      "  -0.49528666 -0.38661452 -0.50614977 -0.47858141]\n",
      " [-0.50261283 -0.38198616 -0.51961478 -0.52190579 -0.39963779 -0.40064168\n",
      "  -0.38597877 -0.50993085 -0.51403528 -0.48436796 -0.47277316 -0.48064272\n",
      "  -0.51057941 -0.38536417 -0.52039885 -0.48599935]\n",
      " [-0.24983201 -0.42228645 -0.22896682 -0.23082755 -0.45565918 -0.54631103\n",
      "  -0.4346337  -0.24376955 -0.21511847 -0.30956306 -0.3995852  -0.34512429\n",
      "  -0.21843211 -0.43586903 -0.21682588 -0.30529638]\n",
      " [-0.55348545 -0.40120108 -0.58630337 -0.59041589 -0.42586371 -0.38984304\n",
      "  -0.39982202 -0.56366399 -0.60395214 -0.52537232 -0.50729516 -0.51966941\n",
      "  -0.60327822 -0.40025455 -0.60300923 -0.52786757]]\n",
      "\n",
      "Layer:  2\n",
      "Activations: \n",
      "[0.00248242 0.00015899 0.00310612 0.01373669 0.00331278 0.00824421\n",
      " 0.00273298 0.00226565 0.00242194 0.00527029 0.00590256 0.02204625\n",
      " 0.00605807 0.00547965 0.03821125 0.00287045]\n",
      "Biases: \n",
      "[-0.45741896 -0.29705354 -0.40527544  0.04383008 -0.38680209 -0.09022484\n",
      " -0.43319324  0.7116828  -0.46467586 -0.24500433 -0.20605274  0.14824173\n",
      " -0.19417425 -0.22624583  0.25428582 -0.4256905 ]\n",
      "Z Vector: \n",
      "[-5.99603551 -8.74649451 -5.77126871 -4.27385324 -5.70664911 -4.78996566\n",
      " -5.8996267  -6.08762694 -6.02076343 -5.24038548 -5.12644917 -3.79231988\n",
      " -5.10028709 -5.20121896 -3.22566498 -5.85041337]\n",
      "Error: \n",
      "[-2.92289157e-06 -1.33969992e-08 -3.74055410e-06 -2.13550372e-05\n",
      " -4.01697648e-06 -1.10551615e-05 -3.24189016e-06  4.59383891e-05\n",
      " -2.83040786e-06 -6.65476324e-06 -7.59101365e-06 -4.01934648e-05\n",
      " -7.78487214e-06 -6.94011426e-06 -8.51813463e-05 -3.42725874e-06]\n",
      "\n",
      "[[-1.16306426 -1.11320903 -1.13030879 -1.13210719 -1.12345738 -1.10279651\n",
      "  -1.14850065 -1.19276066 -1.16621973 -1.0925727  -1.09222603 -1.16270204\n",
      "  -1.09338707 -1.09341692 -1.19675581 -1.14011813]\n",
      " [ 1.41382478  1.31802764  1.54989575  2.15621844  1.59050083  2.04679131\n",
      "   1.47478061  0.8785908   1.39667558  1.85317129  1.90926061  2.20555536\n",
      "   1.92413013  1.8775602   2.22859656  1.50101073]\n",
      " [-1.4453627  -0.07110047 -1.32227944 -0.69581479 -1.28693635 -0.84514989\n",
      "  -1.39360608  2.50571209 -1.45846267 -1.04010666 -0.9857675  -0.6078524\n",
      "  -0.97418258 -1.02150607 -0.53242589 -1.36557727]\n",
      " [-1.19230819 -0.05628769 -1.09744054 -0.5968133  -1.07014822 -0.71915643\n",
      "  -1.1528252   2.43551882 -1.20216889 -0.87672156 -0.8321375  -0.52284487\n",
      "  -0.8228174  -0.86134542 -0.45634714 -1.1310643 ]\n",
      " [-2.0452082  -1.5212866  -1.92570539 -1.49162126 -1.89346765 -1.56201315\n",
      "  -1.99492709 -2.81786977 -2.05693119 -1.68904617 -1.64969644 -1.46670628\n",
      "  -1.64219509 -1.67542441 -1.45560336 -1.96584899]\n",
      " [-1.74652665 -0.58399808 -1.62347972 -1.08445191 -1.58841    -1.19200484\n",
      "  -1.69421707  0.8160291  -1.75850285 -1.35705592 -1.30906033 -1.03505592\n",
      "  -1.2994501  -1.34079875 -1.00676521 -1.66544227]\n",
      " [-1.04892994 -0.16145597 -0.97854799 -0.68106814 -0.95890034 -0.73697748\n",
      "  -1.01956878  2.34212844 -1.05622357 -0.82840899 -0.80077676 -0.6590543\n",
      "  -0.79546927 -0.81861911 -0.64961461 -1.00295947]\n",
      " [-1.60084566 -1.27463357 -1.49555232 -1.10494648 -1.46686782 -1.1664538\n",
      "  -1.55614944 -3.25366689 -1.61142788 -1.28276461 -1.24707018 -1.08582804\n",
      "  -1.24013715 -1.27077098 -1.082647   -1.5315123 ]\n",
      " [-0.75474476 -0.19055275 -0.6473253   0.09879205 -0.6148823  -0.12916179\n",
      "  -0.70952036  1.9961613  -0.76619968 -0.36728849 -0.30470409  0.26866215\n",
      "  -0.29044115 -0.3451351   0.44171526 -0.68548998]\n",
      " [-1.2696512  -1.0540166  -1.1997427  -0.95825888 -1.18096663 -0.99499134\n",
      "  -1.24030184 -2.80351326 -1.27711796 -1.06380894 -1.04144007 -0.94647636\n",
      "  -1.03781433 -1.0565637  -0.94014348 -1.22329063]]\n",
      "\n",
      "Layer:  3\n",
      "Activations: \n",
      "[0.17561504 0.00514053 0.02169109 0.02341215 0.20921748 0.06332921\n",
      " 0.02541053 0.227561   0.03029557 0.21193709]\n",
      "Biases: \n",
      "[-1.40328176 -5.51621563 -3.71829987 -3.65476773 -1.1305229  -2.55132005\n",
      " -3.56190898 -1.06982962 -3.47450726 -1.1839972 ]\n",
      "Z Vector: \n",
      "[-1.54634327 -5.26544454 -3.80892411 -3.73080965 -1.32964873 -2.69398519\n",
      " -3.64685275 -1.22213471 -3.46598991 -1.31328843]\n",
      "Error: \n",
      "[ 2.79833160e-02  1.63493231e-05  6.05908923e-04  7.35571242e-04\n",
      "  3.84009833e-02  4.60498501e-03  9.17464043e-04  4.03535640e-02\n",
      "  8.88736658e-04 -1.32459858e-01]\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "# net = Network(4, 3, 3, 2)\n",
    "# net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = False\n",
    "\n",
    "if training:\n",
    "    # #\n",
    "    # # 4 3 3 2 Test run\n",
    "    # #\n",
    "\n",
    "    # dummyData = [[0.125, 0.25, 0.675, 0.885]]\n",
    "    # dummyLabel = [1]\n",
    "\n",
    "    # for i in range(100):\n",
    "    #     net.trainBatch(dummyData, dummyLabel, 5)\n",
    "    #     net.coutLastLayer()\n",
    "    \n",
    "    # net.test(dummyData, dummyLabel)\n",
    "\n",
    "    # #\n",
    "    # # Training on the actual data\n",
    "    # #\n",
    "\n",
    "    with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "        data_batch_01 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "        label_batch_01 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_05 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_05 = pickle.load(infile)\n",
    "\n",
    "    # with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_first_100 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    print(\"Testing on batch data:\")\n",
    "    net.test(data_batch_01, label_batch_01)\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    print(\"Testing on the batch\")\n",
    "    for i in range(400):\n",
    "        # Number in one batch is 6000 so a learning rate of 6000 will result in an npm of 1\n",
    "        net.trainBatch(data_batch_01, label_batch_01, 60)\n",
    "        net.coutLastLayer()\n",
    "        # net.cout()\n",
    "        \n",
    "        # net.test(data_batch_01, label_batch_01)\n",
    "        #net.test(data_test, label_test)\n",
    "\n",
    "    print(\"Testing on batch data:\")\n",
    "    net.test(data_batch_01, label_batch_01)\n",
    "    print(\"Testing on test data:\")\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    net.checkRandomExamples(data_batch_01, label_batch_01)\n",
    "\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "    # #\n",
    "    # #\n",
    "    # ### CHECKING\n",
    "    # #\n",
    "    # #\n",
    "\n",
    "    # \n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    net.test(data_test, label_test)\n",
    "    net.checkRandomExamples(data_batch_01, label_batch_01)\n",
    "\n",
    "    net.cout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
