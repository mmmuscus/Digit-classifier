{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook\n",
    "\n",
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook\n",
    "\n",
    "# #\n",
    "# # Verify Reading Dataset via MnistDataloader class\n",
    "# #\n",
    "# %matplotlib inline\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pickle\n",
    "\n",
    "# #\n",
    "# # Set file paths based on added MNIST Datasets\n",
    "# #\n",
    "# input_path = 'dataset/'\n",
    "# training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "# training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "# test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "# test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# #\n",
    "# # Helper function to show a list of images with their relating titles\n",
    "# #\n",
    "# def show_images(images, title_texts):\n",
    "#     cols = 5\n",
    "#     rows = int(len(images)/cols) + 1\n",
    "#     plt.figure(figsize=(30,20))\n",
    "#     index = 1    \n",
    "#     for x in zip(images, title_texts):        \n",
    "#         image = x[0]        \n",
    "#         title_text = x[1]\n",
    "#         plt.subplot(rows, cols, index)        \n",
    "#         plt.imshow(image, cmap=plt.cm.gray)\n",
    "#         if (title_text != ''):\n",
    "#             plt.title(title_text, fontsize = 15);        \n",
    "#         index += 1\n",
    "\n",
    "# #\n",
    "# # Load MINST dataset\n",
    "# #\n",
    "# mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "# (data_train, label_train), (data_test, label_test) = mnist_dataloader.load_data()\n",
    "\n",
    "# with open(\"dataset/pickled/data_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_train, outfile)\n",
    "# with open(\"dataset/pickled/label_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_train, outfile)\n",
    "# with open(\"dataset/pickled/data_test.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_test, outfile)\n",
    "# with open(\"dataset/pickled/label_test.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_test, outfile)\n",
    "\n",
    "# first_10_data_train = []\n",
    "# first_10_label_train = []\n",
    "# for i in range(0, 10):\n",
    "#     first_10_data_train.append(data_train[i])\n",
    "#     first_10_label_train.append(label_train[i])\n",
    "\n",
    "# with open(\"dataset/pickled/first_10_data_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(first_10_data_train, outfile)\n",
    "# with open(\"dataset/pickled/first_10_label_train.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(first_10_label_train, outfile)\n",
    "\n",
    "# # There are 60000 training examples\n",
    "# # Try making it into 10 batches => 6000 per batch\n",
    "\n",
    "# data_batch_01 = []\n",
    "# data_batch_02 = []\n",
    "# data_batch_03 = []\n",
    "# data_batch_04 = []\n",
    "# data_batch_05 = []\n",
    "# data_batch_06 = []\n",
    "# data_batch_07 = []\n",
    "# data_batch_08 = []\n",
    "# data_batch_09 = []\n",
    "# data_batch_10 = []\n",
    "# label_batch_01 = []\n",
    "# label_batch_02 = []\n",
    "# label_batch_03 = []\n",
    "# label_batch_04 = []\n",
    "# label_batch_05 = []\n",
    "# label_batch_06 = []\n",
    "# label_batch_07 = []\n",
    "# label_batch_08 = []\n",
    "# label_batch_09 = []\n",
    "# label_batch_10 = []\n",
    "# for i in range(0, 6000):\n",
    "#     data_batch_01.append(data_train[i + (0 * 6000)])\n",
    "#     data_batch_02.append(data_train[i + (1 * 6000)])\n",
    "#     data_batch_03.append(data_train[i + (2 * 6000)])\n",
    "#     data_batch_04.append(data_train[i + (3 * 6000)])\n",
    "#     data_batch_05.append(data_train[i + (4 * 6000)])\n",
    "#     data_batch_06.append(data_train[i + (5 * 6000)])\n",
    "#     data_batch_07.append(data_train[i + (6 * 6000)])\n",
    "#     data_batch_08.append(data_train[i + (7 * 6000)])\n",
    "#     data_batch_09.append(data_train[i + (8 * 6000)])\n",
    "#     data_batch_10.append(data_train[i + (9 * 6000)])\n",
    "#     label_batch_01.append(label_train[i + (0 * 6000)])\n",
    "#     label_batch_02.append(label_train[i + (1 * 6000)])\n",
    "#     label_batch_03.append(label_train[i + (2 * 6000)])\n",
    "#     label_batch_04.append(label_train[i + (3 * 6000)])\n",
    "#     label_batch_05.append(label_train[i + (4 * 6000)])\n",
    "#     label_batch_06.append(label_train[i + (5 * 6000)])\n",
    "#     label_batch_07.append(label_train[i + (6 * 6000)])\n",
    "#     label_batch_08.append(label_train[i + (7 * 6000)])\n",
    "#     label_batch_09.append(label_train[i + (8 * 6000)])\n",
    "#     label_batch_10.append(label_train[i + (9 * 6000)])\n",
    "\n",
    "# with open(\"dataset/pickled/data_batch_01.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_01, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_02.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_02, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_03.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_03, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_04.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_04, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_05.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_05, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_06.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_06, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_07.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_07, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_08.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_08, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_09.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_09, outfile)\n",
    "# with open(\"dataset/pickled/data_batch_10.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_10, outfile)\n",
    "\n",
    "# with open(\"dataset/pickled/label_batch_01.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_01, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_02.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_02, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_03.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_03, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_04.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_04, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_05.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_05, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_06.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_06, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_07.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_07, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_08.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_08, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_09.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_09, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_10.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_10, outfile)\n",
    "\n",
    "# data_batch_first_100 = []\n",
    "# label_batch_first_100 = []\n",
    "# for i in range(100):\n",
    "#     data_batch_first_100.append(data_train[i])\n",
    "#     label_batch_first_100.append(label_train[i])\n",
    "\n",
    "# with open(\"dataset/pickled/data_batch_first_100.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(data_batch_first_100, outfile)\n",
    "# with open(\"dataset/pickled/label_batch_first_100.pickle\", \"wb\") as outfile:\n",
    "#     pickle.dump(label_batch_first_100, outfile)\n",
    "\n",
    "# #\n",
    "# # Show some random training and test images \n",
    "# #\n",
    "# images_2_show = []\n",
    "# titles_2_show = []\n",
    "# for i in range(0, 10):\n",
    "#    images_2_show.append(first_10_data_train[i])\n",
    "#    titles_2_show.append('training image [' + str(i) + '] = ' + str(first_10_label_train[i]))    \n",
    "\n",
    "# #for i in range(0, 5):\n",
    "# #   r = random.randint(1, 10000)\n",
    "# #   images_2_show.append(data_test[r])        \n",
    "# #   titles_2_show.append('test image [' + str(r) + '] = ' + str(label_test[r]))    \n",
    "\n",
    "# show_images(images_2_show, titles_2_show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, currentLayerLen):\n",
    "        self.activationVector = np.array([random.uniform(0, 1) for i in range(currentLayerLen)])\n",
    "        self.biasVector = np.array([random.uniform(0, 1) for i in range(currentLayerLen)])\n",
    "        self.adjBiasVector = np.zeros(currentLayerLen)\n",
    "        self.zVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.errorVector = np.zeros(shape = (currentLayerLen, 1))\n",
    "        self.size = currentLayerLen\n",
    "\n",
    "    def resetAdjBiasVector(self):\n",
    "        self.adjBiasVector = np.zeros(self.biasVector.size)\n",
    "\n",
    "    def cout(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)\n",
    "        print(\"Z Vector: \")\n",
    "        print(self.zVector)\n",
    "        print(\"Error: \")\n",
    "        print(self.errorVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        print(\"Activations: \")\n",
    "        print(self.activationVector)\n",
    "        print(\"Biases: \")\n",
    "        print(self.biasVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMatrix:\n",
    "    def __init__(self, prevLayerLen, nextLayerLen):\n",
    "        self.matrix = np.random.rand(nextLayerLen, prevLayerLen)\n",
    "        self.adjMatrix = np.zeros((nextLayerLen, prevLayerLen))\n",
    "\n",
    "    def resetAdjMatrix(self):\n",
    "        self.adjMatrix = np.zeros((self.matrix.shape))\n",
    "\n",
    "    def cout(self):\n",
    "        print(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDeriv(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, start, first, second, end):\n",
    "        startLayer = Layer(start)\n",
    "        firstLayer = Layer(first)\n",
    "        secondLayer = Layer(second)\n",
    "        endLayer = Layer(end)\n",
    "\n",
    "        self.Layers = np.array([startLayer, firstLayer, secondLayer, endLayer])\n",
    "\n",
    "        firstMatrix = weightMatrix(start, first)\n",
    "        secondMatrix = weightMatrix(first, second)\n",
    "        endMatrix = weightMatrix(second, end)\n",
    "\n",
    "        # Indexed with the layer before the matrix\n",
    "        self.Matrices = np.array([firstMatrix, secondMatrix, endMatrix])\n",
    "\n",
    "    def calculateZVector(self, layerIdx):\n",
    "        # self.Layers[layerIdx - 1] is previous Layer\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "        # self.Matrices[layerIdx - 1].matrix is weight matrix related to this calculation\n",
    "        # self.Layers[layerIdx - 1].activationVector is previous activation\n",
    "        # self.Layers[layerIdx].biasVector is current Bias\n",
    "\n",
    "        # Weight matrix * previous activation vector\n",
    "        self.Layers[layerIdx].zVector = np.dot(self.Matrices[layerIdx - 1].matrix, self.Layers[layerIdx - 1].activationVector)\n",
    "        # += current bias vector\n",
    "        self.Layers[layerIdx].zVector += self.Layers[layerIdx].biasVector\n",
    "\n",
    "    def forwardPropagationStep(self, layerIdx):\n",
    "        # currLayer = self.Layers[layerIdx] is current Layer\n",
    "\n",
    "        self.calculateZVector(layerIdx)\n",
    "\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].activationVector[i] = sigmoid(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "    # Assumes that data is between 0 and 255 value\n",
    "    def setStartLayerActivations(self, dataset):\n",
    "        # self.Layers[0] is start Layer\n",
    "\n",
    "        if len(dataset) * len(dataset[0]) != self.Layers[0].size:\n",
    "            print(\"There is a mismatch between the size of the input data and the start layer!\")\n",
    "            print(\"Size of dataset is: \" + str(len(dataset) * len(dataset[0])))\n",
    "            print(\"Size of first layer is: \" + str(self.Layers[0].size))\n",
    "\n",
    "        #print(\"1) Set the activations of the first layer\")\n",
    "\n",
    "        layerIdx = 0\n",
    "        for row in range(0, len(dataset)):\n",
    "            for col in range(0, len(dataset[0])):\n",
    "                self.Layers[0].activationVector[layerIdx] = dataset[row][col] / 255\n",
    "                layerIdx += 1\n",
    "\n",
    "    def fullForwardPropagation(self, target):\n",
    "        #print(\"2) Feedforward: Compute all activations for all layers\")\n",
    "        self.forwardPropagationStep(1)\n",
    "        self.forwardPropagationStep(2)\n",
    "        self.forwardPropagationStep(3)\n",
    "\n",
    "        #print(\"Target is: \" + str(target))\n",
    "        #print(\"Cost is: \" + str(self.cost(target)))\n",
    "    \n",
    "    def cost(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            if (i == target):\n",
    "                sum += pow(self.Layers[self.Layers.size - 1].activationVector[i] - 1.0, 2)\n",
    "            else:\n",
    "                sum += pow(self.Layers[self.Layers.size - 1].activationVector[i], 2)\n",
    "        return sum / (2 * self.Layers[self.Layers.size - 1].size)\n",
    "    \n",
    "    def fullBackwardPropagation(self, target):\n",
    "        #print(\"3) Output Error in last layer\")\n",
    "        self.calculateErrorInLastLayerForTarget(target)\n",
    "\n",
    "        #print(\"4) Backpropagate error: calculate error for all layers\")\n",
    "        self.calculateErrorFromNextLayerError(2)\n",
    "        self.calculateErrorFromNextLayerError(1)\n",
    "    \n",
    "    # The cost function is hard coded\n",
    "    def calculateErrorInLastLayerForTarget(self, target):\n",
    "        # self.Layers[self.Layers.size - 1] is end Layer\n",
    "\n",
    "        CGradient = self.Layers[self.Layers.size - 1].activationVector\n",
    "        # The desired value for target is 1\n",
    "        # This subtracts from the activations[target] as well !\n",
    "        # The change of the activations shouldnt matter\n",
    "        # It is not used later in the algo for the training example\n",
    "        CGradient[target] -= 1.0\n",
    "\n",
    "        # Apply sigmoid' to endLayer.zVector in place\n",
    "        for i in range(self.Layers[self.Layers.size - 1].size):\n",
    "            self.Layers[self.Layers.size - 1].zVector[i] = sigmoidDeriv(self.Layers[self.Layers.size - 1].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to endLayer.zVector in place\n",
    "        self.Layers[self.Layers.size - 1].errorVector = np.multiply(CGradient, self.Layers[self.Layers.size - 1].zVector)\n",
    "\n",
    "    # Assumes error in next layer is up to date\n",
    "    def calculateErrorFromNextLayerError(self, layerIdx):\n",
    "        # self.Layers[layerIdx] is current Layer\n",
    "        # self.Layers[layerIdx + 1] is next Layer\n",
    "\n",
    "        # self.Matrices[layerIdx].matrix.transpose() is the transposed weight matrix\n",
    "        # np.dot(transposedWeightMatrix, self.Layers[layerIdx + 1].errorVector) is transposed weight matrix * next layer error\n",
    "\n",
    "        # Apply sigmoid' to currLayer.zVector in place\n",
    "        for i in range(self.Layers[layerIdx].size):\n",
    "            self.Layers[layerIdx].zVector[i] = sigmoidDeriv(self.Layers[layerIdx].zVector[i])\n",
    "\n",
    "        # sigmoid' was applied to self.Layers[self.Layers.size - 1].zVector in place\n",
    "        # error = (transposed weight matrix * next layer error) o sigmoid'(z)\n",
    "        # Where o is index by index multiplication\n",
    "        self.Layers[layerIdx].errorVector = np.multiply(np.dot(self.Matrices[layerIdx].matrix.transpose(), self.Layers[layerIdx + 1].errorVector), self.Layers[layerIdx].zVector)\n",
    "\n",
    "    # All of this would be MUCH easier if W and B were stored as matrices and vectors\n",
    "    def adjustBasedOnGradientDescentForCurrentExample(self, learningRate, numberInBatch):\n",
    "        #print(\"5) Gradient Descent\")\n",
    "\n",
    "        # npm stands for n per m\n",
    "        # where: n: learning rate\n",
    "        #        m: number of train examples in batch\n",
    "        npm = learningRate / numberInBatch\n",
    "\n",
    "        # Adjust biases\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            # self.Layers[layerIdx] is current Layer\n",
    "\n",
    "            #self.Layers[layerIdx].biasVector -= npm * self.Layers[layerIdx].errorVector\n",
    "            #print(\"Delta is (bias): \", -npm * self.Layers[layerIdx].errorVector)\n",
    "\n",
    "            self.Layers[layerIdx].adjBiasVector += npm * self.Layers[layerIdx].errorVector\n",
    "\n",
    "        # # Adjust weights\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            # self.Matrices[weightMatrixIdx].matrix is current weight matrix\n",
    "            # self.Layers[weightMatrixIdx + 1] is current Layer\n",
    "            # self.Layers[weightMatrixIdx] is previous Layer\n",
    "\n",
    "            # For matrix mupltiplications the vectors need to be 2D\n",
    "            # This is how we make them 2D\n",
    "            # np.array([self.Layers[weightMatrixIdx].activationVector]) is previous activations\n",
    "            # np.array([self.Layers[weightMatrixIdx + 1].errorVector]) is current error\n",
    "            # Temp variable to make code more understandable\n",
    "            \n",
    "            # weight matrix -= (current Error)T * previous activation\n",
    "            #self.Matrices[weightMatrixIdx].matrix -= npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "            #print(\"Delta is (weight): \", -npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector])))\n",
    "\n",
    "            self.Matrices[weightMatrixIdx].adjMatrix += npm * np.dot(np.array([self.Layers[weightMatrixIdx + 1].errorVector]).transpose(), np.array([self.Layers[weightMatrixIdx].activationVector]))\n",
    "\n",
    "    def clearAdjustVariables(self):\n",
    "        for layer in self.Layers:\n",
    "            layer.resetAdjBiasVector()\n",
    "\n",
    "        for matrix in self.Matrices:\n",
    "            matrix.resetAdjMatrix()\n",
    "\n",
    "    def adjustWithAdjustVariables(self):\n",
    "        for layerIdx in range(1, self.Layers.size):\n",
    "            self.Layers[layerIdx].biasVector -= self.Layers[layerIdx].adjBiasVector\n",
    "\n",
    "        for weightMatrixIdx in range(self.Matrices.size):\n",
    "            self.Matrices[weightMatrixIdx].matrix -= self.Matrices[weightMatrixIdx].adjMatrix\n",
    "\n",
    "    def trainBatch(self, data, labels, learningRate):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        self.clearAdjustVariables()\n",
    "\n",
    "        numberInBatch = len(data)\n",
    "\n",
    "        for idx in range(numberInBatch):\n",
    "            # Steps of one training \n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "            self.fullBackwardPropagation(labels[idx])\n",
    "            self.adjustBasedOnGradientDescentForCurrentExample(learningRate, numberInBatch)\n",
    "\n",
    "        self.adjustWithAdjustVariables()\n",
    "\n",
    "    def findPrediction(self):\n",
    "        endLayerActivations = self.Layers[3].activationVector\n",
    "\n",
    "        max = 0\n",
    "        maxIdx = 11\n",
    "\n",
    "        for idx in range(endLayerActivations.size):\n",
    "            if endLayerActivations[idx] > max:\n",
    "                max = endLayerActivations[idx]\n",
    "                maxIdx = idx\n",
    "\n",
    "        return maxIdx\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        # Check if there is a length mismatch\n",
    "        if (len(data) != len(labels)):\n",
    "            print(\"There is a mismatch between the length of the data and lables\")\n",
    "            print(\"Length of data is: \" + str(data.size))\n",
    "            print(\"Length of labels are: \" + str(labels.size))\n",
    "\n",
    "        # Test for percentage of correct classifications\n",
    "        # numberOfTest = len(data)\n",
    "        # correct = 0\n",
    "\n",
    "        # for idx in range(numberOfTest):\n",
    "        #     self.setStartLayerActivations(data[idx])\n",
    "        #     self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "        #     correctIdx = self.findPrediction()\n",
    "\n",
    "        #     if correctIdx == labels[idx]:\n",
    "        #         correct += 1\n",
    "\n",
    "        # return correct/numberOfTest\n",
    "\n",
    "        numberOfTest = len(data)\n",
    "        sumCost = 0\n",
    "        correct = 0\n",
    "\n",
    "        for idx in range(numberOfTest):\n",
    "            self.setStartLayerActivations(data[idx])\n",
    "            self.fullForwardPropagation(labels[idx])\n",
    "\n",
    "            sumCost += self.cost(labels[idx])\n",
    "\n",
    "            correctIdx = self.findPrediction()\n",
    "            if correctIdx == labels[idx]:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"Average cost is: \", sumCost / numberOfTest)\n",
    "        print(\"Percentage of correct is: \", correct / numberOfTest)\n",
    "\n",
    "    def coutActivation(self):\n",
    "        for idx in range(self.Layers.size):\n",
    "            print(\"Layer: \" + str(idx))\n",
    "            print(self.Layers[idx].activationVector)\n",
    "\n",
    "    def coutBase(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].coutBase()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].coutBase()\n",
    "\n",
    "    def cout(self):\n",
    "        for i in range(self.Matrices.size):\n",
    "            print(\"Layer: \", i)\n",
    "            self.Layers[i].cout()\n",
    "            print()\n",
    "            self.Matrices[i].cout()\n",
    "            print()\n",
    "\n",
    "        print(\"Layer: \", self.Layers.size - 1)\n",
    "        self.Layers[self.Layers.size - 1].cout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cost is:  0.16575847772374006\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575811998343273\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.165757757947469\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657573915347443\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575702066208292\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575664524410266\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575626519315667\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575588041932948\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575549083026223\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575509633108051\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575469682436222\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575429221002644\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575388238522856\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575346724425763\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575304667846005\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657526205760716\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575218882223625\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575175129876188\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575130788403247\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575085845291668\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16575040287663495\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657499410225789\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574947275417495\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574899793080453\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574851640754382\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574802803508376\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574753265950168\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574703012210576\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657465202592573\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657460029021271\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574547787653368\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574494500272466\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657444040951024\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657438549620248\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574329740555796\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574273122113428\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574215619741517\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16574157211584598\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657409787504104\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657403758673888\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657397632248586\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657391405724102\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657385076507925\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16573786419152092\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657372099163447\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16573654453689235\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657358677542056\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16573517925820033\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16573447872715666\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657337658271371\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657330402114036\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16573230151987503\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16573154937830717\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16573078339771222\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657300031735487\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572920828497714\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572839829401895\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572757274454206\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572673116152306\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572587304983744\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657249978932644\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657241051534161\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572319426830953\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572226465128762\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657213156894392\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16572034674222913\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16571935713985295\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16571834618151166\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.165717313133663\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16571625722794742\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16571517765915514\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657140735829677\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16571294411351398\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16571178832074698\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16571060522767683\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1657093938072964\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16570815297932523\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16570688160670516\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16570557849179196\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16570424237215892\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16570287191618127\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16570146571830088\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16570002229355268\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16569854007198967\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1656970173925268\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1656954524960071\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16569384351795327\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16569218848051506\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.1656904852835329\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16568873169509424\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16568692534079177\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16568506369220254\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16568314405408605\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16568116355043155\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16567911910887473\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16567700744353853\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16567482503629194\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16567256811553904\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16567023263305655\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16566781423802437\n",
      "Percentage of correct is:  0.098\n",
      "Average cost is:  0.16566530824811404\n",
      "Percentage of correct is:  0.098\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset: 28 x 28 = 784\n",
    "import pickle\n",
    "\n",
    "with open(\"network.pickle\", \"rb\") as infile:\n",
    "    net = pickle.load(infile)\n",
    "\n",
    "#net = Network(4, 3, 3, 2)\n",
    "#net = Network(784, 16, 16, 10)\n",
    "\n",
    "# Network is loaded into net\n",
    "\n",
    "training = True\n",
    "testing = False\n",
    "\n",
    "if training:\n",
    "    # with open(\"dataset/pickled/data_batch_01.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_01 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_01.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_01 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_02.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_02 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_03.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_03 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_04.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_04 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/data_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     data_batch_05 = pickle.load(infile)\n",
    "    # with open(\"dataset/pickled/label_batch_05.pickle\", \"rb\") as infile:\n",
    "    #     label_batch_05 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_batch_first_100.pickle\", \"rb\") as infile:\n",
    "        data_batch_first_100 = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_batch_first_100.pickle\", \"rb\") as infile:\n",
    "        label_batch_first_100 = pickle.load(infile)\n",
    "\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    #print(\"Percentage of correct is: \" + str(net.test(data_test, label_test)))\n",
    "    net.test(data_test, label_test)\n",
    "\n",
    "    for i in range(500):\n",
    "        net.trainBatch(data_batch_first_100, label_batch_first_100, 1)\n",
    "        #print(\"Percentage of correct after batch 01 is: \" + str(net.test(data_test, label_test)))\n",
    "        net.test(data_test, label_test)\n",
    "        # net.trainBatch(data_batch_02, label_batch_02, 60000)\n",
    "        # print(\"Percentage of correct after batch 02 is: \" + str(net.test(data_test, label_test)))\n",
    "        # net.trainBatch(data_batch_03, label_batch_03, 60000)\n",
    "        # print(\"Percentage of correct after batch 03 is: \" + str(net.test(data_test, label_test)))\n",
    "        # net.trainBatch(data_batch_04, label_batch_04, 60000)\n",
    "        # print(\"Percentage of correct after batch 04 is: \" + str(net.test(data_test, label_test)))\n",
    "        # net.trainBatch(data_batch_05, label_batch_05, 60000)\n",
    "        # print(\"Percentage of correct after batch 05 is: \" + str(net.test(data_test, label_test)))\n",
    "\n",
    "    with open(\"network.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(net, outfile)\n",
    "\n",
    "if testing:\n",
    "    with open(\"dataset/pickled/data_test.pickle\", \"rb\") as infile:\n",
    "        data_test = pickle.load(infile)\n",
    "    with open(\"dataset/pickled/label_test.pickle\", \"rb\") as infile:\n",
    "        label_test = pickle.load(infile)\n",
    "\n",
    "    #print(\"Percentage of correct is: \" + str(net.test(data_test, label_test)))\n",
    "    net.test(data_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
